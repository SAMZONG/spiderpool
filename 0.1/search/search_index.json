{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spiderpool","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Spiderpool: An IP Address Management (IPAM) CNI plugin of Kubernetes for managing static ip for underlay network. Spiderpool can work well with any CNI project that is compatible with third-party IPAM plugins.</p> <p>Why Spiderpool? Given that there has not yet been a comprehensive, user-friendly and intelligent open source solution for what underlay networks' IPAMs need, Spiderpool comes in to eliminate the complexity of allocating IP addresses to underlay networks. With the hope of the operations of IP allocation being as simple as some overlay-network CNI, Spiderpool supports many features, such as static application IP addresses, dynamic scalability of IP addresses, multi-NIC, dual-stack support, etc. Hopefully, Spiderpool will be a new IPAM alternative for open source enthusiasts.</p>"},{"location":"#ipam-for-underlay-and-overlay-network","title":"IPAM for underlay and overlay network","text":"<p>There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios.  Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool.</p>"},{"location":"#overlay-network-solution","title":"Overlay network solution","text":"<p>These solutions implement the decoupling of POD network and host network, such as Calico, Cilium and other CNI plugins. Typically, They use tunnel technology such as vxlan to build an overlay network plane, and use NAT technology for north-south traffic.</p> <p>These IPAM solutions has some characteristics:</p> <ol> <li> <p>divide pod subnet into node-based IP block</p> <p>In terms of a smaller subnet mask, the pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account.</p> <p>First, since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, and achieve more efficient allocation. Second, a specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound POD.</p> </li> <li> <p>Sufficient IP address resources</p> <p>subnets not overlapping with any CIDR, could be used by the cluster, so the cluster have enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP address.</p> </li> <li> <p>No requirement for static IP addresses</p> <p>For the static IP address requirement, there is a difference between stateless application and stateful application. Regarding stateless application like deployment, the POD's name will change when the POD restarts, the business logic of the application itself is stateless, so static IP addresses means that all the POD replicas are fixed in a set of IP addresses; for stateful applications such as statefulset, considering both the fixed information including POD's names and stateful business logic, the strong binding of one POD and one specific IP address needs to be implemented for static IP addresses.</p> <p>The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology, and realizes the east-west communication through DNS, clusterIP and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas to follow the scheduling.Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addressed\", or support it in a rough way.</p> </li> </ol> <p>The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment, and can provide independent subnets with sufficient IP addresses for PODs.</p>"},{"location":"#underlay-network-solution","title":"Underlay network solution","text":"<p>This solution shares node's network for PODs, which means PODs can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications.</p> <p>There are two typical scenarios for underlay network solutions\uff1aclusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\":</p> <ol> <li> <p>An IP address able to be assigned to any node</p> <p>As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the POD replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together.</p> <p>Therefore, an IP address should be able to be allocated to a POD on any node.</p> </li> <li> <p>Different replicas within one application could obtain IP addresses across subnets</p> <p>Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes.</p> </li> <li> <p>Static IP addresses</p> <p>For some traditional applications, the source IPs or destination IPs needs to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means.</p> <p>Therefore, in order to reduce the transformation chores after the applications move to the kubernetes, applications need static IP address.</p> </li> <li> <p>Pods with Multiple NICs need IP addresses of different underlay subnets</p> <p>Since the POD is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets.</p> </li> <li> <p>IP conflict</p> <p>Underlay networks are more prone to IP conflicts. For instance, PODs conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection.</p> </li> <li> <p>Release and recover IP addresses</p> <p>Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched POD may fail due to the lack of IP addresses owing to some IP addresses not released by abnormal Pods. This requires IPAMs to have a more accurate, efficient and timely IP recovery mechanism.</p> </li> </ol> <p>The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of POD communication; no tunneling technology contributes to improved throughput and latency performance of network communications.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>For the architecture of spiderpool, refer to Architecture.</p>"},{"location":"#supported-cnis","title":"Supported CNIs","text":"<p>Any CNI project compatible with third-party IPAM plugins, can work well with spiderpool, such as:</p> <p>macvlan CNI,  vlan CNI,  ipvlan CNI,  sriov CNI,  ovs CNI,  Multus CNI,  calico CNI,  weave CNI</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>If you want to start some Pods with Spiderpool in minutes, refer to Quick start.</p>"},{"location":"#major-features","title":"Major features","text":"<ul> <li> <p>Multiple subnet objects</p> <p>The administrator can create multiple subnets objects mapping to each underlay CIDR, and applications can be assigned IP addresses within different subnets. to meet the complex planning of underlay networks. See example for more details.</p> </li> <li> <p>Automatical ippool for applications needing static ip</p> <p>To realize static IP addresses, some open source projects need hardcode IP addresses in the application's annotation, which is prone to operations accidents, manual operations of IP address conflicts, higher IP management costs caused by application scalability. Spiderpool could automatically create, delete, scale up and down a dedicated ippool with static IP address just for one application, which could minimize operation efforts.</p> </li> <li> <p>For stateless applications, the IP address range can be automatically fixed and IP resources can be dynamically scaled according to the number of application replicas. See example for more details.</p> </li> <li> <p>For stateful applications, IP addresses can be automatically fixed for each POD, and the overall IP scaling range can be fixed as well. And IP resources can be dynamically scaled according to the number of application replicas. See example for more details.</p> </li> <li> <p>The dedicated ippool could have keep some redundant IP address, which supports application to performance a rolling update when creating new pods. See example for more details.</p> </li> <li> <p>Support for third-party application controllers. See example for details</p> </li> <li> <p>Manual ippool for applications needing static ip but the administrator expects specify IP address by hand. See example for details</p> </li> <li> <p>For applications not requiring static IP addresses, they can share an IP pool. See example for details</p> </li> <li> <p>For one application with pods running on nodes accessing different underlay subnet, spiderpool could assign IP addresses within different subnets. See example for details</p> </li> <li> <p>Multiple IP pools can be set for a pod for the usage of backup IP resources. See example for details</p> </li> <li> <p>Set global reserved IPs that will not be assigned to Pods, it can avoid to misuse IP address already used by other network hosts. See example for details</p> </li> <li> <p>when assigning multiple NICs to a pod with Multus CNI, spiderpool could specify different subnet for each NIC. See example for details</p> </li> <li> <p>IP pools can be shared by whole cluster or bound to a specified namespace. See example for details</p> </li> <li> <p>An additional plugin veth provided by spiderpool has features:</p> </li> <li> <p>help some CNI addons be able to access clusterIP and pod-healthy check , such as macvlan CNI,  vlan CNI,  ipvlan CNI,  sriov CNI,  ovs CNI. See example for details.</p> </li> <li> <p>help coordinate routes of each NIC, for pods who has multiple NICs assigned by Multus. See example for details</p> </li> <li> <p>Private IPv4 address is rare, spiderpool provides a reasonable IP recycling mechanism, especially for running new pods when nodes or old pods are abnormal. See example for details</p> </li> <li> <p>The administrator could specify customized route. See example for details</p> </li> <li> <p>Good performance for assigning and release Pod IP, to guarantee the application release, to guarantee disaster recovery for the cluster. See example for details</p> </li> <li> <p>All above features can work in ipv4-only, ipv6-only, and dual-stack scenarios. See example for details</p> </li> <li> <p>Good performance for assigning and release Pod IP, to guarantee the application release, to guarantee disaster recovery for the cluster. See example for details</p> </li> </ul>"},{"location":"#other-features","title":"Other features","text":"<ul> <li> <p>Metrics</p> </li> <li> <p>Support AMD64 and ARM64</p> </li> <li> <p>lots of design can avoid IP leaks, IP conflicts, in case of administrator's fault, concurrent operations and so on.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.</p>"},{"location":"cmdref/spiderpool-agent/","title":"spiderpool-agent","text":"<p>This page describes CLI options and ENV of spiderpool-agent.</p>"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-daemon","title":"spiderpool-agent daemon","text":"<p>Run the spiderpool agent daemon.</p>"},{"location":"cmdref/spiderpool-agent/#options","title":"Options","text":"<pre><code>    --config-dir string         config file path (default /tmp/spiderpool/config-map)\n    --ipam-config-dir string    config file for ipam plugin \n</code></pre>"},{"location":"cmdref/spiderpool-agent/#env","title":"ENV","text":"<pre><code>    SPIDERPOOL_LOG_LEVEL                log level (DEBUG|INFO|ERROR)\n    SPIDERPOOL_ENABLED_METRIC           enable metrics (true|false)\n    SPIDERPOOL_METRIC_HTTP_PORT         metric port (default to 5711)\n    SPIDERPOOL_HEALTH_PORT              http port  (default to 5710)\n</code></pre>"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-shutdown","title":"spiderpool-agent shutdown","text":"<p>Notify of stopping the spiderpool-agent daemon.</p>"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-metric","title":"spiderpool-agent metric","text":"<p>Get local metrics.</p>"},{"location":"cmdref/spiderpool-agent/#options_1","title":"Options","text":"<pre><code>    --port string         http server port of local metric (default to 5711)\n</code></pre>"},{"location":"cmdref/spiderpool-controller/","title":"spiderpool-controller","text":"<p>This page describes CLI options and ENV of spiderpool-controller.</p>"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-daemon","title":"spiderpool-controller daemon","text":"<p>Run the spiderpool controller daemon.</p>"},{"location":"cmdref/spiderpool-controller/#options","title":"Options","text":"<pre><code>    --config-dir string         config file path (default /tmp/spiderpool/config-map)\n</code></pre>"},{"location":"cmdref/spiderpool-controller/#env","title":"ENV","text":"<pre><code>    SPIDERPOOL_LOG_LEVEL                        log level (DEBUG|INFO|ERROR)\n    SPIDERPOOL_ENABLED_METRIC                   enable metrics (true|false)\n    SPIDERPOOL_METRIC_HTTP_PORT                 metric port (default to 5721)\n    SPIDERPOOL_GC_IP_ENABLED                    enable GC ip in ippool, prior to other GC environment (true|false, default to true)\n    SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED    enable GC ip of terminating pod whose graceful-time times out (true|false, default to true)\n    SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY        delay to GC ip after graceful-time times out (second, default to 0)\n    SPIDERPOOL_HEALTH_PORT                      http port  (default to 5710)\n    SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION     all intervals of GC (second, default to 600)\n</code></pre>"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-shutdown","title":"spiderpool-controller shutdown","text":"<p>Notify of stopping spiderpool-controller daemon.</p>"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-metric","title":"spiderpool-controller metric","text":"<p>Get local metrics.</p>"},{"location":"cmdref/spiderpool-controller/#options_1","title":"Options","text":"<pre><code>    --port string         http server port of local metric (default to 5721)\n</code></pre>"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-status","title":"spiderpool-controller status","text":"<p>Show status:</p> <ol> <li>Whether local is controller leader</li> <li>...</li> </ol>"},{"location":"cmdref/spiderpool-controller/#options_2","title":"Options","text":"<pre><code>    --port string         http server port of local metric (default to 5720)\n</code></pre>"},{"location":"cmdref/spiderpoolctl/","title":"spiderpoolctl","text":"<p>This page describes CLI usage of spiderpoolctl for debug.</p>"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-gc","title":"spiderpoolctl gc","text":"<p>Trigger the GC request to spiderpool-controller.</p> <pre><code>    --address string         [optional] address for spider-controller (default to service address)\n</code></pre>"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-show","title":"spiderpoolctl ip show","text":"<p>Show a pod that is taking this IP.</p>"},{"location":"cmdref/spiderpoolctl/#options","title":"Options","text":"<pre><code>    --ip string     [required] ip\n</code></pre>"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-release","title":"spiderpoolctl ip release","text":"<p>Try to release an IP.</p>"},{"location":"cmdref/spiderpoolctl/#options_1","title":"Options","text":"<pre><code>    --ip string     [optional] ip\n    --force         [optional] force release ip\n</code></pre>"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-set","title":"spiderpoolctl ip set","text":"<p>Set IP to be taken by a pod. This will update ippool and workload endpoint resource.</p>"},{"location":"cmdref/spiderpoolctl/#options_2","title":"Options","text":"<pre><code>    --ip string     [required] ip\n    --pod string                [required] pod name\n    --namespace string          [required] pod namespace\n    --containerid string        [required] pod container id\n    --node string               [required] the node name who the pod locates\n    --interface string          [required] pod interface who taking effect the ip\n</code></pre>"},{"location":"concepts/allocation/","title":"IP Allocation","text":"<p>When a pod is creating, it will follow steps below to get an IP.</p> <ol> <li> <p>Get all ippool candidates.</p> <p>For which ippool is used by a pod, the following rules are listed from high to low priority.</p> <ul> <li> <p>SpiderSubnet annotation. \"ipam.spidernet.io/subnets\" and \"ipam.spidernet.io/subnet\" will choose to use auto-created ippool if the SpiderSubnet feature is enabled. See SpiderSubnet for detail.</p> </li> <li> <p>Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used to specify an ippool. See Pod Annotation for detail.</p> </li> <li> <p>Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used to specify an ippool. See namespace annotation for detail.</p> </li> <li> <p>CNI configuration file. It can be set to \"default_ipv4_ippool\" and \"default_ipv6_ippool\" in the CNI configuration file. See configuration for detail.</p> </li> <li> <p>Cluster default subnet.(you can do not specify any annotations and it will try to use cluster default subnet auto-created ippool if the SpiderSubnet feature it enabled)  It can be set to \"clusterDefaultIPv4Subnet\" and \"clusterDefaultIPv6Subnet\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail.</p> </li> <li> <p>Cluster default ippool.   It can be set to \"clusterDefaultIPv4IPPool\" and \"clusterDefaultIPv6IPPool\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail.</p> </li> </ul> </li> <li> <p>Filter valid ippool candidates.</p> <p>After getting IPv4 and IPv6 ippool candidates, it looks into each ippool and figures out whether it meets following rules, and learns which candidate ippool is available.</p> <ul> <li>The \"disable\" field of the ippool is \"false\"</li> <li>The \"ipversion\" field of the ippool must meet the claim</li> <li>The \"namespaceSelector\" field of the ippool must meet the namespace of the pod</li> <li>The \"podSelector\" field of the ippool must meet the pod</li> <li>The \"nodeSelector\" field of the ippool must meet the scheduled node of the pod</li> <li>The available IP resource of the ippool is not exhausted</li> </ul> </li> <li> <p>Assign IP from valid ippool candidates.</p> <p>When trying to assign IP from the ippool candidates, it follows rules as below.</p> <ul> <li>The IP is not reserved by the \"exclude_ips\" field of the ippool and all ReservedIP instances</li> <li>When the pod controller is a StatefulSet, the pod will get an IP in sequence</li> </ul> </li> </ol>"},{"location":"concepts/annotation/","title":"Annotations","text":"<p>Spiderpool provides annotations for configuring custom IPPools and routes.</p>"},{"location":"concepts/annotation/#pod-annotations","title":"Pod annotations","text":"<p>After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations.</p>"},{"location":"concepts/annotation/#ipamspidernetiosubnet","title":"ipam.spidernet.io/subnet","text":"<p>Specify the Subnets used to generate IPPools and allocate IP addresses.</p> <pre><code>ipam.spidernet.io/subnet: |-\n{\n\"ipv4\": [\"demo-v4-subnet1\"],\n\"ipv6\": [\"demo-v6-subnet1\"]\n}\n</code></pre> <ul> <li><code>ipv4</code> (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When <code>enableIPv4</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> <li><code>ipv6</code> (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When <code>enableIPv6</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> </ul>"},{"location":"concepts/annotation/#ipamspidernetiosubnets","title":"ipam.spidernet.io/subnets","text":"<pre><code>ipam.spidernet.io/subnets: |-\n[{\n\"interface\": \"eth0\",\n\"ipv4\": [\"demo-v4-subnet1\"],\n\"ipv6\": [\"demo-v6-subnet1\"]\n},{\n\"interface\": \"net1\",\n\"ipv4\": [\"demo-v4-subnet2\"],\n\"ipv6\": [\"demo-v6-subnet2\"]\n}]\n</code></pre> <ul> <li><code>interface</code> (string, required): Since the CNI request only carries the information of one interface, the field <code>interface</code> shall be specified to distinguish in the case of multiple interfaces.</li> <li><code>ipv4</code> (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When <code>enableIPv4</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> <li><code>ipv6</code> (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When <code>enableIPv6</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> </ul>"},{"location":"concepts/annotation/#ipamspidernetioippool-ip-number","title":"ipam.spidernet.io/ippool-ip-number","text":"<p>This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1').</p> <pre><code>ipam.spidernet.io/ippool-ip-number: +1\n</code></pre>"},{"location":"concepts/annotation/#ipamspidernetioippool-reclaim","title":"ipam.spidernet.io/ippool-reclaim","text":"<p>This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true').</p> <pre><code>ipam.spidernet.io/ippool-reclaim: true\n</code></pre>"},{"location":"concepts/annotation/#ipamspidernetioippool","title":"ipam.spidernet.io/ippool","text":"<p>Specify the IPPools used to allocate IP addresses.</p> <pre><code>ipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"demo-v4-ippool1\"],\n\"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"]\n}\n</code></pre> <ul> <li><code>ipv4</code> (array, optional): Specify which IPPool is used to allocate the IPv4 address. When <code>enableIPv4</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> <li><code>ipv6</code> (array, optional): Specify which IPPool is used to allocate the IPv6 address. When <code>enableIPv6</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> </ul>"},{"location":"concepts/annotation/#ipamspidernetioippools","title":"ipam.spidernet.io/ippools","text":"<p>It is similar to <code>ipam.spidernet.io/ippool</code> but could be used in the case with multiple interfaces. Note that <code>ipam.spidernet.io/ippools</code> has precedence over <code>ipam.spidernet.io/ippool</code>.</p> <pre><code>ipam.spidernet.io/ippools: |-\n[{\n\"interface\": \"eth0\",\n\"ipv4\": [\"demo-v4-ippool1\"],\n\"ipv6\": [\"demo-v6-ippool1\"],\n\"cleangateway\": true\n},{\n\"interface\": \"net1\",\n\"ipv4\": [\"demo-v4-ippool2\"],\n\"ipv6\": [\"demo-v6-ippool2\"],\n\"cleangateway\": false\n}]\n</code></pre> <ul> <li><code>interface</code> (string, required): Since the CNI request only carries the information of one interface, the field <code>interface</code> shall be specified to distinguish in the case of multiple interfaces.</li> <li><code>ipv4</code> (array, optional): Specify which IPPool is used to allocate the IPv4 address. When <code>enableIPv4</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> <li><code>ipv6</code> (array, optional): Specify which IPPool is used to allocate the IPv6 address. When <code>enableIPv6</code> in the ConfigMap <code>spiderpool-conf</code> is set to true, this field is required.</li> <li><code>cleangateway</code> (bool, optional): If set to true, the IPAM plugin will not return the default route (generated by <code>spec.gateway</code>) recorded in the IPPool.</li> </ul>"},{"location":"concepts/annotation/#ipamspidernetioroutes","title":"ipam.spidernet.io/routes","text":"<p>You can use the following code to enable additional routes take effect.</p> <pre><code>ipam.spidernet.io/routes: |-\n[{\n\"dst\": \"10.0.0.0/16\",\n\"gw\": \"192.168.1.1\"\n},{\n\"dst\": \"172.10.40.0/24\",\n\"gw\": \"172.18.40.1\"\n}]\n</code></pre> <ul> <li><code>dst</code> (string, required): Network destination of the route.</li> <li><code>gw</code> (string, required): The forwarding or next hop IP address.</li> </ul>"},{"location":"concepts/annotation/#namespace-annotations","title":"Namespace annotations","text":"<p>A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace.</p>"},{"location":"concepts/annotation/#ipamspidernetiodefault-ipv4-ippool","title":"ipam.spidernet.io/default-ipv4-ippool","text":"<pre><code>ipam.spidernet.io/default-ipv4-ippool: '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]'\n</code></pre>"},{"location":"concepts/annotation/#ipamspidernetiodefault-ipv6-ippool","title":"ipam.spidernet.io/default-ipv6-ippool","text":"<pre><code>ipam.spidernet.io/default-ipv6-ippool: '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'\n</code></pre>"},{"location":"concepts/arch/","title":"Architecture","text":"<p>Spiderpool consists of following components:</p> <ul> <li> <p>Spiderpool IPAM plugin, a binary installed on each host. It is called by a CNI plugin to assign and release IP for a pod.</p> </li> <li> <p>spiderpool-agent, deployed as a daemonset. It receives IPAM requests from the IPAM plugin, assigns and releases IP from the ippool resource.</p> </li> <li> <p>spiderpool-controller, deployed as a deployment.</p> </li> <li> <p>It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. Refer to Resource Reclaim for details.</p> </li> <li> <p>It uses a webhook to watch the ippool resource, help the administrator to validate creation, modification, and deletion.</p> </li> <li> <p>spiderpoolctl, a CLI tool for debugging.</p> </li> </ul>"},{"location":"concepts/arch/#crds","title":"CRDs","text":"<p>Spiderpool supports for the following CRDs:</p> <ul> <li> <p>SpiderSubnet CRD. It is used to represent a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. Refer to SpiderSubnet for detail.</p> </li> <li> <p>SpiderReservedIP CRD. It is used to represent a collection of IP addresses that Spiderpool expects not to be allocated. Refer to SpiderReservedIP for detail.</p> </li> <li> <p>SpiderIPPool CRD. It is used to represent a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Refer to SpiderIPPool for detail.</p> </li> <li> <p>SpiderEndpoint CRD. It is used to represent IP address allocation details for a specific endpoint object. Refer to SpiderEndpoint for detail.</p> </li> </ul>"},{"location":"concepts/config/","title":"Configuration","text":"<p>Instructions for global configuration and environment arguments of Spiderpool.</p>"},{"location":"concepts/config/#ipam-plugin-configuration","title":"IPAM Plugin Configuration","text":"<p>Here is an example of IPAM configuration.</p> <pre><code>{\n\"cniVersion\":\"0.3.1\",\n\"name\":\"macvlan-pod-network\",\n\"plugins\":[\n{\n\"name\":\"macvlan-pod-network\",\n\"type\":\"macvlan\",\n\"master\":\"ens256\",\n\"mode\":\"bridge\",\n\"mtu\":1500,\n\"ipam\":{\n\"type\":\"spiderpool\",\n\"log_file_path\":\"/var/log/spidernet/spiderpool.log\",\n\"log_file_max_size\":\"100\",\n\"log_file_max_age\":\"30\",\n\"log_file_max_count\":7,\n\"log_level\":\"INFO\",\n\"default_ipv4_ippool\": [\"default-ipv4-pool1\",\"default-ipv4-pool2\"],\n\"default_ipv6_ippool\": [\"default-ipv6-pool1\",\"default-ipv6-pool2\"]\n}\n}\n]\n}\n</code></pre> <ul> <li><code>log_file_path</code> (string, optional): Path to log file of IPAM plugin, default to <code>\"/var/log/spidernet/spiderpool.log\"</code>.</li> <li><code>log_file_max_size</code> (string, optional): Max size of each rotated file, default to <code>\"100\"</code>(unit MByte).</li> <li><code>log_file_max_age</code> (string, optional): Max age of each rotated file, default to <code>\"30\"</code>(unit Day).</li> <li><code>log_file_max_count</code> (string, optional): Max number of rotated file, default to <code>\"7\"</code>.</li> <li><code>log_level</code> (string, optional): Log level, default to <code>\"INFO\"</code>. It could be <code>\"INFO\"</code>, <code>\"DEBUG\"</code>, <code>\"WARN\"</code>, <code>\"ERROR\"</code>.</li> <li><code>default_ipv4_ippool</code> (string array, optional): Default IPAM IPv4 Pool to use.</li> <li><code>default_ipv6_ippool</code> (string array, optional): Default IPAM IPv6 Pool to use.</li> </ul>"},{"location":"concepts/config/#configmap-configuration","title":"Configmap Configuration","text":"<p>Configmap \"spiderpool-conf\" is the global configuration of Spiderpool.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: spiderpool-conf\nnamespace: kube-system\ndata:\nconf.yml: |\nipamUnixSocketPath: /var/run/spidernet/spiderpool.sock\nnetworkMode: legacy\nenableIPv4: true\nenableIPv6: true\nenableStatefulSet: true\nenableSpiderSubnet: true\nclusterSubnetDefaultFlexibleIPNumber: 1\n</code></pre> <ul> <li><code>ipamUnixSocketPath</code> (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin.</li> <li><code>networkMode</code>:</li> <li><code>legacy</code>: Applicable to the traditional network consisting of physical machines.</li> <li><code>enableIPv4</code> (bool):</li> <li><code>true</code>: Enable IPv4 IP allocation capability of Spiderpool.</li> <li><code>false</code>: Disable IPv4 IP allocation capability of Spiderpool.</li> <li><code>enableIPv6</code> (bool):</li> <li><code>true</code>: Enable IPv6 IP allocation capability of Spiderpool.</li> <li><code>false</code>: Disable IPv6 IP allocation capability of Spiderpool.</li> <li><code>enableStatefulSet</code> (bool):</li> <li><code>true</code>: Enable StatefulSet capability of Spiderpool.</li> <li><code>false</code>: Disable StatefulSet capability of Spiderpool.</li> <li><code>enableSpiderSubnet</code> (bool):</li> <li><code>true</code>: Enable SpiderSubnet capability of Spiderpool.</li> <li><code>false</code>: Disable SpiderSubnet capability of Spiderpool.</li> <li><code>clusterSubnetDefaultFlexibleIPNumber</code> (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.</li> </ul>"},{"location":"concepts/config/#spiderpool-agent-env","title":"Spiderpool-agent env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide."},{"location":"concepts/config/#spiderpool-controller-env","title":"Spiderpool-controller env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listening on. Disabled if empty. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED true Enable/disable IP GC for Terminating pod."},{"location":"concepts/gc/","title":"Resource Reclaim","text":""},{"location":"concepts/gc/#ip-garbage-collection","title":"IP garbage collection","text":""},{"location":"concepts/gc/#context","title":"Context","text":"<p>When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs.</p> <p>But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod.</p> <p>when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases:</p> <ul> <li> <p>When a CNI plugin is called, its network communication goes wrong and fails to release IP.</p> </li> <li> <p>The container runtime goes wrong and fails to call CNI plugin.</p> </li> <li> <p>A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be <code>deleting</code> status, but the CNI plugin fails to be called.</p> </li> </ul> <p>BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion.</p> <p>This issue will make a bad result:</p> <ul> <li> <p>the new pod may fail to run because the expected IP is still occupied.</p> </li> <li> <p>the IP resource is exhausted gradually although the actual number of pods does not grow.</p> </li> </ul> <p>Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue.</p>"},{"location":"concepts/gc/#solution","title":"Solution","text":"<p>For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time.</p> <p>The spiderpool controller takes charge of this responsibility. For more details, please refer to IP GC.</p>"},{"location":"concepts/gc/#spiderippool-garbage-collection","title":"SpiderIPPool garbage collection","text":"<p>To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules:</p> <ul> <li> <p>For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource.</p> </li> <li> <p>For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it.</p> </li> <li> <p>The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be <code>deleting</code> status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted.</p> </li> </ul>"},{"location":"concepts/gc/#spiderendpoint-garbage-collection","title":"SpiderEndpoint garbage collection","text":"<p>Once a pod is created and gets IPs from <code>SpiderIPPool</code>, Spiderpool will create a corresponding <code>SpiderEndpoint</code> object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to <code>OwnerReference</code> with the pod.</p> <p>When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding <code>SpiderEndpoint</code> object, then spiderpool controller will remove the <code>Current</code> data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet <code>SpiderEndpoint</code>, Spiderpool will delete it directly if its <code>Current</code> data was cleaned up)</p>"},{"location":"concepts/metrics/","title":"Metric","text":"<p>Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller.</p>"},{"location":"concepts/metrics/#spiderpool-controller","title":"spiderpool controller","text":"<p>The metrics of spiderpool controller is set by the following pod environment:</p> environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721"},{"location":"concepts/metrics/#spiderpool-agent","title":"spiderpool agent","text":"<p>The metrics of spiderpool agent is set by the following pod environment:</p> environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711"},{"location":"concepts/metrics/#get-started","title":"Get Started","text":""},{"location":"concepts/metrics/#enable-metric-support","title":"Enable Metric support","text":"<p>Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer install for details</p> <p>Check the environment variable <code>SPIDERPOOL_ENABLED_METRIC</code> of the daemonset <code>spiderpool-agent</code> for whether it is already set to <code>true</code> or not.</p> <p>Check the environment variable <code>SPIDERPOOL_ENABLED_METRIC</code> of deployment <code>spiderpool-controller</code> for whether it is already set to <code>true</code> or not.</p> <pre><code>kubectl -n kube-system get daemonset spiderpool-agent -o yaml\n------\nkubectl -n kube-system get deployment spiderpool-controller -o yaml\n</code></pre> <p>You can set one or both of them to <code>true</code>. For example, let's enable spiderpool agent metrics by running <code>helm upgrade --set spiderpoolAgent.prometheus.enabled=true</code>.</p>"},{"location":"concepts/metrics/#metric-reference","title":"Metric reference","text":""},{"location":"concepts/metrics/#spiderpool-agent_1","title":"Spiderpool Agent","text":"<p>Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include:</p> Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric)"},{"location":"concepts/metrics/#spiderpool-controller_1","title":"Spiderpool Controller","text":"<p>Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include:</p> Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)"},{"location":"concepts/spiderendpoint/","title":"SpiderEndpoint","text":"<p>A SpiderEndpoint resource represents IP address allocation details for a specific endpoint object.</p>"},{"location":"concepts/spiderendpoint/#crd-definition","title":"CRD definition","text":"<p>The SpiderEndpoint custom resource is modeled after a standard Kubernetes resource and is split into the <code>status</code> section:</p> <pre><code>type SpiderEndpoint struct {\n    [...]\n\n    // Status is the status of the SpiderEndpoint\n    Status WorkloadEndpointStatus `json:\"status,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spiderendpoint/#spiderendpoint-status","title":"SpiderEndpoint status","text":"<p>The <code>status</code> section contains some fields to describe details about the current Endpoint allocation.</p> <pre><code>// WorkloadEndpointStatus defines the observed state of SpiderEndpoint\ntype WorkloadEndpointStatus struct {\n    // the endpoint current allocation details\n    Current *PodIPAllocation `json:\"current,omitempty\"`\n\n    // the endpoint history allocation details\n    History []PodIPAllocation `json:\"history,omitempty\"`\n\n    // kubernetes controller owner reference\n    OwnerControllerType string `json:\"ownerControllerType\"`\n}\n\ntype PodIPAllocation struct {\n    // container ID\n    ContainerID string `json:\"containerID\"`\n\n    // node name\n    Node *string `json:\"node,omitempty\"`\n\n    // allocated IPs\n    IPs []IPAllocationDetail `json:\"ips,omitempty\"`\n\n    // created time\n    CreationTime *metav1.Time `json:\"creationTime,omitempty\"`\n}\n\ntype IPAllocationDetail struct {\n    // interface name\n    NIC string `json:\"interface\"`\n\n    // IPv4 address\n    IPv4 *string `json:\"ipv4,omitempty\"`\n\n    // IPv6 address\n    IPv6 *string `json:\"ipv6,omitempty\"`\n\n    // IPv4 SpiderIPPool name\n    IPv4Pool *string `json:\"ipv4Pool,omitempty\"`\n\n    // IPv6 SpiderIPPool name\n    IPv6Pool *string `json:\"ipv6Pool,omitempty\"`\n\n    // vlan ID\n    Vlan *int64 `json:\"vlan,omitempty\"`\n\n    // IPv4 gateway\n    IPv4Gateway *string `json:\"ipv4Gateway,omitempty\"`\n\n    // IPv6 gateway\n    IPv6Gateway *string `json:\"ipv6Gateway,omitempty\"`\n\n    CleanGateway *bool `json:\"cleanGateway,omitempty\"`\n\n    // route\n    Routes []Route `json:\"routes,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spiderippool/","title":"SpiderIPPool","text":"<p>A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned.</p>"},{"location":"concepts/spiderippool/#crd-definition","title":"CRD definition","text":"<p>The SpiderIPPool custom resource is modeled after a standard Kubernetes resource and is split into a <code>spec</code> and a <code>status</code> section:</p> <pre><code>type SpiderIPPool struct {\n    [...]\n\n    // Spec is the specification of the IPPool\n    Spec   IPPoolSpec   `json:\"spec,omitempty\"`\n\n    // Status is the status of the IPPool\n    Status IPPoolStatus `json:\"status,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spiderippool/#ippool-spec","title":"IPPool spec","text":"<p>The <code>spec</code> section embeds a specific IPPool field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the IPPool object for allocation:</p> <pre><code>// IPPoolSpec defines the desired state of SpiderIPPool\ntype IPPoolSpec struct {\n    // specify the IPPool's IP version\n    IPVersion *int64 `json:\"ipVersion,omitempty\"`\n\n    // specify the IPPool's subnet\n    Subnet string `json:\"subnet\"`\n\n    // specify the IPPool's IP ranges\n    IPs []string `json:\"ips\"`\n\n    // determine whether ths IPPool could be used or not\n    Disable *bool `json:\"disable,omitempty\"`\n\n    // specify the exclude IPs for the IPPool\n    ExcludeIPs []string `json:\"excludeIPs,omitempty\"`\n\n    // specify the gateway\n    Gateway *string `json:\"gateway,omitempty\"`\n\n    // specify the vlan\n    Vlan *int64 `json:\"vlan,omitempty\"`\n\n    //specify the routes\n    Routes []Route `json:\"routes,omitempty\"`\n\n    PodAffinity *metav1.LabelSelector `json:\"podAffinity,omitempty\"`\n\n    NamesapceAffinity *metav1.LabelSelector `json:\"namespaceAffinity,omitempty\"`\n\n    NodeAffinity *metav1.LabelSelector `json:\"nodeAffinity,omitempty\"`\n}\n\ntype Route struct {\n    // destination\n    Dst string `json:\"dst\"`\n\n    // gateway\n    Gw string `json:\"gw\"`\n}\n</code></pre>"},{"location":"concepts/spiderippool/#ippool-status","title":"IPPool status","text":"<p>The <code>status</code> section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses.</p> <pre><code>// IPPoolStatus defines the observed state of SpiderIPPool\ntype IPPoolStatus struct {\n    // all used addresses details\n    AllocatedIPs PoolIPAllocations `json:\"allocatedIPs,omitempty\"`\n\n    // the IPPool total addresses counts\n    TotalIPCount *int64 `json:\"totalIPCount,omitempty\"`\n\n    // the IPPool used addresses counts\n    AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"`\n}\n\n// PoolIPAllocations is a map of allocated IPs indexed by IP\ntype PoolIPAllocations map[string]PoolIPAllocation\n\n// PoolIPAllocation is an IP already has been allocated\ntype PoolIPAllocation struct {\n    // container ID\n    ContainerID string `json:\"containerID\"`\n\n    // interface name\n    NIC string `json:\"interface\"`\n\n    // node name\n    Node string `json:\"node\"`\n\n    // namespace\n    Namespace string `json:\"namespace\"`\n\n    // pod name\n    Pod string `json:\"pod\"`\n\n    // kubernetes controller owner reference\n    OwnerControllerType string `json:\"ownerControllerType\"`\n}\n</code></pre>"},{"location":"concepts/spiderreservedip/","title":"SpiderReservedIP","text":"<p>A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated.</p>"},{"location":"concepts/spiderreservedip/#crd-definition","title":"CRD definition","text":"<p>The SpiderReservedIP custom resource is modeled after a standard Kubernetes resource and is split into a <code>spec</code> section:</p> <pre><code>// SpiderReservedIP is the Schema for the spiderreservedips API\ntype SpiderReservedIP struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec ReservedIPSpec `json:\"spec,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spiderreservedip/#spiderreservedip-spec","title":"SpiderReservedIP spec","text":"<p>The <code>spec</code> section embeds a specific ReservedIP field which allows to define the list of all reserved IPs:</p> <pre><code>// ReservedIPSpec defines the desired state of SpiderReservedIP\ntype ReservedIPSpec struct {\n    // IP version\n    IPVersion *int64 `json:\"ipVersion,omitempty\"`\nni\n    // reserved IPs\n    IPs []string `json:\"ips\"`\n}\n</code></pre>"},{"location":"concepts/spidersubnet/","title":"SpiderSubnet","text":"<p>A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned.</p>"},{"location":"concepts/spidersubnet/#crd-definition","title":"CRD definition","text":"<p>The SpiderSubnet custom resource is modeled after a standard Kubernetes resource and is split into a <code>spec</code> and a <code>status</code> section:</p> <pre><code>type SpiderSubnet struct {\n    [...]\n\n    // Spec is the specification of the Subnet\n    Spec   SubnetSpec   `json:\"spec,omitempty\"`\n\n    // Status is the status of the SpiderSubnet\n    Status SubnetStatus `json:\"status,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spidersubnet/#subnet-spec","title":"Subnet spec","text":"<p>The <code>spec</code> section embeds a specific Subnet field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the Subnet object for allocation:</p> <pre><code>// SubnetSpec defines the desired state of SpiderSubnet\ntype SubnetSpec struct {\n    // specify the SpiderSubnet's IP version\n    IPVersion *int64 `json:\"ipVersion,omitempty\"`\n\n    // specify the SpiderSubnet's subnet\n    Subnet string `json:\"subnet\"`\n\n    // specify the SpiderSubnet's IP ranges\n    IPs []string `json:\"ips\"`\n\n    // specify the exclude IPs for the SpiderSubnet\n    ExcludeIPs []string `json:\"excludeIPs,omitempty\"`\n\n    // specify the gateway\n    Gateway *string `json:\"gateway,omitempty\"`\n\n    // specify the vlan\n    Vlan *int64 `json:\"vlan,omitempty\"`\n\n    //specify the routes\n    Routes []Route `json:\"routes,omitempty\"`\n}\n</code></pre>"},{"location":"concepts/spidersubnet/#subnet-status","title":"Subnet status","text":"<p>The <code>status</code> section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses.</p> <pre><code>// SubnetStatus defines the observed state of SpiderSubnet\ntype SubnetStatus struct {\n    // the SpiderSubnet IPPool pre-allocations\n    ControlledIPPools PoolIPPreAllocations `json:\"controlledIPPools,omitempty\"`\n\n    // the SpiderSubnet total addresses counts\n    TotalIPCount *int64 `json:\"totalIPCount,omitempty\"`\n\n    // the SpiderSubnet allocated addresses counts\n    AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"`\n}\n</code></pre> <pre><code>// PoolIPPreAllocations is a map of pool IP pre-allocation details indexed by pool name.\ntype PoolIPPreAllocations map[string]PoolIPPreAllocation\n\ntype PoolIPPreAllocation struct {\n    // specify the SpiderSubnet's IPPool allocation IP ranges\n    IPs []string `json:\"ips\"`\n}\n</code></pre>"},{"location":"develop/CODE-OF-CONDUCT/","title":"Code of Conduct","text":"<p>This project follows the CNCF Code of Conduct.</p>"},{"location":"develop/contributing/","title":"contributing","text":""},{"location":"develop/contributing/#setup-cluster-and-run-test","title":"setup cluster and run test","text":"<ol> <li> <p>check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them</p> <pre><code># make dev-doctor\ngo version go1.17 linux/amd64\ncheck e2e tools \npass   'docker' installed\npass   'kubectl' installed\npass   'kind' installed\npass   'p2ctl' installed\nfinish checking e2e tools\n</code></pre> </li> <li> <p>run the e2e</p> <pre><code># make e2e\n</code></pre> </li> </ol> <p>if your run it for the first time, it will download some images, you could set the http proxy</p> <pre><code>    # ADDR=10.6.0.1\n    # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890\n    # make e2e\n</code></pre> <p>run a specified case</p> <pre><code>    # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\"\n</code></pre> <ol> <li>you could do it step by step with the follow</li> </ol> <p>build the image</p> <pre><code>    # do some coding\n\n    $ git add .\n    $ git commit -s -m 'message'\n\n    # !!! images is built by commit sha, so make sure the commit is submit locally\n    $ make build_image\n    # or (if buildx fail to pull images)\n    $ make build_docker_image\n</code></pre> <p>setup the cluster</p> <pre><code>    # setup the kind cluster of dual-stack\n    # !!! images is tested by commit sha, so make sure the commit is submit locally\n    $ make e2e_init\n      .......\n      -----------------------------------------------------------------------------------------------------\n         succeeded to setup cluster spider\n         you could use following command to access the cluster\n            export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config\n            kubectl get nodes\n      -----------------------------------------------------------------------------------------------------\n\n    # setup the kind cluster of ipv4-only\n    $ make e2e_init -e E2E_IP_FAMILY=ipv4\n\n    # setup the kind cluster of ipv6-only\n    $ make e2e_init -e E2E_IP_FAMILY=ipv6\n</code></pre> <p>run the e2e test</p> <pre><code>    # run all e2e test on dual-stack cluster\n    $ make e2e_test\n\n    # run all e2e test on ipv4-only cluster\n    $ make e2e_test -e E2E_IP_FAMILY=ipv4\n\n    # run all e2e test on ipv6-only cluster\n    $ make e2e_test -e E2E_IP_FAMILY=ipv6\n\n    # Run all e2e tests on an enableSpiderSubnet=false cluster\n    $ make e2e_test -e E2E_SPIDERPOOL_ENABLE_SUBNET=false\n\n    # run smoke test\n    $ make e2e_test -e E2E_GINKGO_LABELS=smoke\n\n    # after finishing e2e case , you could test repeated for debugging flaky tests\n    # example: run a case repeatedly\n    $ make e2e_test -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \"\n\n    # example: run a case until fails\n    $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \"\n\n    $ ls e2ereport.json\n\n    $ make clean_e2e\n</code></pre> <ol> <li>you could test specified images with the follow<pre><code># load images to docker\n$ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG}\n$ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG}\n\n# setup the cluster with the specified image\n$ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\\n        -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME}   \\\n        -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME}\n\n# run all e2e test\n$ make e2e_test\n</code></pre> </li> </ol> <p>5 finally, you could visit \"http://HostIp:4040\" the in the browser of your desktop, and get flamegraph</p>"},{"location":"develop/contributing/#submit-pull-request","title":"Submit Pull Request","text":"<p>A pull request will be checked by following workflow, which is required for merging.</p>"},{"location":"develop/contributing/#action-your-pr-should-be-signed-off","title":"Action: your PR should be signed off","text":"<p>When you commit your modification, add <code>-s</code> in your commit command <code>git commit -s</code></p>"},{"location":"develop/contributing/#action-check-yaml-files","title":"Action: check yaml files","text":"<p>If this check fails, see the yaml rule.</p> <p>Once the issue is fixed, it could be verified on your local host by command <code>make lint-yaml</code>.</p> <p>Note: To ignore a yaml rule, you can add it into <code>.github/yamllint-conf.yml</code>.</p>"},{"location":"develop/contributing/#action-check-golang-source-code","title":"Action: check golang source code","text":"<p>It checks the following items against any updated golang file.</p> <ul> <li> <p>Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg</p> </li> <li> <p>Comment <code>// TODO</code> should follow the format: <code>// TODO (AuthorName) ...</code>, which easy to trace the owner of the remaining job</p> </li> <li> <p>Unitest and upload coverage to codecov</p> </li> <li> <p>Each golang test file should mark ginkgo label</p> </li> </ul>"},{"location":"develop/contributing/#action-check-licenses","title":"Action: check licenses","text":"<p>Any golang or shell file should be licensed correctly.</p>"},{"location":"develop/contributing/#action-check-markdown-file","title":"Action: check markdown file","text":"<ul> <li>Check markdown format, if fails, See the Markdown Rule</li> </ul> <p>You can test it on your local machine with the command <code>make lint-markdown-format</code>.</p> <p>You can fix it on your local machine with the command <code>make fix-markdown-format</code>.</p> <p>If you believe it can be ignored, you can add it to <code>.github/markdownlint.yaml</code>.</p> <ul> <li>Check markdown spell error.</li> </ul> <p>You can test it with the command <code>make lint-markdown-spell-colour</code>.</p> <p>If you believe it can be ignored, you can add it to <code>.github/.spelling</code>.</p>"},{"location":"develop/contributing/#action-lint-yaml-file","title":"Action: lint yaml file","text":"<p>If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons.</p> <p>You can test it on your local machine with the command <code>make lint-yaml</code>.</p>"},{"location":"develop/contributing/#action-lint-chart","title":"Action: lint chart","text":""},{"location":"develop/contributing/#action-lint-openapiyaml","title":"Action: lint openapi.yaml","text":""},{"location":"develop/contributing/#action-check-code-spell","title":"Action: check code spell","text":"<p>Any code spell error of golang files will be checked.</p> <p>You can check it on your local machine with the command <code>make lint-code-spell</code>.</p> <p>It could be automatically fixed on your local machine with the command <code>make fix-code-spell</code>.</p> <p>If you believe it can be ignored, edit <code>.github/codespell-ignorewords</code> and make sure all letters are lower-case.</p>"},{"location":"develop/contributing/#changelog","title":"Changelog","text":"<p>How to automatically generate changelogs:</p> <ol> <li> <p>All PRs should be labeled with \"pr/release/***\" and can be merged.</p> </li> <li> <p>When you add the label, the changelog will be created automatically.</p> </li> </ol> <p>The changelog contents include:</p> <ul> <li> <p>New Features: it includes all PRs labeled with \"pr/release/feature-new\"</p> </li> <li> <p>Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\"</p> </li> <li> <p>Fixes: it includes all PRs labeled with \"pr/release/bug\"</p> </li> <li> <p>All historical commits within this version</p> </li> <li> <p>The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.</p> </li> </ul>"},{"location":"develop/release/","title":"workflow for release","text":""},{"location":"develop/release/#pre-steps","title":"pre-steps","text":"<ul> <li>update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml'</li> <li>update version in '/VERSION'</li> <li>a version tag should be set on right branch. The version should go with </li> <li>v0.1.0-rc0</li> <li>v0.1.0-rc1</li> <li>v0.1.0</li> <li>v0.1.1</li> <li>v0.1.2</li> <li>v0.2.0-rc0</li> <li>v0.2.0</li> </ul>"},{"location":"develop/release/#push-a-version-tag","title":"push a version tag","text":"<p>If a tag vx.x.x is pushed , the following steps will automatically run:</p> <ol> <li> <p>check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION</p> </li> <li> <p>create a branch named 'release-vx.x.x'</p> </li> <li> <p>build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool</p> </li> <li> <p>generate the changelog by historical PR labeled as \"pr/release/*\"</p> <p>submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\".</p> <p>changelogs is generated by historical PR label:</p> <p>label \"pr/release/feature-new\" to be classified to \"New Features\"</p> <p>label \"pr/release/feature-changed\" to be classified to \"Changed Features\"</p> <p>label \"pr/release/feature-bug\" to be classified to \"Fixes\"</p> </li> <li> <p>build the chart package with the pushed tag, and submit a PR to branch 'github_pages' </p> <p>you cloud get the chart with command <code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool</code></p> </li> <li> <p>submit '/docs' to '/docs' of branch 'github_pages'</p> </li> <li> <p>create a GitHub Release attached with the chart package and changelog</p> </li> <li> <p>Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"</p> </li> </ol> <p>For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml</p>"},{"location":"develop/release/#post","title":"post","text":"<ol> <li>Submit a issue of the version update to the documentation site --&gt; https://github.com/DaoCloud/DaoCloud-docs</li> </ol>"},{"location":"develop/roadmap/","title":"roadmap","text":"<p>The following features are considered for the near future</p>"},{"location":"develop/roadmap/#support-cli","title":"support CLI","text":"<p>provide a CLI tool to debug and operate</p> <ul> <li> <p>check which pod an IP is taken by </p> </li> <li> <p>check IP usage</p> </li> <li> <p>trigger GC </p> </li> </ul>"},{"location":"develop/roadmap/#support-none-kubernetes-native-controller","title":"support none kubernetes-native controller","text":"<p>Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application.</p> <p>This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet.</p> <p>This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: </p> <ul> <li> <p>does not support automatically scale up and down the IP</p> </li> <li> <p>does not support automatically delete the ippool</p> </li> </ul> <p>In the future, spiderpool may support all operation of automatical ippool.</p>"},{"location":"develop/roadmap/#support-multi-cluster","title":"support multi-cluster","text":"<ul> <li> <p>when multi-cluster use Spiderpool to assign underlay IP address in the same CIDR, spiderpool could      synchronize ippool resource within a same subnet from other cluster, so it could help avoid IP conflict </p> </li> <li> <p>when multi-cluster use Spiderpool to manager underlay IP address, leader cluster could     synchronize all Spiderpool resource from member clusters, which help manager all underlay IP address</p> </li> </ul>"},{"location":"develop/roadmap/#strengthen-veth","title":"strengthen veth","text":"<ul> <li>support to detect IP conflict when setting up the network of pod </li> </ul>"},{"location":"develop/roadmap/#integrate-more-cni","title":"integrate more CNI","text":"<ul> <li>integrate more CNI addon to solve underlay network needs </li> </ul>"},{"location":"develop/roadmap/#improve-performance","title":"improve performance","text":"<ul> <li>continually improve the performance in kinds of scenes</li> </ul>"},{"location":"develop/roadmap/#egress-gateway-for-underlay-solution","title":"egress gateway for underlay solution","text":""},{"location":"develop/roadmap/#assign-ip-on-public-cloud-platform","title":"assign IP on public cloud platform","text":""},{"location":"develop/swagger_openapi/","title":"SWAGGER OPENAPI","text":"<p>Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec. source codes.</p>"},{"location":"develop/swagger_openapi/#features","title":"Features","text":"<ul> <li>Validate spec</li> <li>Generate C/S codes</li> <li>Verify spec with current source codes</li> <li>Clean codes</li> <li>Use swagger-ui to analyze the given specs.</li> </ul>"},{"location":"develop/swagger_openapi/#usages","title":"Usages","text":"<p>There are two ways for you to get access to the features.</p> <ul> <li>Use <code>makefile</code>, it's the simplest way.</li> <li>Use shell <code>swag.sh</code>. The format usage for 'swag.sh' is <code>swag.sh $ACTION $SPEC_DIR</code>.</li> </ul>"},{"location":"develop/swagger_openapi/#validate-spec","title":"validate spec","text":"<p>Validate the current spec just give the second parameter with the spec directory.</p> <pre><code>./tools/scripts/swag.sh validate ./api/v1/agent\n</code></pre> <p>Or you can use <code>makefile</code> to validate the spiderpool agent and controller with the following command.  </p> <pre><code>make openapi-validate-spec\n</code></pre>"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","title":"generate source codes with the given spec","text":"<p>To generate agent source codes:</p> <pre><code>./tools/scripts/swag.sh generate ./api/v1/agent\n</code></pre> <p>Or you can use <code>makefile</code> to generate for both of agent and controller two:</p> <pre><code>make openapi-code-gen\n</code></pre>"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","title":"verify the spec with current source codes to make sure whether the current source codes is out of date","text":"<p>To verify the given spec whether valid or not:</p> <pre><code>./tools/scripts/swag.sh verify ./api/v1/agent\n</code></pre> <p>Or you can use <code>makefile</code> to verify for both of agent and controller two:</p> <pre><code>make openapi-verify\n</code></pre>"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","title":"clean the generated source codes","text":"<p>To clean the generated agent codes:</p> <pre><code>./tools/scripts/swag.sh verify ./api/v1/agent\n</code></pre> <p>Or you can use <code>makefile</code> to clean for both of agent and controller two:</p> <pre><code>make clean-openapi-code\n</code></pre>"},{"location":"develop/swagger_openapi/#use-swagger-ui","title":"Use swagger-ui","text":"<p>To analyze the defined specs in your local environment with docker:</p> <pre><code>make openapi-ui\n</code></pre> <p>Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.</p>"},{"location":"develop/swagger_openapi/#steps-for-developers","title":"Steps For Developers","text":"<ol> <li>Modify the specs: agent-swagger spec and    controller-swagger spec</li> <li>Validate the modified specs</li> <li>Use swagger-ui to check the effects in your local environment with docker</li> <li>Re-generate the source codes with the modified specs</li> <li>Commit your PR.</li> </ol>"},{"location":"usage/certificate/","title":"Certificates","text":"<p>Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways.</p>"},{"location":"usage/certificate/#auto-certificates","title":"Auto certificates","text":"<p>Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure:</p> <pre><code>helm install spiderpool spiderpool/spiderpool --namespace kube-system \\\n--set spiderpoolController.tls.method=auto\n</code></pre> <p>Note that the default value of parameter <code>spiderpoolController.tls.method</code> is <code>auto</code>.</p>"},{"location":"usage/certificate/#provided-certificates","title":"Provided certificates","text":"<p>If you want to run spiderpool-controller with a self-signed certificate, <code>provided</code> would be a good choice. You can use OpenSSL to generate certificates, or run the following script:</p> <pre><code>wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh\n</code></pre> <p>Generate the certificates:</p> <pre><code>chmod +x generateCert.sh &amp;&amp; ./generateCert.sh \"/tmp/tls\"\n\nCA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n'`\nSERVER_CERT=`cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n'`\nSERVER_KEY=`cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n'`\n</code></pre> <p>Then, deploy Spiderpool in the <code>provided</code> mode:</p> <pre><code>helm install spiderpool spiderpool/spiderpool --namespace kube-system \\\n--set spiderpoolController.tls.method=provided \\\n--set spiderpoolController.tls.provided.tlsCa=${CA} \\\n--set spiderpoolController.tls.provided.tlsCert=${SERVER_CERT} \\\n--set spiderpoolController.tls.provided.tlsKey=${SERVER_KEY}\n</code></pre>"},{"location":"usage/certificate/#cert-manager-certificates","title":"Cert-manager certificates","text":"<p>It is not recommended to use this mode directly, because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference).</p> <p>Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool.</p> <pre><code>helm install spiderpool spiderpool/spiderpool --namespace kube-system \\\n--set spiderpoolController.tls.method=certmanager \\\n--set spiderpoolController.tls.certmanager.issuerName=${CERT_MANAGER_ISSUER_NAME}\n</code></pre>"},{"location":"usage/cli/","title":"Command line tool (spiderpoolctl)","text":"<p>TODO.</p>"},{"location":"usage/debug/","title":"Q&amp;A","text":"<p>TODO.</p>"},{"location":"usage/gc/","title":"Reclaim IP","text":""},{"location":"usage/gc/#description","title":"Description","text":"<p>The spiderpool owns an IP garbage collection mechanism, it helps to clean up leaked IPs once CNI cmdDel failed.</p>"},{"location":"usage/gc/#enable-ip-gc-support","title":"Enable IP GC support","text":"<p>Check k8s deployment <code>spiderpool-controller</code> environment property <code>SPIDERPOOL_GC_IP_ENABLED</code> whether is already set to <code>true</code> or not. (It would be enabled by default)</p> <pre><code>kubectl edit deploy spiderpool-controller -n kube-system\n</code></pre>"},{"location":"usage/gc/#design","title":"Design","text":"<p>The spiderpool-controller uses <code>pod informer</code> and regular interval <code>scan all SpiderIPPool</code> to clean up leaked IPs and corresponding SpiderEndpoint object. We used a memory cache to record the Pods object that the corresponding IPs and SpiderEndpoint objects should be cleaned.</p> <p>Here are several cases that we will release IP:</p> <ul> <li> <p>pod was <code>deleted</code>, except StatefulSet restarts its pod situation.</p> </li> <li> <p>pod is <code>Terminating</code>, we will release its IPs after <code>pod.DeletionGracePeriodSeconds</code>, you can set environment <code>AdditionalGraceDelay</code>(default 0 seconds) to add delay duration. you can also determine whether gc <code>Terminating</code> status pod or not with environment <code>SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED</code>. (It would be enabled by default).  There are 2 situation that this env may help: 1. If one node downtime in your cluster, you must reply on the IP GC to release the IPs. 2. In some underlay mode, if you do not release one IP with terminating pod and the new pod cannot fetch available IP to start because of the IP resources shortage.  But there's a special situation we should watch out for: if the node lost the connection with the master API server due to the node Interface or network issues, the pod network may also good and still serves well. In this case, if we release its IPs and allocate them to other pods, this will lead to IP conflict.</p> </li> <li> <p>pod is <code>Succeeded</code> or <code>Failed</code> phase, we'll clean the pod's IPs after <code>pod.DeletionGracePeriodSeconds</code>, you can set environment <code>AdditionalGraceDelay</code>(default 0 seconds) to add delay duration.</p> </li> <li> <p>SpiderIPPool allocated IP corresponding pod is not exist in the Kubernetes, except StatefulSet restarts its pod situation.</p> </li> <li> <p>The pod UID is different from the SpiderIPPool IP allocation pod UID.</p> </li> </ul>"},{"location":"usage/gc/#notice","title":"Notice","text":"<ul> <li> <p>The spiderpool controller owns multiple replicas and uses leader election, and the IP Garbage collection <code>pod informer</code> only serves for <code>Master</code>.   But every backup will use <code>scan all SpiderIPPool</code> to release the leaked IPs that should be cleaned up immediately, it wouldn't trace the pod into memory cache with the upper pod status cases.</p> </li> <li> <p>We can change tracing pod <code>AdditionalGraceDelay</code> with the environment <code>SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY</code>. (default 5 seconds)</p> </li> <li> <p>If one node broke in your cluster, the IP GC will forcibly release the unreachable pod corresponding IPs if you enabled env <code>SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED</code>.  And there's one rare situation that your pod is still alive even after pod <code>DeletionGracePeriod</code> duration, the IP GC will still forcibly release the unreachable pod corresponding IPs. For these two cases, we recommend the Main CNI has the ability to check IP conflict.  The Veth plugin implements this feature already, you can use it to collaborate with <code>Macvlan</code> or <code>SR-IOV</code> Main CNI.</p> </li> </ul>"},{"location":"usage/get-started-calico-zh_CN/","title":"Calico Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3:</p> <ul> <li> <p>\u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740</p> </li> <li> <p>IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9</p> </li> <li> <p>\u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528</p> </li> </ul>"},{"location":"usage/get-started-calico-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ul> <li> <p>\u4e00\u4e2a Kubernetes \u96c6\u7fa4(\u63a8\u8350 k8s version &gt; 1.22), \u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002</p> <p>\u786e\u8ba4 calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002</p> <p>\u786e\u8ba4 calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002</p> </li> <li> <p>Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177</p> </li> </ul>"},{"location":"usage/get-started-calico-zh_CN/#spiderpool","title":"\u5b89\u88c5 Spiderpool","text":"<pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system </code></pre> <p>Spiderpool \u9ed8\u8ba4 IPv4-Only, \u5982\u9700\u542f\u7528 IPv6 \u8bf7\u53c2\u8003 Spiderpool IPv6</p> <p>\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 <code>--set global.imageRegistryOverride=ghcr.m.daocloud.io</code> \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002</p> <p>\u521b\u5efa Pod \u7684\u5b50\u7f51(SpiderSubnet):</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: nginx-subnet-v4\n  labels:  \n    ipam.spidernet.io/subnet-cidr: 10-244-0-0-16\nspec:\n  ips:\n  - 10.244.100.0-10.244.200.1\n  subnet: 10.244.0.0/16\nEOF\n</code></pre> <p>\u9a8c\u8bc1\u5b89\u88c5\uff1a</p> <pre><code>[root@master ~]# kubectl get po -n kube-system  | grep spiderpool\nspiderpool-agent-27fr2                     1/1     Running     0          2m\n  spiderpool-agent-8vwxj                     1/1     Running     0          2m\n  spiderpool-controller-bc8d67b5f-xwsql      1/1     Running     0          2m\n  [root@master ~]# kubectl get ss\nNAME              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\n  nginx-subnet-v4   4         10.244.0.0/16   0                    25602\n</code></pre>"},{"location":"usage/get-started-calico-zh_CN/#calico-bgp","title":"\u914d\u7f6e Calico BGP [\u53ef\u9009]","text":"<p>\u672c\u4f8b\u5e0c\u671b calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u5b50\u7f51(<code>10.244.0.0/16</code>)\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 pod ip\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002</p> <p>\u7f51\u7edc\u62d3\u6251\u5982\u4e0b:</p> <p></p> <ol> <li> <p>\u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router</p> <p>\u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR:</p> <pre><code>root@router:~# apt install -y frr\n</code></pre> <p>FRR \u5f00\u542f BGP \u529f\u80fd:</p> <pre><code>root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons\nroot@router:~# systemctl restart frr\n</code></pre> <p>\u914d\u7f6e FRR:</p> <pre><code>root@router:~# vtysh\nrouter# config\nrouter(config)# router bgp 23000 \nrouter(config)# bgp router-id 172.16.13.1 \nrouter(config)# neighbor 172.16.13.11 remote-as 64512 \nrouter(config)# neighbor 172.16.13.21 remote-as 64512  \nrouter(config)# no bgp ebgp-requires-policy \n</code></pre> <p>\u914d\u7f6e\u89e3\u91ca:</p> <ul> <li>Router \u4fa7\u7684 AS \u4e3a <code>23000</code>, \u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a <code>64512</code>\u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a <code>ebgp</code>, \u8282\u70b9\u4e4b\u95f4\u4e3a <code>ibgp</code></li> <li>\u9700\u8981\u5173\u95ed <code>ebgp-requires-policy</code>, \u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb</li> <li>172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP</li> </ul> <p>\u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting\u3002</p> </li> <li> <p>\u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45</p> <p>Calico \u9700\u8981\u914d\u7f6e <code>calico_backend: bird</code>, \u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd:</p> <pre><code>[root@master1 ~]# kubectl get cm -n kube-system calico-config -o yaml\napiVersion: v1\ndata:\n  calico_backend: bird\n  cluster_type: kubespray,bgp\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n{\"apiVersion\":\"v1\",\"data\":{\"calico_backend\":\"bird\",\"cluster_type\":\"kubespray,bgp\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"calico-config\",\"namespace\":\"kube-system\"}}\ncreationTimestamp: \"2023-02-26T15:16:35Z\"\nname: calico-config\nnamespace: kube-system\nresourceVersion: \"2056\"\nuid: 001bbd09-9e6f-42c6-9339-39f71f81d363\n</code></pre> <p>\u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh:</p> <pre><code>[root@master1 ~]# calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}'\n</code></pre> <p>\u521b\u5efa BGPPeer:</p> <pre><code>[root@master1 ~]# cat &lt;&lt; EOF | calicoctl apply -f -\napiVersion: projectcalico.org/v3\nkind: BGPPeer\nmetadata:\n  name: my-global-peer\nspec:\n  peerIP: 172.16.13.1\n  asNumber: 23000\nEOF\n</code></pre> <p>peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740</p> <p>asNumber \u4e3a BGP Router \u7684 AS \u53f7</p> <p>\u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb:</p> <pre><code>[root@master1 ~]# calicoctl node status\nCalico process is running.\n\nIPv4 BGP status\n+--------------+-----------+-------+------------+-------------+\n| PEER ADDRESS | PEER TYPE | STATE |   SINCE    |    INFO     |\n+--------------+-----------+-------+------------+-------------+\n| 172.16.13.1  | global    | up    | 2023-03-15 | Established |\n+--------------+-----------+-------+------------+-------------+\n\nIPv6 BGP status\nNo IPv6 peers found.\n</code></pre> <p>\u66f4\u591a Calico BGP \u914d\u7f6e, \u8bf7\u53c2\u8003 Calico BGP</p> </li> </ol>"},{"location":"usage/get-started-calico-zh_CN/#calico-ip","title":"\u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60","text":"<p>\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60, \u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531:</p> <pre><code>[root@master1 ~]# cat &lt;&lt; EOF | calicoctl apply -f -\napiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\n  name: spiderpool-subnet\nspec:\n  blockSize: 26\ncidr: 10.244.0.0/16\n  ipipMode: Never\n  natOutgoing: false\nnodeSelector: all()\nvxlanMode: Never\nEOF\n</code></pre> <p>cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51: <code>10.244.0.0/16</code></p> <p>\u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a: Never</p>"},{"location":"usage/get-started-calico-zh_CN/#calico-ipam-spiderpool","title":"\u5207\u6362 Calico \u7684 <code>IPAM</code> \u4e3a Spiderpool","text":"<p>\u7f16\u8f91 calico \u7684 configMap: <code>calico-config</code> ,\u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a spiderpool \u5e76\u91cd\u542f <code>calico-node</code>:</p> <pre><code>[root@master1 ~]# kubectl edit cm -n kube-system calico-config \napiVersion: v1\ndata:\n  calico_backend: bird\n  cni_network_config: |-\n    {\n...\n        \"ipam\": {\n-         \"type\": \"calico-ipam\"\n+         \"type\": \"spiderpool\"\n},\n      ...\n    }\n</code></pre> <p>\u91cd\u542f <code>calico-node</code>, \u68c0\u67e5 <code>ipam</code> \u662f\u5426\u6210\u529f\u5207\u6362\u4e3a <code>spiderpool</code> :</p> <pre><code>[root@master1 ~]# kubectl delete po -n kube-system -l k8s-app=calico-node\n[root@master1 ~]# # wait calico-node ready\n[root@master1 ~]# cat /etc/cni/net.d/10-calico.conflist | grep ipam -A1\n\"ipam\": {\n\"type\": \"spiderpool\",\n</code></pre>"},{"location":"usage/get-started-calico-zh_CN/#_2","title":"\u521b\u5efa\u5e94\u7528","text":"<p>\u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b:</p> <pre><code>[root@master1 ~]# cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\nselector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: '{\"ipv4\":[\"nginx-subnet-v4\"]}'\nipam.spidernet.io/ippool-ip-number: '+3'\nlabels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\nprotocol: TCP\nEOF\n</code></pre> <p><code>ipam.spidernet.io/subnet</code>: \u4ece \"nginx-subnet-v4\" SpiderSubnet \u4e2d\u5206\u914d\u56fa\u5b9a IP</p> <p><code>ipam.spidernet.io/ippool-ip-number</code>: '+3' \u8868\u793a\u5e94\u7528\u5206\u914d\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u526f\u672c\u6570\u591a 3 \u4e2a\uff0c\u7528\u4e8e\u5e94\u7528\u6eda\u52a8\u53d1\u5e03\u65f6\u6709\u4e34\u65f6 IP \u53ef\u7528</p> <p>\u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annnotations \u6307\u5b9a\u7684 <code>subnet: nginx-subnet-v4</code> \u4e2d\u81ea\u52a8\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a <code>auto-nginx-v4-eth0-452e737e5e12</code> \u7684 IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528\u7ed1\u5b9a\u3002IP \u8303\u56f4\u4e3a: <code>10.244.100.90-10.244.100.95</code>, \u6c60 IP \u6570\u91cf\u4e3a <code>5</code>:</p> <pre><code>[root@master1 ~]# kubectl get sp\nNAME                              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-nginx-v4-eth0-452e737e5e12   4         10.244.0.0/16   2                    5                false     false\n[root@master ~]# kubectl get sp auto-nginx-v4-eth0-452e737e5e12 -o jsonpath='{.spec.ips}' \n[\"10.244.100.90-10.244.100.95\"]\n</code></pre> <p>\u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 <code>auto-nginx-v4-eth0-452e737e5e12</code> \u7684 IP \u6c60\u8303\u56f4\u5185:</p> <pre><code>[root@master1 ~]# kubectl get po -o wide\nNAME                     READY   STATUS        RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES\nnginx-644659db67-szgcg   1/1     Running       0          23s     10.244.100.90    worker5   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-98rcg   1/1     Running       0          23s     10.244.100.92    master1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>\u6269\u5bb9\u526f\u672c\u6570\u5230 <code>3</code>, \u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece\u81ea\u52a8\u6c60: <code>auto-nginx-v4-eth0-452e737e5e12(10.244.100.90-10.244.100.95)</code> \u4e2d\u5206\u914d:</p> <pre><code>[root@master1 ~]# kubectl scale deploy nginx --replicas 3  # scale pods\ndeployment.apps/nginx scaled\n[root@master1 ~]# kubectl get po -o wide\nNAME                     READY   STATUS        RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES\nnginx-644659db67-szgcg   1/1     Running       0          1m     10.244.100.90    worker5   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-98rcg   1/1     Running       0          1m     10.244.100.92    master1   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-brqdg   1/1     Running       0          10s    10.244.100.94    master1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>\u67e5\u770b\u81ea\u52a8\u6c60: <code>auto-nginx-v4-eth0-452e737e5e12</code> \u7684 <code>ALLOCATED-IP-COUNT</code> \u548c <code>TOTAL-IP-COUNT</code> \u90fd\u65b0\u589e 1 :</p> <pre><code>[root@master1 ~]# kubectl get sp\nNAME                              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-nginx-v4-eth0-452e737e5e12   4         10.244.0.0/16   3                    6                false     false\n</code></pre>"},{"location":"usage/get-started-calico-zh_CN/#_3","title":"\u7ed3\u8bba","text":"<p>\u7ecf\u8fc7\u6d4b\u8bd5: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38(\u5305\u62ec\u8de8 Calico \u5b50\u7f51)\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002</p>"},{"location":"usage/get-started-calico/","title":"Calico Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements:</p> <ul> <li> <p>Assign static IP addresses to applications</p> </li> <li> <p>Scale IP pools dynamically based on replica counts</p> </li> <li> <p>Enable external clients outside the cluster to access applications without their IPs</p> </li> </ul>"},{"location":"usage/get-started-calico/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI.</p> <p>Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks.</p> <p>Confirm that Calico has enabled BGP configuration in full-mesh mode\u3002</p> </li> <li> <p>Helm and Calicoctl</p> </li> </ul>"},{"location":"usage/get-started-calico/#install-spiderpool","title":"Install Spiderpool","text":"<pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system </code></pre> <p>Spiderpool is IPv4-Only by default. If you want to enable IPv6, refer to Spiderpool IPv6.</p> <p>If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter <code>-set global.imageRegistryOverride=ghcr.m.daocloud.io</code> to avoid image pulling failures for Spiderpool.</p> <p>Create a subnet for a Pod (SpiderSubnet):</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: nginx-subnet-v4\n  labels:  \n    ipam.spidernet.io/subnet-cidr: 10-244-0-0-16\nspec:\n  ips:\n  - 10.244.100.0-10.244.200.1\n  subnet: 10.244.0.0/16\nEOF\n</code></pre> <p>Verify the installation\uff1a</p> <pre><code>[root@master ~]# kubectl get po -n kube-system  | grep spiderpool\nspiderpool-agent-27fr2                     1/1     Running     0          2m\nspiderpool-agent-8vwxj                     1/1     Running     0          2m\nspiderpool-controller-bc8d67b5f-xwsql      1/1     Running     0          2m\n\n[root@master ~]# kubectl get ss\nNAME              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nnginx-subnet-v4   4         10.244.0.0/16   0                    25602\n</code></pre>"},{"location":"usage/get-started-calico/#configure-calico-bgp-optional","title":"Configure Calico BGP [optional]","text":"<p>In this example, we want Calico to work in underlay mode and announce the Spiderpool subnet (<code>10.244.0.0/16</code>) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router.</p> <p>If you don't need external clients to access pod IPs directly, skip this step.</p> <p>The network topology is as follows:</p> <p></p> <ol> <li> <p>Configure a host outside the cluster as BGP Router</p> <p>We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand:</p> <pre><code>root@router:~# apt install -y frr\n</code></pre> <p>FRR enable BGP:</p> <pre><code>root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons\nroot@router:~# systemctl restart frr\n</code></pre> <p>Configure FRR:</p> <pre><code>root@router:~# vtysh\nrouter# config\nrouter(config)# router bgp 23000 \nrouter(config)# bgp router-id 172.16.13.1 \nrouter(config)# neighbor 172.16.13.11 remote-as 64512 \nrouter(config)# neighbor 172.16.13.21 remote-as 64512  \nrouter(config)# no bgp ebgp-requires-policy \n</code></pre> <p>Configuration descriptions:</p> <ul> <li>The AS on the router side is <code>23000</code>, and the AS on the cluster node side is <code>64512</code>. The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp.</li> <li><code>ebgp-requires-policy</code> needs to be disabled, otherwise the BGP session cannot be established.</li> <li>172.16.13.11/21 is the IP address of the cluster node.</li> </ul> <p>For more information, refer to frrouting.</p> </li> <li> <p>Configure BGP neighbor for Calico</p> <p><code>calico_backend: bird</code> needs to be configured to establish a BGP session:</p> <pre><code>[root@master1 ~]# kubectl get cm -n kube-system calico-config -o yaml\napiVersion: v1\ndata:\n  calico_backend: bird\n  cluster_type: kubespray,bgp\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n{\"apiVersion\":\"v1\",\"data\":{\"calico_backend\":\"bird\",\"cluster_type\":\"kubespray,bgp\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"calico-config\",\"namespace\":\"kube-system\"}}\ncreationTimestamp: \"2023-02-26T15:16:35Z\"\nname: calico-config\nnamespace: kube-system\nresourceVersion: \"2056\"\nuid: 001bbd09-9e6f-42c6-9339-39f71f81d363\n</code></pre> <p>In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled:</p> <pre><code>[root@master1 ~]# calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}'\n</code></pre> <p>Create BGPPeer:</p> <pre><code>[root@master1 ~]# cat &lt;&lt; EOF | calicoctl apply -f -\napiVersion: projectcalico.org/v3\nkind: BGPPeer\nmetadata:\n  name: my-global-peer\nspec:\n  peerIP: 172.16.13.1\n  asNumber: 23000\nEOF\n</code></pre> <p>peerIP is the IP address of BGP Router</p> <p>asNumber is the AS number of BGP Router</p> <p>Check if the BGP session is established:</p> <pre><code>[root@master1 ~]# calicoctl node status\nCalico process is running.\n\nIPv4 BGP status\n+--------------+-----------+-------+------------+-------------+\n| PEER ADDRESS | PEER TYPE | STATE |   SINCE    |    INFO     |\n+--------------+-----------+-------+------------+-------------+\n| 172.16.13.1  | global    | up    | 2023-03-15 | Established |\n+--------------+-----------+-------+------------+-------------+\n\nIPv6 BGP status\nNo IPv6 peers found.\n</code></pre> <p>For more information on Calico BGP configuration, refer to Calico BGP.</p> </li> </ol>"},{"location":"usage/get-started-calico/#create-a-calico-ip-pool-in-the-same-subnet","title":"Create a Calico IP pool in the same subnet","text":"<p>Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet:</p> <pre><code>[root@master1 ~]# cat &lt;&lt; EOF | calicoctl apply -f -\napiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\n  name: spiderpool-subnet\nspec:\n  blockSize: 26\ncidr: 10.244.0.0/16\n  ipipMode: Never\n  natOutgoing: false\nnodeSelector: all()\nvxlanMode: Never\nEOF\n</code></pre> <p>The CIDR needs to correspond to the subnet of Spiderpool: <code>10.244.0.0/16</code></p> <p>Set ipipMode and vxlanMode to: Never</p>"},{"location":"usage/get-started-calico/#switch-calicos-ipam-to-spiderpool","title":"Switch Calico's <code>IPAM</code> to Spiderpool","text":"<p>Change the Calico configMap: <code>calico-config</code>, switch the ipam field to spiderpool and restart calico-node :</p> <pre><code>[root@master1 ~]# kubectl edit cm -n kube-system calico-config \napiVersion: v1\ndata:\n  calico_backend: bird\n  cni_network_config: |-\n    {\n...\n        \"ipam\": {\n-         \"type\": \"calico-ipam\"\n+         \"type\": \"spiderpool\"\n},\n      ...\n    }\n</code></pre> <p>restart calico-node and check if the ipam field is spiderpool:</p> <pre><code>[root@master1 ~]# kubectl delete po -n kube-system -l k8s-app=calico-node\n[root@master1 ~]# # wait calico-node ready\n[root@master1 ~]# cat /etc/cni/net.d/10-calico.conflist | grep ipam -A1\n\"ipam\": {\n\"type\": \"spiderpool\",\n</code></pre>"},{"location":"usage/get-started-calico/#create-applications","title":"Create applications","text":"<p>Take the Nginx application as an example:</p> <pre><code>[root@master1 ~]# cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\nselector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: '{\"ipv4\":[\"nginx-subnet-v4\"]}'\nipam.spidernet.io/ippool-ip-number: '+3'\nlabels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\nprotocol: TCP\nEOF\n</code></pre> <p><code>ipam.spidernet.io/subnet</code>: Assign static IPs from \"nginx-subnet-v4\" SpiderSubnet</p> <p><code>ipam.spidernet.io/ippool-ip-number</code>: '+3' means the number of static IPs allocated to the application is three more than the number of its replicas, guaranteeing available temporary IPs during application rolling updates.</p> <p>When a Pod is created, Spiderpool automatically creates an IP pool named <code>auto-nginx-v4-eth0-452e737e5e12</code> from <code>subnet: nginx-subnet-v4</code> specified in the annotations and binds it to the Pod. The IP range is <code>10.244.100.90-10.244.100.95</code>, and the number of IPs in the pool is <code>5</code>:</p> <pre><code>[root@master1 ~]# kubectl get sp\nNAME                              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-nginx-v4-eth0-452e737e5e12   4         10.244.0.0/16   2                    5                false     false\n\n[root@master ~]# kubectl get sp auto-nginx-v4-eth0-452e737e5e12 -o jsonpath='{.spec.ips}' \n[\"10.244.100.90-10.244.100.95\"]\n</code></pre> <p>When the replicas restart, its IP is fixed within the IP pool range of <code>auto-nginx-v4-eth0-452e737e5e12</code>:</p> <pre><code>[root@master1 ~]# kubectl get po -o wide\nNAME                     READY   STATUS        RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES\nnginx-644659db67-szgcg   1/1     Running       0          23s     10.244.100.90    worker5   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-98rcg   1/1     Running       0          23s     10.244.100.92    master1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>When the replica count increases to <code>3</code>, the IP address of the new replica is still assigned from the automatic pool: <code>auto-nginx-v4-eth0-452e737e5e12(10.244.100.90-10.244.100.95)</code></p> <pre><code>[root@master1 ~]# kubectl scale deploy nginx --replicas 3  # scale pods\ndeployment.apps/nginx scaled\n\n[root@master1 ~]# kubectl get po -o wide\nNAME                     READY   STATUS        RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES\nnginx-644659db67-szgcg   1/1     Running       0          1m     10.244.100.90    worker5   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-98rcg   1/1     Running       0          1m     10.244.100.92    master1   &lt;none&gt;           &lt;none&gt;\nnginx-644659db67-brqdg   1/1     Running       0          10s    10.244.100.94    master1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Check if both <code>ALLOCATED-IP-COUNT</code> and <code>TOTAL-IP-COUNT</code> of the automatic pool <code>auto-nginx-v4-eth0-452e737e5e12</code> increase by 1:</p> <pre><code>[root@master1 ~]# kubectl get sp\nNAME                              VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-nginx-v4-eth0-452e737e5e12   4         10.244.0.0/16   3                    6                false     false\n</code></pre>"},{"location":"usage/get-started-calico/#conclusion","title":"Conclusion","text":"<p>The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.</p>"},{"location":"usage/get-started-kind-zh_CN/","title":"Kind Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u80fd\u5feb\u901f\u642d\u5efa\u4e00\u5957\u4ee5 Macvlan \u4e3a main CNI \u642d\u914d Multus\u3001Spiderpool \u7684 Kind \u96c6\u7fa4\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002</p>"},{"location":"usage/get-started-kind-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ul> <li>\u5df2\u5b89\u88c5 Go</li> </ul>"},{"location":"usage/get-started-kind-zh_CN/#kind-spiderpool","title":"Kind \u4e0a\u96c6\u7fa4\u90e8\u7f72 Spiderpool","text":"<ol> <li> <p>\u514b\u9686 Spiderpool \u4ee3\u7801\u4ed3\u5e93\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002</p> <pre><code>git clone https://github.com/spidernet-io/spiderpool.git &amp;&amp; cd spiderpool\n</code></pre> </li> <li> <p>\u6267\u884c <code>make dev-doctor</code>\uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\uff0c\u5982\u679c\u7f3a\u5c11\u7ec4\u4ef6\u4f1a\u4e3a\u60a8\u81ea\u52a8\u5b89\u88c5\u3002</p> </li> <li> <p>\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6 Spiderpool \u7684\u6700\u65b0\u955c\u50cf\u3002</p> <pre><code>~# LATEST_IMAGE_TAG=$(curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1)\n</code></pre> </li> <li> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa Kind \u96c6\u7fa4\uff0c\u5e76\u4e3a\u60a8\u5b89\u88c5 Multus\u3001Macvlan\u3001Spiderpool\u3002</p> <pre><code>~# make e2e_init -e TEST_IMAGE_TAG=$LATEST_IMAGE_TAG\n</code></pre> <p>\u6ce8\u610f\uff1a\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u907f\u514d\u62c9\u53d6 Spiderpool \u4e0e Multus \u955c\u50cf\u5931\u8d25\u3002</p> <pre><code>~# make e2e_init -e TEST_IMAGE_TAG=$LATEST_IMAGE_TAG -e SPIDERPOOL_REGISTER=ghcr.m.daocloud.io -e IMAGE_MULTUS_REPO=ghcr.m.daocloud.io\n</code></pre> </li> </ol>"},{"location":"usage/get-started-kind-zh_CN/#_2","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002</p> <pre><code>~# export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config\n</code></pre> <p>\u60a8\u53ef\u4ee5\u770b\u5230\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a</p> <pre><code>~# kubectl get nodes \nNAME                   STATUS   ROLES           AGE     VERSION\nspider-control-plane   Ready    control-plane   2m29s   v1.26.2\nspider-worker          Ready    &lt;none&gt;          2m58s   v1.26.2\n\n~# kubectll get po -n kube-sysem | grep spiderpool\nspiderpool-agent-fmx74                         1/1     Running   0               4m26s\nspiderpool-agent-jzfh8                         1/1     Running   0               4m26s\nspiderpool-controller-79fcd4d75f-n9kmd         1/1     Running   0               4m25s\nspiderpool-controller-79fcd4d75f-scw2v         1/1     Running   0               4m25s\nspiderpool-init                                1/1     Running   0               4m26s\n\n~# kubectl get spidersubnet\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\ndefault-v4-subnet   4         172.18.0.0/16             253                  253\ndefault-v6-subnet   6         fc00:f853:ccd:e793::/64   253                  253\n\n~# kubectl get spiderippool\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\ndefault-v4-ippool   4         172.18.0.0/16             5                    253              true      false\ndefault-v6-ippool   6         fc00:f853:ccd:e793::/64   5                    253              true      false\n</code></pre> <p>Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a</p> <pre><code>~# kubectl get po -l app=test-pod -o wide\nNAME                       READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES\ntest-pod-856f9689d-876nm   1/1     Running   0          5m34s   172.18.40.63   spider-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>\u60a8\u4e5f\u53ef\u4ee5\u624b\u52a8\u521b\u5efa\u5e94\u7528\u9a8c\u8bc1 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\nEOF\n</code></pre> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                        READY   STATUS    RESTARTS   AGE     IP              NODE                   NOMINATED NODE   READINESS GATES\ntest-app-84d5699474-dbtl5   1/1     Running   0          6m23s   172.18.40.112   spider-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>\u901a\u8fc7\u6d4b\u8bd5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u5b83\u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684\u66f4\u591a\u529f\u80fd\uff0c\u8bf7\u53c2\u9605 \u5feb\u901f\u542f\u52a8\u3002</p>"},{"location":"usage/get-started-kind-zh_CN/#_3","title":"\u5378\u8f7d","text":"<ul> <li> <p>\u5378\u8f7d Kind \u96c6\u7fa4</p> <p>\u6267\u884c <code>make clean</code> \u5378\u8f7d Kind \u96c6\u7fa4\u3002</p> </li> <li> <p>\u5220\u9664\u6d4b\u8bd5\u955c\u50cf</p> <pre><code>~# docker rmi -f $(docker images | grep spiderpool | awk '{print $3}') \n~# docker rmi -f $(docker images | grep multus | awk '{print $3}')\n</code></pre> </li> </ul>"},{"location":"usage/get-started-kind/","title":"Kind Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. Spiderpool provides a script for installing a Kind cluster, which allows you to quickly build a set of Kind clusters with Macvlan as the main CNI and Multus and Spiderpool, which you can use to test and experience Spiderpool.</p>"},{"location":"usage/get-started-kind/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go has already been installed.</li> </ul>"},{"location":"usage/get-started-kind/#deploying-spiderpool-on-a-kind-cluster","title":"Deploying Spiderpool on a Kind cluster","text":"<ol> <li> <p>Clone the Spiderpool code repository to the local host and go to the root directory of the Spiderpool project.</p> <pre><code>~# git clone https://github.com/spidernet-io/spiderpool.git &amp;&amp; cd spiderpool\n</code></pre> </li> <li> <p>Execute <code>make dev-doctor</code> to check that the development tools on the local host meet the conditions for deploying a Kind cluster with Spiderpool, and that the components are automatically installed for you if they are missing.</p> </li> <li> <p>Get the latest image of Spiderpool.</p> <pre><code>~# LATEST_IMAGE_TAG=$(curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1)\n</code></pre> </li> <li> <p>Execute the following command to create a Kind cluster and install Multus, Macvlan, Spiderpool for you.</p> <pre><code>~# make e2e_init -e TEST_IMAGE_TAG=$LATEST_IMAGE_TAG\n</code></pre> <p>Note: If you are mainland user who is not available to access ghcr.io, you can use the following command to avoid failures in pulling Spiderpool and Multus images.</p> <pre><code>~# make e2e_init -e TEST_IMAGE_TAG=$LATEST_IMAGE_TAG -e SPIDERPOOL_REGISTER=ghcr.m.daocloud.io -e IMAGE_MULTUS_REPO=ghcr.m.daocloud.io\n</code></pre> </li> </ol>"},{"location":"usage/get-started-kind/#check-that-everything-is-working","title":"Check that everything is working","text":"<p>Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl.</p> <pre><code>~# export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config\n</code></pre> <p>It should be possible to observe the following:</p> <pre><code>~# kubectl get nodes \nNAME                   STATUS   ROLES           AGE     VERSION\nspider-control-plane   Ready    control-plane   2m29s   v1.26.2\nspider-worker          Ready    &lt;none&gt;          2m58s   v1.26.2\n\n~# kubectll get po -n kube-sysem | grep spiderpool\nspiderpool-agent-fmx74                         1/1     Running   0               4m26s\nspiderpool-agent-jzfh8                         1/1     Running   0               4m26s\nspiderpool-controller-79fcd4d75f-n9kmd         1/1     Running   0               4m25s\nspiderpool-controller-79fcd4d75f-scw2v         1/1     Running   0               4m25s\nspiderpool-init                                1/1     Running   0               4m26s\n\n~# kubectl get spidersubnet\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\ndefault-v4-subnet   4         172.18.0.0/16             253                  253\ndefault-v6-subnet   6         fc00:f853:ccd:e793::/64   253                  253\n\n~# kubectl get spiderippool\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\ndefault-v4-ippool   4         172.18.0.0/16             5                    253              true      false\ndefault-v6-ippool   6         fc00:f853:ccd:e793::/64   5                    253              true      false\n</code></pre> <p>The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application:</p> <pre><code>~# kubectl get po -l app=test-pod -o wide\nNAME                       READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES\ntest-pod-856f9689d-876nm   1/1     Running   0          5m34s   172.18.40.63   spider-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>You can also manually create an application to verify that the cluster is available, the following command will create 1 copy of Deployment:</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\nEOF\n</code></pre> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                        READY   STATUS    RESTARTS   AGE     IP              NODE                   NOMINATED NODE   READINESS GATES\ntest-app-84d5699474-dbtl5   1/1     Running   0          6m23s   172.18.40.112   spider-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>As tested, everything works fine with the Kind cluster. You can test and experience more features of Spiderpool based on kind clusters, see Quick start.</p>"},{"location":"usage/get-started-kind/#uninstall","title":"Uninstall","text":"<ul> <li> <p>Uninstall a Kind cluster</p> <p>Execute <code>make clean</code> to uninstall the Kind cluster.</p> </li> <li> <p>Delete test's images</p> <pre><code>~# docker rmi -f $(docker images | grep spiderpool | awk '{print $3}') \n~# docker rmi -f $(docker images | grep multus | awk '{print $3}')\n</code></pre> </li> </ul>"},{"location":"usage/get-started-macvlan-zh_CN/","title":"Macvlan Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus\u3001Macvlan\u3001Veth\u3001Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a</p> <ul> <li> <p>\u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740</p> </li> <li> <p>Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51</p> </li> <li> <p>Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1</p> </li> </ul>"},{"location":"usage/get-started-macvlan-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ol> <li> <p>\u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4</p> </li> <li> <p>\u5df2\u5b89\u88c5 Helm</p> </li> </ol>"},{"location":"usage/get-started-macvlan-zh_CN/#macvlan","title":"\u5b89\u88c5 Macvlan","text":"<p><code>Macvlan</code> \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u80fd\u591f\u4e3a Pod \u5206\u914d Macvlan \u865a\u62df\u7f51\u5361\uff0c\u53ef\u7528\u4e8e\u5bf9\u63a5 Underlay \u7f51\u7edc\u3002</p> <p>\u4e00\u4e9b Kubernetes \u5b89\u88c5\u5668\u9879\u76ee\uff0c\u9ed8\u8ba4\u5b89\u88c5\u4e86 Macvlan \u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u786e\u8ba4\u8282\u70b9\u4e0a\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/macvlan \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a</p> <pre><code>~# wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz \n\n~# tar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin\n\n~# chmod +x /opt/cni/bin/macvlan\n</code></pre>"},{"location":"usage/get-started-macvlan-zh_CN/#veth","title":"\u5b89\u88c5 Veth","text":"<p><code>Veth</code> \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u4e00\u4e9b CNI \uff08\u4f8b\u5982 Macvlan\u3001SR-IOV \u7b49\uff09\u89e3\u51b3\u5982\u4e0b\u95ee\u9898\uff1a</p> <ul> <li> <p>\u5728 Macvlan CNI \u573a\u666f\u4e0b\uff0c\u5e2e\u52a9 Pod \u5b9e\u73b0 clusterIP \u901a\u4fe1</p> </li> <li> <p>\u82e5 Pod \u7684 Macvlan IP \u4e0d\u80fd\u4e0e\u672c\u5730\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u4f1a\u5f71\u54cd Pod \u7684\u5065\u5eb7\u68c0\u6d4b\u3002Veth \u63d2\u4ef6\u80fd\u591f\u5e2e\u52a9 Pod \u4e0e\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u89e3\u51b3\u5065\u5eb7\u68c0\u6d4b\u573a\u666f\u4e0b\u7684\u8054\u901a\u6027\u95ee\u9898</p> </li> <li> <p>\u5728 Pod \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cVeth \u80fd\u81ea\u52a8\u591f\u534f\u8c03\u591a\u7f51\u5361\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u89e3\u51b3\u591a\u7f51\u5361\u901a\u4fe1\u95ee\u9898</p> </li> </ul> <p>\u8bf7\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\uff0c\u4e0b\u8f7d\u5b89\u88c5 Veth \u4e8c\u8fdb\u5236\uff1a</p> <pre><code>~# wget https://github.com/spidernet-io/plugins/releases/download/v0.1.4/spider-plugins-linux-amd64-v0.1.4.tar\n\n~# tar xvfzp ./spider-plugins-linux-amd64-v0.1.4.tar -C /opt/cni/bin\n\n~# chmod +x /opt/cni/bin/veth\n</code></pre>"},{"location":"usage/get-started-macvlan-zh_CN/#multus","title":"\u5b89\u88c5 Multus","text":"<p><code>Multus</code> \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14\uff0cMultus \u63d0\u4f9b\u4e86 CRD \u65b9\u5f0f\u7ba1\u7406 Macvlan \u7684 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\u3002</p> <ol> <li> <p>\u901a\u8fc7 manifest \u5b89\u88c5 Multus \u7684\u7ec4\u4ef6\u3002</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>\u786e\u8ba4 Multus \u8fd0\u7ef4\u72b6\u6001\uff1a</p> <pre><code>~# kubectl get pods -A | grep -i multus\nkube-system          kube-multus-ds-hfzpl                         1/1     Running   0   5m\nkube-system          kube-multus-ds-qm8j7                         1/1     Running   0   5m\n</code></pre> <p>\u786e\u8ba4\u8282\u70b9\u4e0a\u5b58\u5728 Multus \u7684\u914d\u7f6e\u6587\u4ef6 <code>ls /etc/cni/net.d/00-multus.conf</code></p> </li> <li> <p>\u4e3a Macvlan \u521b\u5efa Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\u3002</p> <p>\u9700\u8981\u786e\u8ba4\u5982\u4e0b\u53c2\u6570\uff1a</p> <ul> <li> <p>\u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528</p> </li> <li> <p>\u4e3a\u4f7f\u7528 Veth \u63d2\u4ef6\u6765\u5b9e\u73b0 clusterIP \u901a\u4fe1\uff0c\u9700\u786e\u8ba4\u96c6\u7fa4\u7684 service CIDR\uff0c\u4f8b\u5982\u53ef\u57fa\u4e8e\u547d\u4ee4 <code>kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service</code> \u67e5\u8be2</p> </li> </ul> <p>\u4ee5\u4e0b\u4e3a\u521b\u5efa NetworkAttachmentDefinition \u7684\u914d\u7f6e\uff1a</p> <pre><code>MACVLAN_MASTER_INTERFACE=\"eth0\"\nSERVICE_CIDR=\"10.96.0.0/16\"\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"macvlan-conf\",\n        \"plugins\": [\n            {\n                \"type\": \"macvlan\",\n                \"master\": \"${MACLVAN_MASTER_INTERFACE}\",\n                \"mode\": \"bridge\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            },{\n                  \"type\": \"veth\",\n                  \"service_cidr\": [\"${SERVICE_CIDR}\"]\n              }\n        ]\n    }\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-macvlan-zh_CN/#spiderpool","title":"\u5b89\u88c5 Spiderpool","text":"<ol> <li> <p>\u5b89\u88c5 Spiderpool\u3002</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\n\nhelm repo update spiderpool\n\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 <code>--set global.imageRegistryOverride=ghcr.m.daocloud.io</code> \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002</p> </li> <li> <p>\u521b\u5efa SpiderSubnet \u5b9e\u4f8b\u3002</p> <p>Macvlan \u662f\u4ee5\u5bbf\u4e3b\u673a eth0 \u4e3a\u7236\u63a5\u53e3\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u521b\u5efa eth0 \u5e95\u5c42\u7684 Underlay \u5b50\u7f51\u4f9b Pod \u4f7f\u7528\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderSubnet \u793a\u4f8b\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ips:\n  - \"172.18.30.131-172.18.30.140\"\n  subnet: 172.18.0.0/16\n  gateway: 172.18.0.1\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-macvlan-zh_CN/#_2","title":"\u521b\u5efa\u5e94\u7528","text":"<ol> <li> <p>\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/macvlan-conf\n      labels:\n        app: test-app\n    spec:\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-app-svc\n  labels:\n    app: test-app\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: test-app \nEOF\n</code></pre> <p>\u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a * <code>ipam.spidernet.io/subnet</code>\uff1a\u8be5 annotation \u6307\u5b9a\u4f7f\u7528\u54ea\u4e2a subnet \u5206\u914d IP \u5730\u5740\u7ed9 Pod     &gt; \u66f4\u591a Spiderpool \u6ce8\u89e3\u7684\u4f7f\u7528\u8bf7\u53c2\u8003 Spiderpool \u6ce8\u89e3\u3002</p> <ul> <li><code>v1.multus-cni.io/default-network</code>\uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002     &gt; \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3\u3002</li> </ul> </li> <li> <p>\u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a</p> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP              NODE                 NOMINATED NODE   READINESS GATES\ntest-app-f9f94688-2srj7   1/1     Running   0          2m13s   172.18.30.139   ipv4-worker          &lt;none&gt;           &lt;none&gt;\ntest-app-f9f94688-8982v   1/1     Running   0          2m13s   172.18.30.138   ipv4-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u521b\u5efa\u4e86 IP \u56fa\u5b9a\u6c60\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a</p> <pre><code>~# kubectl get spiderippool\nNAME                                               VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nauto-deployment-default-test-app-v4-a0ae75eb5d47   4         172.18.0.0/16   2                    2                false\n\n~#  kubectl get spiderendpoints\nNAME                      INTERFACE   IPV4POOL                                           IPV4               IPV6POOL   IPV6   NODE                 CREATETION TIME\ntest-app-f9f94688-2srj7   eth0        auto-deployment-default-test-app-v4-a0ae75eb5d47   172.18.30.139/16                     ipv4-worker          3m5s\ntest-app-f9f94688-8982v   eth0        auto-deployment-default-test-app-v4-a0ae75eb5d47   172.18.30.138/16                     ipv4-control-plane   3m5s\n</code></pre> </li> <li> <p>\u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a</p> <pre><code>~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172.18.30.138 -c 2\n\nPING 172.18.30.138 (172.18.30.138): 56 data bytes\n64 bytes from 172.18.30.138: seq=0 ttl=64 time=1.524 ms\n64 bytes from 172.18.30.138: seq=1 ttl=64 time=0.194 ms\n\n--- 172.18.30.138 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.194/0.859/1.524 ms\n</code></pre> </li> <li> <p>\u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a</p> <ul> <li> <p>\u67e5\u770b service \u7684 IP\uff1a</p> <pre><code>~# kubectl get service\n\nNAME           TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nkubernetes     ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP   20h\ntest-app-svc   ClusterIP   10.96.190.4   &lt;none&gt;        80/TCP    109m\n</code></pre> </li> <li> <p>Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a</p> <pre><code>~# kubectl exec -ti  test-app-85cf87dc9c-7dm7m -- curl 10.96.190.4:80 -I\n\nHTTP/1.1 200 OK\nServer: nginx/1.23.1\nDate: Thu, 23 Mar 2023 05:01:04 GMT\nContent-Type: text/html\nContent-Length: 4055\nLast-Modified: Fri, 23 Sep 2022 02:53:30 GMT\nConnection: keep-alive\nETag: \"632d1faa-fd7\"\nAccept-Ranges: bytes\n</code></pre> </li> </ul> </li> </ol>"},{"location":"usage/get-started-macvlan/","title":"Macvlan Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus, Macvlan, Veth, and Spiderpool, which meets the following kinds of requirements:</p> <ul> <li> <p>Applications can be assigned static Underlay IP addresses through simple operations.</p> </li> <li> <p>Pods with multiple Underlay NICs connect to multiple Underlay subnets.</p> </li> <li> <p>Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.</p> </li> </ul>"},{"location":"usage/get-started-macvlan/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Make sure a Kubernetes cluster is ready.</p> </li> <li> <p>Helm has been already installed.</p> </li> </ol>"},{"location":"usage/get-started-macvlan/#install-macvlan","title":"Install Macvlan","text":"<p><code>Macvlan</code> is a CNI plugin that allows pods to be assigned Macvlan virtual NICs for connecting to Underlay networks.</p> <p>Some Kubernetes installers include the Macvlan binary file by default that can be found at \"/opt/cni/bin/macvlan\" on your nodes. If the binary file is missing, you can download and install it on all nodes using the following command:</p> <pre><code>wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz \ntar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin\nchmod +x /opt/cni/bin/macvlan\n</code></pre>"},{"location":"usage/get-started-macvlan/#install-veth","title":"Install Veth","text":"<p><code>Veth</code> is a CNI plugin designed to resolve the following issues in other CNIs like Macvlan and SR-IOV:</p> <ul> <li> <p>Enable clusterIP communication for Pods in Macvlan CNI environments</p> </li> <li> <p>Facilitate connection between Pods and the host during health checks where Pods' Macvlan IPs are unable to communicate with the host</p> </li> <li> <p>Address communication issues in multiple NICs for Pods by automatically coordinating policy routing between NICs</p> </li> </ul> <p>Download and install the Veth binary on all nodes:</p> <pre><code>wget https://github.com/spidernet-io/plugins/releases/download/v0.1.4/spider-plugins-linux-amd64-v0.1.4.tar\ntar xvfzp ./spider-plugins-linux-amd64-v0.1.4.tar -C /opt/cni/bin\nchmod +x /opt/cni/bin/veth\n</code></pre>"},{"location":"usage/get-started-macvlan/#install-multus","title":"Install Multus","text":"<p><code>Multus</code> is a CNI plugin that allows Pods to have multiple NICs by scheduling third-party CNIs. The management of the Macvlan CNI configuration is simplified through the CRD-based approach provided by Multus, with nothing for manual editing of CNI configuration files on each host.</p> <ol> <li> <p>Install Multus via the manifest:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>Confirm the operational status of Multus:</p> <pre><code>~# kubectl get pods -A | grep -i multus\nkube-system          kube-multus-ds-hfzpl                         1/1     Running   0   5m\nkube-system          kube-multus-ds-qm8j7                         1/1     Running   0   5m\n</code></pre> <p>Verify the existence of the Multus configuration files <code>ls /etc/cni/net.d/00-multus.conf</code> on the node.</p> </li> <li> <p>Create a NetworkAttachmentDefinition for Macvlan.</p> <p>The following parameters need to be confirmed:</p> <ul> <li> <p>Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0.</p> </li> <li> <p>To implement clusterIP communication using Veth, confirm the service CIDR of the cluster through a query command <code>kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service</code> or other methods.</p> </li> </ul> <p>The following is the configuration for creating a NetworkAttachmentDefinition:</p> <pre><code>MACLVAN_MASTER_INTERFACE=\"eth0\"\nSERVICE_CIDR=\"10.96.0.0/16\"\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"macvlan-conf\",\n        \"plugins\": [\n            {\n                \"type\": \"macvlan\",\n                \"master\": \"${MACLVAN_MASTER_INTERFACE}\",\n                \"mode\": \"bridge\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            },{\n                  \"type\": \"veth\",\n                  \"service_cidr\": [\"${SERVICE_CIDR}\"]\n              }\n        ]\n    }\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-macvlan/#install-spiderpool","title":"Install Spiderpool","text":"<ol> <li> <p>Install Spiderpool.</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm repo update spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter <code>-set global.imageRegistryOverride=ghcr.m.daocloud.io</code> to avoid image pulling failures for Spiderpool.</p> </li> <li> <p>Create a SpiderSubnet instance.</p> <p>An Underlay subnet for eth0 needs to be created for Pods as Macvlan uses eth0 as the parent interface. Here is an example of creating a SpiderSubnet instance:\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ips:\n  - \"172.18.30.131-172.18.30.140\"\n  subnet: 172.18.0.0/16\n  gateway: 172.18.0.1\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-macvlan/#create-applications","title":"Create applications","text":"<ol> <li> <p>Create test Pods and service via the command below\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/macvlan-conf\n      labels:\n        app: test-app\n    spec:\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-app-svc\n  labels:\n    app: test-app\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: test-app \nEOF\n</code></pre> <p>Spec descriptions\uff1a</p> <ul> <li> <p><code>ipam.spidernet.io/subnet</code>: defines which subnets to be used to assign IP addresses to Pods.</p> <p>For more information on Spiderpool annotations, refer to Spiderpool Annotations.</p> </li> <li> <p><code>v1.multus-cni.io/default-network</code>\uff1aspecifies the CNI configuration for Multus.</p> <p>For more information on Multus annotations, refer to Multus Quickstart.</p> </li> </ul> </li> <li> <p>Check the status of Pods:</p> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP              NODE                 NOMINATED NODE   READINESS GATES\ntest-app-f9f94688-2srj7   1/1     Running   0          2m13s   172.18.30.139   ipv4-worker          &lt;none&gt;           &lt;none&gt;\ntest-app-f9f94688-8982v   1/1     Running   0          2m13s   172.18.30.138   ipv4-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges.</p> <pre><code>~# kubectl get spiderippool\nNAME                                               VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nauto-deployment-default-test-app-v4-a0ae75eb5d47   4         172.18.0.0/16   2                    2                false\n\n~#  kubectl get spiderendpoints\nNAME                      INTERFACE   IPV4POOL                                           IPV4               IPV6POOL   IPV6   NODE                 CREATETION TIME\ntest-app-f9f94688-2srj7   eth0        auto-deployment-default-test-app-v4-a0ae75eb5d47   172.18.30.139/16                     ipv4-worker          3m5s\ntest-app-f9f94688-8982v   eth0        auto-deployment-default-test-app-v4-a0ae75eb5d47   172.18.30.138/16                     ipv4-control-plane   3m5s\n</code></pre> </li> <li> <p>Test the communication between Pods:</p> <pre><code>~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172.18.30.138 -c 2\n\nPING 172.18.30.138 (172.18.30.138): 56 data bytes\n64 bytes from 172.18.30.138: seq=0 ttl=64 time=1.524 ms\n64 bytes from 172.18.30.138: seq=1 ttl=64 time=0.194 ms\n\n--- 172.18.30.138 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.194/0.859/1.524 ms\n</code></pre> </li> <li> <p>Test the communication between Pods and service IP:</p> <pre><code>~# kubectl get service\n\nNAME           TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nkubernetes     ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP   20h\ntest-app-svc   ClusterIP   10.96.190.4   &lt;none&gt;        80/TCP    109m\n\n~# kubectl exec -ti  test-app-85cf87dc9c-7dm7m -- curl 10.96.190.4:80 -I\n\nHTTP/1.1 200 OK\nServer: nginx/1.23.1\nDate: Thu, 23 Mar 2023 05:01:04 GMT\nContent-Type: text/html\nContent-Length: 4055\nLast-Modified: Fri, 23 Sep 2022 02:53:30 GMT\nConnection: keep-alive\nETag: \"632d1faa-fd7\"\nAccept-Ranges: bytes\n</code></pre> </li> </ol>"},{"location":"usage/get-started-ovs-zh_CN/","title":"Ovs-cni Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus\u3001Ovs-cni \u3001Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002</p>"},{"location":"usage/get-started-ovs-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ol> <li> <p>\u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4</p> </li> <li> <p>Helm \u5de5\u5177</p> </li> <li> <p>\u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003\u5b98\u65b9\u5b89\u88c5\u8bf4\u660e</p> <p>\u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002</p> <pre><code>~# sudo apt-get install -y openvswitch-switch\n~# sudo systemctl start openvswitch-switch\n</code></pre> </li> </ol>"},{"location":"usage/get-started-ovs-zh_CN/#ovs-cni","title":"\u5b89\u88c5 Ovs-cni","text":"<p><code>ovs-cni</code> \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002</p> <p>\u786e\u8ba4\u8282\u70b9\u4e0a\u662f\u5426\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/ovs \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a</p> <pre><code>~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin\n\n~# mv ./plugin /opt/cni/bin/ovs\n\n~# chmod +x /opt/cni/bin/ovs\n</code></pre> <p>Ovs-cni \u4e0d\u4f1a\u914d\u7f6e\u7f51\u6865\uff0c\u7531\u7528\u6237\u521b\u5efa\u5b83\u4eec\uff0c\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u5230 L2\u3001L3 \u7f51\u7edc\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u7f51\u6865\u7684\u793a\u4f8b\uff0c\u8bf7\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\uff1a</p> <ol> <li> <p>\u521b\u5efa Open vSwitch \u7f51\u6865\u3002</p> <pre><code>~# ovs-vsctl add-br br1\n</code></pre> </li> <li> <p>\u7f51\u7edc\u63a5\u53e3\u8fde\u63a5\u5230\u7f51\u6865</p> <p>\u6b64\u8fc7\u7a0b\u53d6\u51b3\u4e8e\u60a8\u7684\u5e73\u53f0\uff0c\u4ee5\u4e0b\u547d\u4ee4\u53ea\u662f\u793a\u4f8b\u8bf4\u660e\uff0c\u5b83\u53ef\u80fd\u4f1a\u7834\u574f\u60a8\u7684\u7cfb\u7edf\u3002\u9996\u5148\u4f7f\u7528 <code>ip link show</code> \u67e5\u8be2\u4e3b\u673a\u7684\u53ef\u7528\u63a5\u53e3\uff0c\u793a\u4f8b\u4e2d\u4f7f\u7528\u4e3b\u673a\u4e0a\u7684\u63a5\u53e3\uff1a<code>eth0</code> \u4e3a\u4f8b\u3002 </p> <pre><code>~# ovs-vsctl add-port br1 eth0\n~# ip addr add &lt;IP\u5730\u5740&gt;/&lt;\u5b50\u7f51\u63a9\u7801&gt; dev br1\n~# ip link set br1 up\n~# ip route add default via &lt;\u9ed8\u8ba4\u7f51\u5173IP&gt; dev br1\n</code></pre> </li> </ol> <p>\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a</p> <pre><code>~# ovs-vsctl show\nec16d9e1-6187-4b21-9c2f-8b6cb75434b9\n    Bridge br1\n        Port eth0\n            Interface eth0\n        Port br1\n            Interface br1\n                type: internal\n        Port veth97fb4795\n            Interface veth97fb4795\n    ovs_version: \"2.17.3\"\n</code></pre>"},{"location":"usage/get-started-ovs-zh_CN/#multus","title":"\u5b89\u88c5 Multus","text":"<p><code>Multus</code> \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14 Multus \u63d0\u4f9b\u4e86 CRD \u65b9\u5f0f\u7ba1\u7406 Ovs-cni \u7684 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\uff0c\u80fd\u591f\u964d\u4f4e\u8fd0\u7ef4\u5de5\u4f5c\u91cf\u3002</p> <ol> <li> <p>\u901a\u8fc7 manifest \u5b89\u88c5 Multus</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>\u4e3a Ovs-cni \u521b\u5efa Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e</p> <p>\u9700\u8981\u786e\u8ba4\u5982\u4e0b\u53c2\u6570\uff1a</p> <ul> <li>\u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7f51\u6865\uff0c\u4f8b\u5982\u53ef\u57fa\u4e8e\u547d\u4ee4 <code>ovs-vsctl show</code> \u67e5\u8be2\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a\u7684\u7f51\u6865\uff1a<code>br1</code> \u4e3a\u4f8b</li> </ul> </li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ovs-conf\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"ovs-conf\",\n        \"plugins\": [\n            {\n                \"type\": \"ovs\",\n                \"bridge\": \"br1\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            }\n        ]\n    }\nEOF\n</code></pre>"},{"location":"usage/get-started-ovs-zh_CN/#spiderpool","title":"\u5b89\u88c5 Spiderpool","text":"<ol> <li> <p>\u5b89\u88c5 Spiderpool\u3002</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm repo update spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 <code>--set global.imageRegistryOverride=ghcr.m.daocloud.io</code> \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002</p> </li> <li> <p>\u521b\u5efa SpiderSubnet \u5b9e\u4f8b\u3002</p> <p>Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002</p> <p>\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderSubnet \u793a\u4f8b\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ipVersion: 4\n  ips:\n    - \"172.18.30.131-172.18.30.140\"\n  subnet: 172.18.0.0/16\n  gateway: 172.18.0.1\nEOF\n</code></pre> </li> </ol> <p>\u9a8c\u8bc1\u5b89\u88c5\uff1a</p> <pre><code>~# kubectl get po -n kube-system | grep spiderpool\nspiderpool-agent-f899f                       1/1     Running   0             2m\nspiderpool-agent-w69z6                       1/1     Running   0             2m\nspiderpool-controller-5bf7b5ddd9-6vd2w       1/1     Running   0             2m\n~# kubectl get spidersubnet\nNAME          VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nsubnet-test   4         172.18.0.0/16   0                    10\n</code></pre>"},{"location":"usage/get-started-ovs-zh_CN/#_2","title":"\u521b\u5efa\u5e94\u7528","text":"<p>\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a</p> <ul> <li> <p><code>ipam.spidernet.io/subnet</code>\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u80fd\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002</p> </li> <li> <p><code>v1.multus-cni.io/default-network</code>\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002</p> </li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/ovs-conf\n      labels:\n        app: test-app\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app: test-app\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\nEOF\n</code></pre> <p>Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u521b\u5efa\u4e86 IP \u56fa\u5b9a\u6c60\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a</p> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                        READY   STATUS    RESTARTS   AGE     IP              NODE                 NOMINATED NODE   READINESS GATES\ntest-app-6f8dddd88d-hstg7   1/1     Running   0          3m37s   172.18.30.131   ipv4-worker          &lt;none&gt;           &lt;none&gt;\ntest-app-6f8dddd88d-rj7sm   1/1     Running   0          3m37s   172.18.30.132   ipv4-control-plane   &lt;none&gt;           &lt;none&gt;\n\n~# kubectl get spiderippool\nNAME                                 VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-test-app-v4-eth0-9b208a961acd   4         172.18.0.0/16   2                    2                false     false\n\n~#  kubectl get spiderendpoints\nNAME                        INTERFACE   IPV4POOL                             IPV4               IPV6POOL   IPV6   NODE\ntest-app-6f8dddd88d-hstg7   eth0        auto-test-app-v4-eth0-9b208a961acd   172.18.30.131/16                     ipv4-worker\ntest-app-6f8dddd88d-rj7sm   eth0        auto-test-app-v4-eth0-9b208a961acd   172.18.30.132/16                     ipv4-control-plane\n</code></pre> <p>\u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a</p> <pre><code>~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172.18.30.132 -c 2\n\nPING 172.18.30.132 (172.18.30.132): 56 data bytes\n64 bytes from 172.18.30.132: seq=0 ttl=64 time=1.882 ms\n64 bytes from 172.18.30.132: seq=1 ttl=64 time=0.195 ms\n\n--- 172.18.30.132 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.195/1.038/1.882 ms\n</code></pre>"},{"location":"usage/get-started-ovs/","title":"Ovs-cni Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus, Ovs-cni, and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster.</p>"},{"location":"usage/get-started-ovs/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Make sure a multi-node Kubernetes cluster is ready.</p> </li> <li> <p>Helm has been already installed.</p> </li> <li> <p>Open vSwitch must be installed and running on the host. It could refer to Installation.</p> <p>The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system.</p> <pre><code>~# sudo apt-get install -y openvswitch-switch\n~# sudo systemctl start openvswitch-switch\n</code></pre> </li> </ol>"},{"location":"usage/get-started-ovs/#install-ovs-cni","title":"Install Ovs-cni","text":"<p><code>ovs-cni</code> is a Kubernetes CNI plugin based on Open vSwitch (OVS) that provides a way to use OVS for network virtualization in a Kubernetes cluster in a Kubernetes cluster.</p> <p>Verify that the binary /opt/cni/bin/ovs exists on the node. if the binary is missing, you can download it and install it on all nodes using the following command:</p> <pre><code>~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin\n\n~# mv ./plugin /opt/cni/bin/ovs\n\n~# chmod +x /opt/cni/bin/ovs\n</code></pre> <p>Note: Ovs-cni does not configure bridges, it is up to the user to create them and connect them to L2, L3, The following is an example of creating a bridge, to be executed on each node:</p> <ol> <li> <p>Create an Open vSwitch bridge.</p> <pre><code>~# ovs-vsctl add-br br1\n</code></pre> </li> <li> <p>Network interface connected to the bridge\uff1a</p> <p>This procedure depends on your platform, the following commands are only example instructions and it may break your system. First use <code>ip link show</code> to query the host for available interfaces, the example uses the interface on the host: <code>eth0</code> as an example. </p> <pre><code>~# ovs-vsctl add-port br1 eth0\n~# ip addr add &lt;IP\u5730\u5740&gt;/&lt;\u5b50\u7f51\u63a9\u7801&gt; dev br1\n~# ip link set br1 up\n~# ip route add default via &lt;\u9ed8\u8ba4\u7f51\u5173IP&gt; dev br1\n</code></pre> </li> </ol> <p>Once created, the following bridge information can be viewed on each node:</p> <pre><code>~# ovs-vsctl show\nec16d9e1-6187-4b21-9c2f-8b6cb75434b9\n    Bridge br1\n        Port eth0\n            Interface eth0\n        Port br1\n            Interface br1\n                type: internal\n        Port veth97fb4795\n            Interface veth97fb4795\n    ovs_version: \"2.17.3\"\n</code></pre>"},{"location":"usage/get-started-ovs/#install-multus","title":"Install Multus","text":"<p><code>Multus</code> is a CNI plugin that allows Pods to have multiple NICs by scheduling third-party CNIs. The management of the ovs-cni CNI configuration is simplified through the CRD-based approach provided by Multus, with nothing for manual editing of CNI configuration files on each host.</p> <ol> <li> <p>Install Multus via the manifest:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>Create a NetworkAttachmentDefinition for ovs-cni.</p> <p>The following parameters need to be confirmed:</p> <ul> <li>Confirm the required host bridge for ovs-cni, for example based on the command <code>ovs-vsctl show</code>, this example takes the host bridge: <code>br1</code> as an example.</li> </ul> </li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ovs-conf\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"ovs-conf\",\n        \"plugins\": [\n            {\n                \"type\": \"ovs\",\n                \"bridge\": \"br1\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            }\n        ]\n    }\nEOF\n</code></pre>"},{"location":"usage/get-started-ovs/#install-spiderpool","title":"Install Spiderpool","text":"<ol> <li> <p>Install Spiderpool</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm repo update spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter <code>-set global.imageRegistryOverride=ghcr.m.daocloud.io</code> to avoid image pulling failures for Spiderpool.</p> </li> <li> <p>Create a SpiderSubnet instance.</p> <p>The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed.</p> <p>Here is an example of creating a SpiderSubnet instance:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ipVersion: 4\n  ips:\n  - \"172.18.30.131-172.18.30.140\"\n  subnet: 172.18.0.0/16\n  gateway: 172.18.0.1\n  vlan: 0\nEOF\n</code></pre> </li> <li> <p>Verify the installation\uff1a</p> </li> </ol> <pre><code>~# kubectl get po -n kube-system | grep spiderpool\nspiderpool-agent-f899f                       1/1     Running   0             2m\nspiderpool-agent-w69z6                       1/1     Running   0             2m\nspiderpool-controller-5bf7b5ddd9-6vd2w       1/1     Running   0             2m\n~# kubectl get spidersubnet\nNAME          VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nsubnet-test   4         172.18.0.0/16   0                    10\n</code></pre>"},{"location":"usage/get-started-ovs/#create-applications","title":"Create applications","text":"<p>In the following example Yaml, 2 copies of the Deployment are created, of which:</p> <ul> <li> <p><code>ipam.spidernet.io/subnet</code>: used to specify the subnet of Spiderpool, Spiderpool will automatically select some random IPs in this subnet to create a fixed IP pool to bind with this application, which can achieve the effect of IP fixing.</p> </li> <li> <p><code>v1.multus-cni.io/default-network</code>: used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application.</p> </li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/ovs-conf\n      labels:\n        app: test-app\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app: test-app\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: test-app\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\nEOF\n</code></pre> <p>Spiderpool automatically creates a pool of IP fixes for the application and the application's IP will be automatically fixed to that IP range:</p> <pre><code>~# kubectl get po -l app=test-app -o wide\nNAME                        READY   STATUS    RESTARTS   AGE     IP              NODE                 NOMINATED NODE   READINESS GATES\ntest-app-6f8dddd88d-hstg7   1/1     Running   0          3m37s   172.18.30.131   ipv4-worker          &lt;none&gt;           &lt;none&gt;\ntest-app-6f8dddd88d-rj7sm   1/1     Running   0          3m37s   172.18.30.132   ipv4-control-plane   &lt;none&gt;           &lt;none&gt;\n\n~# kubectl get spiderippool\nNAME                                 VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-test-app-v4-eth0-9b208a961acd   4         172.18.0.0/16   2                    2                false     false\n\n~#  kubectl get spiderendpoints\nNAME                        INTERFACE   IPV4POOL                             IPV4               IPV6POOL   IPV6   NODE\ntest-app-6f8dddd88d-hstg7   eth0        auto-test-app-v4-eth0-9b208a961acd   172.18.30.131/16                     ipv4-worker\ntest-app-6f8dddd88d-rj7sm   eth0        auto-test-app-v4-eth0-9b208a961acd   172.18.30.132/16                     ipv4-control-plane\n</code></pre> <p>Testing Pod communication with cross-node Pods:</p> <pre><code>~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172.18.30.132 -c 2\n\nPING 172.18.30.132 (172.18.30.132): 56 data bytes\n64 bytes from 172.18.30.132: seq=0 ttl=64 time=1.882 ms\n64 bytes from 172.18.30.132: seq=1 ttl=64 time=0.195 ms\n\n--- 172.18.30.132 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.195/1.038/1.882 ms\n</code></pre>"},{"location":"usage/get-started-sriov-zh_CN/","title":"SRIOV Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus\u3001Sriov \u3001Veth\u3001Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a</p> <ul> <li> <p>\u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740</p> </li> <li> <p>Pod \u7684\u7f51\u5361\u5177\u6709 Sriov \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd</p> </li> <li> <p>Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1</p> </li> </ul>"},{"location":"usage/get-started-sriov-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ol> <li>\u4e00\u4e2a Kubernetes \u96c6\u7fa4</li> <li>Helm \u5de5\u5177</li> <li> <p>\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361</p> <ul> <li> <p>\u67e5\u8be2\u7f51\u5361 bus-info\uff1a</p> <pre><code>~# ethtool -i enp4s0f0np0 |grep bus-info\nbus-info: 0000:04:00.0\n</code></pre> </li> <li> <p>\u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 <code>Single Root I/O Virtualization (SR-IOV)</code> \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a</p> <pre><code>~# lspci -s 0000:04:00.0 -v |grep SR-IOV\nCapabilities: [180] Single Root I/O Virtualization (SR-IOV)      </code></pre> </li> </ul> </li> </ol>"},{"location":"usage/get-started-sriov-zh_CN/#veth","title":"\u5b89\u88c5 Veth","text":"<p><code>Veth</code> \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u4e00\u4e9b CNI \uff08\u4f8b\u5982 Macvlan\u3001SR-IOV \u7b49\uff09\u89e3\u51b3\u5982\u4e0b\u95ee\u9898\uff1a</p> <ul> <li> <p>\u5728 Sriov CNI \u573a\u666f\u4e0b\uff0c\u5e2e\u52a9 Pod \u5b9e\u73b0 clusterIP \u901a\u4fe1</p> </li> <li> <p>\u5728 Pod \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cVeth \u80fd\u81ea\u52a8\u591f\u534f\u8c03\u591a\u7f51\u5361\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u89e3\u51b3\u591a\u7f51\u5361\u901a\u4fe1\u95ee\u9898</p> </li> </ul> <p>\u8bf7\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\uff0c\u4e0b\u8f7d\u5b89\u88c5 Veth \u4e8c\u8fdb\u5236\uff1a</p> <pre><code>wget https://github.com/spidernet-io/plugins/releases/download/v0.1.4/spider-plugins-linux-amd64-v0.1.4.tar\ntar xvfzp ./spider-plugins-linux-amd64-v0.1.4.tar -C /opt/cni/bin\nchmod +x /opt/cni/bin/veth\n</code></pre>"},{"location":"usage/get-started-sriov-zh_CN/#sriov-configmap","title":"\u521b\u5efa\u4e0e\u7f51\u5361\u914d\u7f6e\u5339\u914d\u7684 Sriov Configmap","text":"<ul> <li> <p>\u67e5\u8be2\u7f51\u5361 vendor\u3001deviceID \u548c driver \u4fe1\u606f\uff1a</p> <pre><code>~# ethtool -i enp4s0f0np0 |grep -e driver -e bus-info\ndriver: mlx5_core\nbus-info: 0000:04:00.0\n~#\n~# lspci -s 0000:04:00.0 -n\n04:00.0 0200: 15b3:1018\n</code></pre> <p>\u672c\u793a\u4f8b\u4e2d\uff0cvendor \u4e3a 15b3\uff0cdeviceID \u4e3a 1018\uff0cdriver \u4e3a mlx5_core</p> </li> <li> <p>\u521b\u5efa Configmap</p> <pre><code>vendor=\"15b3\"\ndeviceID=\"1018\"\ndriver=\"mlx5_core\"\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: sriovdp-config\n    namespace: kube-system\ndata:\n    config.json: |\n    {\n        \"resourceList\": [{\n                \"resourceName\": \"mlnx_sriov\",\n                \"selectors\": {\n                    \"vendors\": [ \"$vendor\" ],\n                    \"devices\": [ \"$deviceID\" ],\n                    \"drivers\": [ \"$driver\" ]\n                    }\n            }\n        ]\n    }\nEOF\n</code></pre> <p>resourceName \u4e3a sriov \u8d44\u6e90\u540d\u79f0\uff0c\u5728 configmap \u58f0\u660e\u540e\uff0c\u5728 sriov-plugin \u751f\u6548\u540e\uff0c\u4f1a\u5728 node \u4e0a\u4ea7\u751f\u4e00\u4e2a\u540d\u4e3a <code>intel.com/mlnx_sriov</code> \u7684 sriov \u8d44\u6e90\u4f9b Pod \u4f7f\u7528\uff0c\u524d\u7f00 <code>intel.com</code> \u53ef\u901a\u8fc7 <code>resourcePrefix</code> \u5b57\u6bb5\u5b9a\u4e49 \u5177\u4f53\u914d\u7f6e\u89c4\u5219\u53c2\u8003 Sriov Configmap</p> </li> </ul>"},{"location":"usage/get-started-sriov-zh_CN/#sriov-vf","title":"\u521b\u5efa Sriov VF","text":"<ol> <li> <p>\u67e5\u8be2\u5f53\u524d VF \u6570\u91cf</p> <pre><code>~# cat /sys/class/net/enp4s0f0np0/device/sriov_numvfs\n0\n</code></pre> </li> <li> <p>\u521b\u5efa 8\u4e2a VF</p> <pre><code>echo 8 &gt; /sys/class/net/enp4s0f0np0/device/sriov_numvfs\n</code></pre> <p>\u5177\u4f53\u914d\u7f6e\u53c2\u8003 sriov \u5b98\u65b9\u6587\u6863 Setting up Virtual Functions</p> </li> </ol>"},{"location":"usage/get-started-sriov-zh_CN/#sriov-device-plugin","title":"\u5b89\u88c5 Sriov Device Plugin","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-network-device-plugin/v3.5.1/deployments/k8s-v1.16/sriovdp-daemonset.yaml\n</code></pre> <p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u7b49\u5f85\u63d2\u4ef6\u751f\u6548\u3002</p> <ul> <li> <p>\u67e5\u770b Node \u53d1\u73b0\u5728 configmap \u4e2d\u5b9a\u4e49\u7684\u540d\u4e3a <code>intel.com/mlnx_sriov</code> \u7684 sriov \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d 8 \u4e3a VF \u7684\u6570\u91cf\uff1a</p> <pre><code>~# kubectl get  node  master-11 -ojson |jq '.status.allocatable'\n{\n\"cpu\": \"24\",\n  \"ephemeral-storage\": \"94580335255\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"intel.com/mlnx_sriov\": \"8\",\n  \"memory\": \"16247944Ki\",\n  \"pods\": \"110\"\n}\n</code></pre> </li> </ul>"},{"location":"usage/get-started-sriov-zh_CN/#sriov-cni","title":"\u5b89\u88c5 Sriov CNI","text":"<p>\u901a\u8fc7 manifest \u5b89\u88c5 Sriov CNI</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-cni/v2.7.0/images/k8s-v1.16/sriov-cni-daemonset.yaml\n</code></pre>"},{"location":"usage/get-started-sriov-zh_CN/#multus","title":"\u5b89\u88c5 Multus","text":"<ol> <li> <p>\u901a\u8fc7 manifest \u5b89\u88c5 Multus</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>\u4e3a Sriov \u521b\u5efa Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e</p> <p>\u56e0\u4e3a\u4f7f\u7528 Veth \u63d2\u4ef6\u6765\u5b9e\u73b0 clusterIP \u901a\u4fe1\uff0c\u9700\u786e\u8ba4\u96c6\u7fa4\u7684 service CIDR\uff0c\u4f8b\u5982\u53ef\u57fa\u4e8e\u547d\u4ee4 <code>kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service</code> \u67e5\u8be2</p> <pre><code>SERVICE_CIDR=\"10.43.0.0/16\"\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov\n  name: sriov-test\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"sriov-test\",\n        \"plugins\": [\n            {\n                \"type\": \"sriov\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            },{\n                  \"type\": \"veth\",\n                  \"service_cidr\": [\"${SERVICE_CIDR}\"]\n              }\n        ]\n    }\nEOF\n</code></pre> <p><code>k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov</code> \u8be5 annotations \u8868\u793a\u8981\u4f7f\u7528\u7684 sriov \u8d44\u6e90\u540d\u79f0.</p> </li> </ol>"},{"location":"usage/get-started-sriov-zh_CN/#spiderpool","title":"\u5b89\u88c5 Spiderpool","text":"<ol> <li> <p>\u5b89\u88c5 Spiderpool\u3002</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm repo update spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 <code>--set global.imageRegistryOverride=ghcr.m.daocloud.io</code> \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002</p> </li> <li> <p>\u521b\u5efa SpiderSubnet \u5b9e\u4f8b\u3002</p> <p>Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderSubnet \u793a\u4f8b</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ips:\n  - \"10.20.168.190-10.20.168.199\"\n  subnet: 10.20.0.0/16\n  gateway: 10.20.0.1\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-sriov-zh_CN/#_2","title":"\u521b\u5efa\u5e94\u7528","text":"<ol> <li> <p>\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sriov-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sriov-deploy\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/sriov-test\n      labels:\n        app: sriov-deploy\n    spec:\n      containers:\n      - name: sriov-deploy\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        resources:\n          requests:\n            intel.com/mlnx_sriov: '1' \n          limits:\n            intel.com/mlnx_sriov: '1'  \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sriov-deploy-svc\n  labels:\n    app: sriov-deploy\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: sriov-deploy \nEOF\n</code></pre> <p>\u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a</p> <p><code>intel.com/mlnx_sriov</code>: \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 Sriov \u8d44\u6e90\u3002</p> <p><code>v1.multus-cni.io/default-network</code>\uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002</p> <p>\u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3</p> </li> <li> <p>\u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001</p> <pre><code>~# kubectl get pod -l app=sriov-deploy -owide\nNAME                           READY   STATUS    RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES\nsriov-deploy-9b4b9f6d9-mmpsm   1/1     Running   0          6m54s   10.20.168.191   worker-12   &lt;none&gt;           &lt;none&gt;\nsriov-deploy-9b4b9f6d9-xfsvj   1/1     Running   0          6m54s   10.20.168.190   master-11   &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u521b\u5efa\u4e86 IP \u56fa\u5b9a\u6c60\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185</p> <pre><code>~# kubectl get spiderippool\nNAME                                     VERSION   SUBNET         ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-sriov-deploy-v4-eth0-f5488b112fd9   4         10.20.0.0/16   2                    2                false     false\n\n~#  kubectl get spiderendpoints\nNAME                           INTERFACE   IPV4POOL                                 IPV4               IPV6POOL   IPV6   NODE\nsriov-deploy-9b4b9f6d9-mmpsm   eth0        auto-sriov-deploy-v4-eth0-f5488b112fd9   10.20.168.191/16                     worker-12\nsriov-deploy-9b4b9f6d9-xfsvj   eth0        auto-sriov-deploy-v4-eth0-f5488b112fd9   10.20.168.190/16                     master-11\n</code></pre> </li> <li> <p>\u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf</p> <pre><code>~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10.20.168.190 -c 3\nPING 10.20.168.190 (10.20.168.190) 56(84) bytes of data.\n64 bytes from 10.20.168.190: icmp_seq=1 ttl=64 time=0.162 ms\n64 bytes from 10.20.168.190: icmp_seq=2 ttl=64 time=0.138 ms\n64 bytes from 10.20.168.190: icmp_seq=3 ttl=64 time=0.191 ms\n\n--- 10.20.168.190 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2051ms\nrtt min/avg/max/mdev = 0.138/0.163/0.191/0.021 ms\n</code></pre> </li> <li> <p>\u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf</p> <ul> <li> <p>\u67e5\u770b Service \u7684 IP\uff1a</p> <pre><code>~# kubectl get svc\nNAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE\nkubernetes         ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP              23d\nsriov-deploy-svc   ClusterIP   10.43.54.100   &lt;none&gt;        80/TCP               20m\n</code></pre> </li> <li> <p>Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a</p> <pre><code>~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10.43.54.100 -I\nHTTP/1.1 200 OK\nServer: nginx/1.23.3\nDate: Mon, 27 Mar 2023 08:22:39 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 13 Dec 2022 15:53:53 GMT\nConnection: keep-alive\nETag: \"6398a011-267\"\nAccept-Ranges: bytes\n</code></pre> </li> </ul> </li> </ol>"},{"location":"usage/get-started-sriov/","title":"SRIOV Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p>Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus, Sriov, Veth, and Spiderpool, which meets the following kinds of requirements:</p> <ul> <li> <p>Applications can be assigned static Underlay IP addresses through simple operations.</p> </li> <li> <p>Pods with multiple Underlay NICs connect to multiple Underlay subnets.</p> </li> <li> <p>Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.</p> </li> </ul>"},{"location":"usage/get-started-sriov/#prerequisites","title":"Prerequisites","text":"<ol> <li>Make sure a Kubernetes cluster is ready</li> <li>Helm has already been installed</li> <li> <p>A SR-IOV-enabled NIC</p> <ul> <li> <p>Check the NIC's bus-info:</p> <pre><code>~# ethtool -i enp4s0f0np0 |grep bus-info\nbus-info: 0000:04:00.0\n</code></pre> </li> <li> <p>Check whether the NIC supports SR-IOV via bus-info. If the <code>Single Root I/O Virtualization (SR-IOV)</code> field appears, it means that SR-IOV is supported:</p> <pre><code>~# lspci -s 0000:04:00.0 -v |grep SR-IOV\nCapabilities: [180] Single Root I/O Virtualization (SR-IOV)      </code></pre> </li> </ul> </li> </ol>"},{"location":"usage/get-started-sriov/#install-veth","title":"Install Veth","text":"<p><code>Veth</code> is a CNI plugin designed to resolve the following issues in other CNIs like Macvlan and SR-IOV:</p> <ul> <li> <p>Enable clusterIP communication for Pods in the Sriov CNI scenario</p> </li> <li> <p>Address communication issues in multiple NICs for Pods by automatically coordinating policy routing between NICs</p> </li> </ul> <p>Download and install the Veth binary on all nodes:</p> <pre><code>wget https://github.com/spidernet-io/plugins/releases/download/v0.1.4/spider-plugins-linux-amd64-v0.1.4.tar\ntar xvfzp ./spider-plugins-linux-amd64-v0.1.4.tar -C /opt/cni/bin\nchmod +x /opt/cni/bin/veth\n</code></pre>"},{"location":"usage/get-started-sriov/#create-sriov-configmap-that-matches-the-nic-configuration","title":"Create Sriov Configmap that matches the NIC configuration","text":"<ul> <li> <p>Check the vendor, deviceID and driver information of the NIC:</p> <pre><code>~# ethtool -i enp4s0f0np0 |grep -e driver -e bus-info\ndriver: mlx5_core\nbus-info: 0000:04:00.0\n\n~# lspci -s 0000:04:00.0 -n\n04:00.0 0200: 15b3:1018\n</code></pre> <p>In this example, the vendor is 15b3, the deviceID is 1018, and the driver is mlx5_core.</p> </li> <li> <p>Create Configmap</p> <pre><code>vendor=\"15b3\"\ndeviceID=\"1018\"\ndriver=\"mlx5_core\"\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: sriovdp-config\n    namespace: kube-system\ndata:\n    config.json: |\n    {\n        \"resourceList\": [{\n                \"resourceName\": \"mlnx_sriov\",\n                \"selectors\": {\n                    \"vendors\": [ \"$vendor\" ],\n                    \"devices\": [ \"$deviceID\" ],\n                    \"drivers\": [ \"$driver\" ]\n                    }\n            }\n        ]\n    }\nEOF\n</code></pre> <p>resourceName is the name of the Sriov resource, and after it is declared in Configmap, a Sriov resource named <code>intel.com/mlnx_sriov</code> will be generated on the node for Pods after the sriov-plugin takes effect. The prefix <code>intel.com</code> can be defined through the <code>resourcePrefix</code> field. Refer to Sriov Configmap for more configuration rules.</p> </li> </ul>"},{"location":"usage/get-started-sriov/#create-sriov-vfs","title":"Create Sriov VFs","text":"<ol> <li> <p>Check the current number of VFs:</p> <pre><code>~# cat /sys/class/net/enp4s0f0np0/device/sriov_numvfs\n0\n</code></pre> </li> <li> <p>Create 8 VFs</p> <pre><code>echo 8 &gt; /sys/class/net/enp4s0f0np0/device/sriov_numvfs\n</code></pre> <p>Refer to Setting up Virtual Functions for more details.</p> </li> </ol>"},{"location":"usage/get-started-sriov/#install-sriov-device-plugin","title":"Install Sriov Device Plugin","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-network-device-plugin/v3.5.1/deployments/k8s-v1.16/sriovdp-daemonset.yaml\n</code></pre> <p>Wait for the plugin to take effect After installation.</p> <ul> <li> <p>Check the Node and verify that the Sriov resource named <code>intel.com/mlnx_sriov</code> defined in Configmap has taken effect, with 8 as the number of VFs:</p> <pre><code>~# kubectl get  node  master-11 -ojson |jq '.status.allocatable'\n{\n\"cpu\": \"24\",\n  \"ephemeral-storage\": \"94580335255\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"intel.com/mlnx_sriov\": \"8\",\n  \"memory\": \"16247944Ki\",\n  \"pods\": \"110\"\n}\n</code></pre> </li> </ul>"},{"location":"usage/get-started-sriov/#install-sriov-cni","title":"Install Sriov CNI","text":"<p>Install Sriov CNI through manifest:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-cni/v2.7.0/images/k8s-v1.16/sriov-cni-daemonset.yaml\n</code></pre>"},{"location":"usage/get-started-sriov/#install-multus","title":"Install Multus","text":"<ol> <li> <p>Install Multus through manifest:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/v3.9/deployments/multus-daemonset.yml\n</code></pre> </li> <li> <p>Create a NetworkAttachmentDefinition configuration for Sriov in Multus</p> <p>To implement clusterIP communication using Veth, confirm the service CIDR of the cluster through a query command <code>kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service</code> or other methods:</p> <pre><code>SERVICE_CIDR=\"10.43.0.0/16\"\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov\n  name: sriov-test\n  namespace: kube-system\nspec:\n  config: |-\n    {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"sriov-test\",\n        \"plugins\": [\n            {\n                \"type\": \"sriov\",\n                \"ipam\": {\n                    \"type\": \"spiderpool\"\n                }\n            },{\n                  \"type\": \"veth\",\n                  \"service_cidr\": [\"${SERVICE_CIDR}\"]\n              }\n        ]\n    }\nEOF\n</code></pre> <p><code>k8s.v1.cni.cncf.io/resourceName: intel.com/mlnx_sriov</code>: the name of the Sriov resource to be used</p> </li> </ol>"},{"location":"usage/get-started-sriov/#install-spiderpool","title":"Install Spiderpool","text":"<ol> <li> <p>Install Spiderpool.</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm repo update spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system\n</code></pre> <p>If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter <code>-set global.imageRegistryOverride=ghcr.m.daocloud.io</code> to avoid image pulling failures for Spiderpool.</p> </li> <li> <p>Create a SpiderSubnet instance.</p> <p>The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: subnet-test\nspec:\n  ips:\n  - \"10.20.168.190-10.20.168.199\"\n  subnet: 10.20.0.0/16\n  gateway: 10.20.0.1\nEOF\n</code></pre> </li> </ol>"},{"location":"usage/get-started-sriov/#create-applications","title":"Create applications","text":"<ol> <li> <p>Create test Pods and Services via the command below\uff1a</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sriov-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sriov-deploy\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: |-\n          {\n            \"ipv4\": [\"subnet-test\"]\n          }\n        v1.multus-cni.io/default-network: kube-system/sriov-test\n      labels:\n        app: sriov-deploy\n    spec:\n      containers:\n      - name: sriov-deploy\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        resources:\n          requests:\n            intel.com/mlnx_sriov: '1' \n          limits:\n            intel.com/mlnx_sriov: '1'  \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sriov-deploy-svc\n  labels:\n    app: sriov-deploy\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: sriov-deploy \nEOF\n</code></pre> <p>Spec descriptions:</p> <p><code>intel.com/mlnx_sriov</code>: Sriov resources used.</p> <p><code>v1.multus-cni.io/default-network</code>: specifies the CNI configuration for Multus.</p> <p>For more information on Multus annotations, refer to Multus Quickstart.</p> </li> <li> <p>Check the status of Pods:</p> <pre><code>~# kubectl get pod -l app=sriov-deploy -owide\nNAME                           READY   STATUS    RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES\nsriov-deploy-9b4b9f6d9-mmpsm   1/1     Running   0          6m54s   10.20.168.191   worker-12   &lt;none&gt;           &lt;none&gt;\nsriov-deploy-9b4b9f6d9-xfsvj   1/1     Running   0          6m54s   10.20.168.190   master-11   &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges.</p> <pre><code>~# kubectl get spiderippool\nNAME                                     VERSION   SUBNET         ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-sriov-deploy-v4-eth0-f5488b112fd9   4         10.20.0.0/16   2                    2                false     false\n\n~#  kubectl get spiderendpoints\nNAME                           INTERFACE   IPV4POOL                                 IPV4               IPV6POOL   IPV6   NODE\nsriov-deploy-9b4b9f6d9-mmpsm   eth0        auto-sriov-deploy-v4-eth0-f5488b112fd9   10.20.168.191/16                     worker-12\nsriov-deploy-9b4b9f6d9-xfsvj   eth0        auto-sriov-deploy-v4-eth0-f5488b112fd9   10.20.168.190/16                     master-11\n</code></pre> </li> <li> <p>Test the communication between Pods:</p> <pre><code>~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10.20.168.190 -c 3\nPING 10.20.168.190 (10.20.168.190) 56(84) bytes of data.\n64 bytes from 10.20.168.190: icmp_seq=1 ttl=64 time=0.162 ms\n64 bytes from 10.20.168.190: icmp_seq=2 ttl=64 time=0.138 ms\n64 bytes from 10.20.168.190: icmp_seq=3 ttl=64 time=0.191 ms\n\n--- 10.20.168.190 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2051ms\nrtt min/avg/max/mdev = 0.138/0.163/0.191/0.021 ms\n</code></pre> </li> <li> <p>Test the communication between Pods and Services:</p> <ul> <li> <p>Check Services' IPs\uff1a</p> <pre><code>~# kubectl get svc\nNAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE\nkubernetes         ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP              23d\nsriov-deploy-svc   ClusterIP   10.43.54.100   &lt;none&gt;        80/TCP               20m\n</code></pre> </li> <li> <p>Access its own service within the Pod:</p> <pre><code>~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10.43.54.100 -I\nHTTP/1.1 200 OK\nServer: nginx/1.23.3\nDate: Mon, 27 Mar 2023 08:22:39 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 13 Dec 2022 15:53:53 GMT\nConnection: keep-alive\nETag: \"6398a011-267\"\nAccept-Ranges: bytes\n</code></pre> </li> </ul> </li> </ol>"},{"location":"usage/get-started-weave-zh_CN/","title":"Weave Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p><code>Weave</code> \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c<code>Weave</code> \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 <code>IPAM</code> \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 <code>IPAM</code> \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d <code>Weave</code>, \u5728\u4fdd\u7559 <code>Weave</code> \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 <code>Spiderpool</code> \u6269\u5c55 <code>Weave</code> \u7684 <code>IPAM</code> \u80fd\u529b\u3002</p>"},{"location":"usage/get-started-weave-zh_CN/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ul> <li>\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI</li> <li>Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177</li> </ul>"},{"location":"usage/get-started-weave-zh_CN/#_2","title":"\u5b89\u88c5","text":"<ol> <li> <p>\u5b89\u88c5 Weave :</p> <pre><code>kubectl apply -f  https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> <p>\u7b49\u5f85 Pod Running:</p> <pre><code>[root@node1 ~]# kubectl get po -n kube-system  | grep weave\nweave-net-ck849                         2/2     Running     4     0   1m\nweave-net-vhmqx                         2/2     Running     4     0   1m\n</code></pre> </li> <li> <p>\u5b89\u88c5 Spiderpool</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system </code></pre> <p>\u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 <code>--set global.imageRegistryOverride=ghcr.m.daocloud.io</code> \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002</p> <p>\u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u7684\u5b50\u7f51(SpiderSubnet):</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: weave-subnet-v4\n  labels:  \n    ipam.spidernet.io/subnet-cidr: 10-32-0-0-12\nspec:\n  ips:\n  - 10.32.0.100-10.32.50.200\n  subnet: 10.32.0.0/12\nEOF\n</code></pre> <p><code>Weave</code> \u4f7f\u7528 <code>10.32.0.0/12</code> \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u8fd9\u91cc\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u7684 SpiderSubnet</p> </li> <li> <p>\u9a8c\u8bc1\u5b89\u88c5</p> </li> </ol> <pre><code> [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool\nspiderpool-agent-lgdw7                  1/1     Running   0          65s\n spiderpool-agent-x974l                  1/1     Running   0          65s\n spiderpool-controller-9df44bc47-hbhbg   1/1     Running   0          65s\n [root@node1 ~]# kubectl get ss\nNAME               VERSION   SUBNET         ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\n weave-subnet-v4    4         10.32.0.0/12   0                    12901            false\n</code></pre>"},{"location":"usage/get-started-weave-zh_CN/#weave-ipam-spiderpool","title":"\u5207\u6362 <code>Weave</code> \u7684 <code>IPAM</code> \u4e3a <code>Spiderpool</code>","text":"<p>\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: <code>/etc/cni/net.d/10-weave.conflist</code> \u7684 <code>ipam</code>\u5b57\u6bb5:</p> <pre><code>[root@node1 ~]# cat /etc/cni/net.d/10-weave.conflist\n{\n\"cniVersion\": \"0.3.0\",\n    \"name\": \"weave\",\n    \"plugins\": [\n{\n\"name\": \"weave\",\n            \"type\": \"weave-net\",\n            \"hairpinMode\": true\n},\n        {\n\"type\": \"portmap\",\n            \"capabilities\": {\"portMappings\": true},\n            \"snat\": true\n}\n]\n}\n</code></pre> <p>\u4fee\u6539\u4e3a:</p> <pre><code>{\n\"cniVersion\": \"0.3.0\",\n\"name\": \"weave\",\n\"plugins\": [\n{\n\"name\": \"weave\",\n\"type\": \"weave-net\",\n\"ipam\": {\n\"type\": \"spiderpool\"\n},\n\"hairpinMode\": true\n},\n{\n\"type\": \"portmap\",\n\"capabilities\": {\"portMappings\": true},\n\"snat\": true\n}\n]\n}\n</code></pre> <p>\u6216\u53ef\u901a\u8fc7 <code>jq</code>  \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 <code>jq</code> \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5:</p> <pre><code># \u4ee5 centos7 \u4e3a\u4f8b\nyum -y install jq\n</code></pre> <p>\u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6:</p> <pre><code>cat &lt;&lt;&lt; $(jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist) &gt; /etc/cni/net.d/10-weave.conflist\n</code></pre> <p>\u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c</p>"},{"location":"usage/get-started-weave-zh_CN/#_3","title":"\u521b\u5efa\u5e94\u7528","text":"<p>\u4f7f\u7528\u6ce8\u89e3: <code>ipam.spidernet.io/subnet</code> \u6307\u5b9a Pod \u4ece\u8be5 SpiderSubnet \u4e2d\u5206\u914d IP:</p> <pre><code>[root@node1 ~]# cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\nselector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: '{\"ipv4\":[\"weave-subnet-v4\"]}'\nlabels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: IfNotPresent\n        lifecycle: {}\nname: container-1\nEOF\n</code></pre> <p>spec.template.metadata.annotations.ipam.spidernet.io/subnet\uff1a\u6307\u5b9a Pod \u4ece SpiderSubnet:  <code>weave-subnet-v4</code> \u4e2d\u5206\u914d IP</p> <p>Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool Subnet \u4e2d\u5206\u914d IP \u5730\u5740:</p> <pre><code>[root@node1 ~]# kubectl get po  -o wide\nNAME                     READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES\nnginx-5745d9b5d7-2rvn7   1/1     Running   0          8s    10.32.22.190   node1   &lt;none&gt;           &lt;none&gt;\nnginx-5745d9b5d7-5ssck   1/1     Running   0          8s    10.32.35.87    node2   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Spiderpool \u4e3a\u8be5 Nginx \u5e94\u7528\u81ea\u52a8\u521b\u5efa\u4e86\u4e00\u4e2a IP \u6c60: <code>auto-deployment-default-nginx-v4-a0ae75eb5d47</code>, \u6c60\u7684 IP \u6570\u91cf\u4e3a 2:</p> <pre><code>[root@node1 ~]# kubectl get sp\nNAME                                            VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nauto-deployment-default-nginx-v4-a0ae75eb5d47   4         10.32.0.0/12    2                    2                false\n</code></pre> <p>\u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b:</p> <pre><code>[root@node1 ~]# kubectl exec  nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2\nPING 10.32.35.87 (10.32.35.87): 56 data bytes\n64 bytes from 10.32.35.87: seq=0 ttl=64 time=4.561 ms\n64 bytes from 10.32.35.87: seq=1 ttl=64 time=0.632 ms\n\n--- 10.32.35.87 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.632/2.596/4.561 ms\n</code></pre> <p>\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002 \u901a\u8fc7<code>Spiderpool</code>, \u786e\u5b9e\u6269\u5c55\u4e86 <code>Weave</code> \u7684 <code>IPAM</code> \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528\uff0c\u4f53\u9a8c <code>Spiderpool</code> \u5176\u4ed6\u7684\u529f\u80fd\u3002</p>"},{"location":"usage/get-started-weave/","title":"Weave Quick Start","text":"<p>English | \u7b80\u4f53\u4e2d\u6587</p> <p><code>Weave</code>, an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, <code>Weave</code> utilizes the built-in <code>IPAM</code> to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how <code>Weave</code> and <code>Spiderpool</code> can be integrated to extend <code>Weave</code>'s IPAM capabilities while preserving its original functions.</p>"},{"location":"usage/get-started-weave/#prerequisites","title":"Prerequisites","text":"<ul> <li>A ready Kubernetes cluster without any CNI installed</li> <li>Helm, Kubectl and Jq (optional)</li> </ul>"},{"location":"usage/get-started-weave/#install","title":"Install","text":"<ol> <li> <p>Install Weave:</p> <pre><code>kubectl apply -f  https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> <p>Wait for Pod Running:</p> <pre><code>[root@node1 ~]# kubectl get po -n kube-system  | grep weave\nweave-net-ck849                         2/2     Running     4     0   1m\nweave-net-vhmqx                         2/2     Running     4     0   1m\n</code></pre> </li> <li> <p>Install Spiderpool</p> <pre><code>helm repo add spiderpool https://spidernet-io.github.io/spiderpool\nhelm install spiderpool spiderpool/spiderpool --namespace kube-system </code></pre> <p>If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter <code>-set global.imageRegistryOverride=ghcr.m.daocloud.io</code> to avoid image pulling failures for Spiderpool.</p> <p>Wait for Pod Running and create a subnet for Pod (SpiderSubnet):</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\n  name: weave-subnet-v4\n  labels:  \n    ipam.spidernet.io/subnet-cidr: 10-32-0-0-12\nspec:\n  ips:\n  - 10.32.0.100-10.32.50.200\n  subnet: 10.32.0.0/12\nEOF\n</code></pre> <p><code>Weave</code> uses <code>10.32.0.0/12</code> as the cluster's default subnet, and thus a SpiderSubnet with the same subnet needs to be created in this case</p> </li> <li> <p>Verify installation</p> <pre><code>[root@node1 ~]# kubectl get po -n kube-system | grep spiderpool\nspiderpool-agent-lgdw7                  1/1     Running   0          65s\nspiderpool-agent-x974l                  1/1     Running   0          65s\nspiderpool-controller-9df44bc47-hbhbg   1/1     Running   0          65s\n[root@node1 ~]# kubectl get ss\nNAME               VERSION   SUBNET         ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nweave-subnet-v4    4         10.32.0.0/12   0                    12901            false\n</code></pre> </li> </ol>"},{"location":"usage/get-started-weave/#switch-weaves-ipam-to-spiderpool","title":"Switch <code>Weave</code>'s <code>IPAM</code> to Spiderpool","text":"<p>Change the <code>ipam</code> field of <code>/etc/cni/net.d/10-weave.conflist</code> on each node:</p> <p>Change the following:</p> <pre><code>[root@node1 ~]# cat /etc/cni/net.d/10-weave.conflist\n{\n\"cniVersion\": \"0.3.0\",\n    \"name\": \"weave\",\n    \"plugins\": [\n{\n\"name\": \"weave\",\n            \"type\": \"weave-net\",\n            \"hairpinMode\": true\n},\n        {\n\"type\": \"portmap\",\n            \"capabilities\": {\"portMappings\": true},\n            \"snat\": true\n}\n]\n}\n</code></pre> <p>To:</p> <pre><code>{\n\"cniVersion\": \"0.3.0\",\n\"name\": \"weave\",\n\"plugins\": [\n{\n\"name\": \"weave\",\n\"type\": \"weave-net\",\n\"ipam\": {\n\"type\": \"spiderpool\"\n},\n\"hairpinMode\": true\n},\n{\n\"type\": \"portmap\",\n\"capabilities\": {\"portMappings\": true},\n\"snat\": true\n}\n]\n}\n</code></pre> <p>Alternatively, it can be changed with <code>jq</code> in one step. If <code>jq</code> is not installed, you can use the following command to install it:</p> <pre><code># Take centos7 as an example\nyum -y install jq\n</code></pre> <p>Change the CNI configuration file:</p> <pre><code>cat &lt;&lt;&lt; $(jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist) &gt; /etc/cni/net.d/10-weave.conflist\n</code></pre> <p>Make sure to run this command at each node</p>"},{"location":"usage/get-started-weave/#create-applications","title":"Create applications","text":"<p>Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation <code>ipam.spidernet.io/subnet</code>:</p> <pre><code>[root@node1 ~]# cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\nselector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      annotations:\n        ipam.spidernet.io/subnet: '{\"ipv4\":[\"weave-subnet-v4\"]}'\nlabels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: IfNotPresent\n        lifecycle: {}\nname: container-1\nEOF\n</code></pre> <p>spec.template.metadata.annotations.ipam.spidernet.io/subnet: specifies that the Pods will be assigned IPs from SpiderSubnet: <code>weave-subnet-v4</code>.</p> <p>The Pods have been created and allocated IP addresses from Spiderpool Subnets:</p> <pre><code>[root@node1 ~]# kubectl get po  -o wide\nNAME                     READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES\nnginx-5745d9b5d7-2rvn7   1/1     Running   0          8s    10.32.22.190   node1   &lt;none&gt;           &lt;none&gt;\nnginx-5745d9b5d7-5ssck   1/1     Running   0          8s    10.32.35.87    node2   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Spiderpool has automatically created an IP pool for the Nginx application named <code>auto-deployment-default-nginx-v4-a0ae75eb5d47</code>, with a pool size of 2 IP addresses:</p> <pre><code>[root@node1 ~]# kubectl get sp\nNAME                                            VERSION   SUBNET          ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nauto-deployment-default-nginx-v4-a0ae75eb5d47   4         10.32.0.0/12    2                    2                false\n</code></pre> <p>To test connectivity, let's use inter-node communication between Pods as an example:</p> <pre><code>[root@node1 ~]# kubectl exec  nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2\nPING 10.32.35.87 (10.32.35.87): 56 data bytes\n64 bytes from 10.32.35.87: seq=0 ttl=64 time=4.561 ms\n64 bytes from 10.32.35.87: seq=1 ttl=64 time=0.632 ms\n\n--- 10.32.35.87 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.632/2.596/4.561 ms\n</code></pre> <p>The test results indicate that IP allocation and network connectivity are normal. <code>Spiderpool</code> has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of <code>Spiderpool</code>.</p>"},{"location":"usage/install/","title":"Installation","text":""},{"location":"usage/install/#install","title":"Install","text":"<p>Any CNI project compatible with third-party IPAM plugins, can work well with spiderpool, such as:</p> <ul> <li> <p>macvlan CNI</p> </li> <li> <p>vlan CNI</p> </li> <li> <p>ipvlan CNI</p> </li> <li> <p>sriov CNI</p> </li> <li> <p>ovs CNI</p> </li> <li> <p>Multus CNI</p> </li> <li> <p>calico CNI</p> </li> <li> <p>weave CNI</p> </li> </ul> <p>The following examples are guides to install spiderpool with different CNI:</p> <ul> <li> <p>spiderpool with macvlan in Kind</p> </li> <li> <p>spiderpool with macvlan CNI</p> </li> <li> <p>spiderpool with SRIOV CNI</p> </li> <li> <p>spiderpool with calico CNI</p> </li> <li> <p>spiderpool with weave CNI</p> </li> <li> <p>spiderpool with ovs CNI</p> </li> </ul>"},{"location":"usage/install/#uninstall","title":"Uninstall","text":"<p>Generally, you can uninstall Spiderpool release in this way:</p> <pre><code>helm uninstall spiderpool -n kube-system\n</code></pre> <p>However, there are finalizers in some CRs of Spiderpool, the <code>helm uninstall</code> cmd may not clean up all relevant CRs. Get this cleanup script and execute it to ensure that unexpected errors will not occur when deploying Spiderpool next time.</p> <pre><code>wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh\nchmod +x cleanCRD.sh &amp;&amp; ./cleanCRD.sh\n</code></pre>"},{"location":"usage/ippool-affinity-namespace/","title":"Namespace affinity of IPPool","text":"<p>Spiderpool supports affinity between IP pools and Namespaces. It means only Pods running under these Namespaces can use the IP pools that have an affinity to these Namespaces.</p> <p>Namespace affinity should be regarded as a filtering mechanism rather than a pool selection rule.</p>"},{"location":"usage/ippool-affinity-namespace/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.</p>"},{"location":"usage/ippool-affinity-namespace/#get-started","title":"Get started","text":"<p>First, create a new Namespace <code>test-ns</code>.</p> <pre><code>kubectl create namespace test-ns\n</code></pre> <p>Create an IPPool that will be bound to it.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: test-ns-ipv4-ippool\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.41\nnamespaceAffinity:\nmatchLabels:\nkubernetes.io/metadata.name: test-ns\n</code></pre> <p>For convenience, this example uses a native Namespace label <code>kubernetes.io/metadata.name</code> as the matching condition of IPPool affinity. You can replace them with desired labels to match the corresponding Namespaces.</p> <p>Next, create two Deployments under <code>test-ns</code> and <code>default</code> Namespaces respectively, and configure the Pods therein to get IP addresses from the IPPool above.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml\n</code></pre> <p>You will find that the Deployment under Namespace <code>test-ns</code> is running.</p> <pre><code>kubectl get deploy -n test-ns\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ntest-ns-deploy   1/1     1            1           35s\n</code></pre> <p>And its Pod has been assigned with an IP address from that IPPool.</p> <pre><code>kubectl get se -n test-ns\nNAME                             INTERFACE   IPV4POOL              IPV4              IPV6POOL   IPV6   NODE            CREATETION TIME\ntest-ns-deploy-74c6784f9-dlkmx   eth0        test-ns-ipv4-ippool   172.18.41.41/24                     spider-worker   46s\n</code></pre> <p>However, the Deployment under Namespace <code>default</code> cannot work properly. You can troubleshoot with the Events of its Pod:</p> <pre><code>kubectl describe po default-ns-deploy-5587c7bd47-xbmj2 -n default\n...\nEvents:\n  Type     Reason                  Age   From               Message\n  ----     ------                  ----  ----               -------\n  Normal   Scheduled               18s   default-scheduler  Successfully assigned default/default-ns-deploy-5587c7bd47-xbmj2 to spider-worker\n  Warning  FailedCreatePodSandBox  17s   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"97f18ae3ee315f58347f8936f819dd20b29c2d0a3d457fc6f0022282bf513e91\": [default/default-ns-deploy-5587c7bd47-xbmj2:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure  failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [test-ns-ipv4-ippool] of eth0 filtered out: unmatched Namespace affinity of IPPool test-ns-ipv4-ippool\n</code></pre> <p>Obviously, this Pod has no permission to get IP addresses from IPPool <code>test-ns-ipv4-ippool</code>.</p> <p>You can specify a default IP pool for a Namespace and set the corresponding <code>namespaceAffinity</code> for the IPPool to achieve the effect of \"a Namespace static IP pool\".</p>"},{"location":"usage/ippool-affinity-namespace/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete ns test-ns\nkubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/ippool-affinity-node/","title":"Node affinity of IPPool","text":"<p>Spiderpool supports affinity between IP pools and Nodes. It means only Pods running on these Nodes can use the IP pools that have an affinity to these Nodes.</p> <p>Node affinity should be regarded as a filtering mechanism rather than a pool selection rule.</p>"},{"location":"usage/ippool-affinity-node/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.</p>"},{"location":"usage/ippool-affinity-node/#get-started","title":"Get started","text":"<p>Since the cluster in this example has only two Nodes (1 master and 1 worker), it is required to remove the relevant taints on the master Node through <code>kubectl taint</code>, so that ordinary Pods can also be scheduled to it. If your cluster has two or more worker Nodes, please ignore the step above.</p> <p>Create two IPPools with 1 IP address each, one of which will provide IP addresses for all Pods running on the master Node.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: master-ipv4-ippool\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40\nnodeAffinity:\nmatchExpressions:\n- {key: node-role.kubernetes.io/master, operator: Exists}\n</code></pre> <p>The other provides IP addresses for the Pods on the worker Node.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: worker-ipv4-ippool\nspec:\nsubnet: 172.18.42.0/24\nips:\n- 172.18.42.40\nnodeAffinity:\nmatchExpressions:\n- {key: node-role.kubernetes.io/master, operator: DoesNotExist}\n</code></pre> <p>Here, the value of the Node annotation <code>node-role.kubernetes.io/master</code> distinguishes two Nodes with different roles (or different node regions). If there is no annotation <code>node-role.kubernetes.io/master</code> on your Nodes, you can change it to another one or add some annotations you want.</p> <p>Then, create a Deployment with 2 replicas, and set <code>podAntiAffinity</code> to ensure that the two Pods which select the above IPPools according to the syntax of alternative IP pools can be scheduled to different Nodes.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: node-affinity-deploy\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: node-affinity-deploy\ntemplate:\nmetadata:\nannotations:   ipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"master-ipv4-ippool\", \"worker-ipv4-ippool\"]\n}\nlabels:\napp: node-affinity-deploy\nspec:\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchLabels:\napp: node-affinity-deploy\ntopologyKey: kubernetes.io/hostname\ncontainers:\n- name: node-affinity-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>Finally, you will find that Pods on different Nodes will use different IPPools.</p> <pre><code>kubectl get se\nNAME                                    INTERFACE   IPV4POOL             IPV4              IPV6POOL   IPV6   NODE                   CREATETION TIME\nnode-affinity-deploy-66c9874465-rvdkm   eth0        master-ipv4-ippool   172.18.41.40/24                     spider-control-plane   35s\nnode-affinity-deploy-66c9874465-wb8ds   eth0        worker-ipv4-ippool   172.18.42.40/24                     spider-worker          35s\n</code></pre>"},{"location":"usage/ippool-affinity-node/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/ippool-affinity-pod/","title":"Pod affinity of IPPool","text":"<p>IPPool supports affinity setting for Pods. when setting <code>sepc.podAffinity</code> in SpiderIPPool, only the selected pods could get IP address from it, and another pods could not even specifying the annotation.</p> <ul> <li> <p>When setting the <code>sepc.podAffinity</code>, it helps to better implement the capability of static IP for workloads like Deployment, StatefulSet etc.</p> </li> <li> <p>When no <code>sepc.podAffinity</code>, all applications could share the IP address in the ippool</p> </li> </ul>"},{"location":"usage/ippool-affinity-pod/#get-started","title":"Get started","text":"<p>The example shows how <code>sepc.podAffinity</code> works.</p>"},{"location":"usage/ippool-affinity-pod/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>Follow the installation guide to install Spiderpool.</p>"},{"location":"usage/ippool-affinity-pod/#create-spidersubnet","title":"Create SpiderSubnet","text":"<p>Create a subnet to allocate IP addresses for the IPPool, from which both the Shared IPPool and the Occupied IPPool will receive their IP addresses.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-subnet.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\nname: static-ipv4-subnet\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.47\n</code></pre>"},{"location":"usage/ippool-affinity-pod/#shared-ippool","title":"Shared IPPool","text":"<ol> <li> <p>Create an IPPool</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: shared-static-ipv4-ippool\nspec:\nipVersion: 4\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.44-172.18.41.47\n</code></pre> </li> <li> <p>Create two Deployment  whose Pods are setting the Pod annotation <code>ipam.spidernet.io/ippool</code> to explicitly specify the pool selection rule. It will succeed to get IP address.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: shared-static-ippool-deploy-1\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: static\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"shared-static-ipv4-ippool\"]\n}\nlabels:\napp: static\nspec:\ncontainers:\n- name: shared-static-ippool-deploy-1\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: shared-static-ippool-deploy-2\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: static\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"shared-static-ipv4-ippool\"]\n}\nlabels:\napp: static\nspec:\ncontainers:\n- name: shared-static-ippool-deploy-2\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>The Pods are running.</p> <pre><code>kubectl get po -l app=static -o wide\nNAME                                             READY   STATUS    RESTARTS   AGE   IP             NODE              \nshared-static-ippool-deploy-1-8588c887cb-gcbjb   1/1     Running   0          62s   172.18.41.45   spider-control-plane \nshared-static-ippool-deploy-1-8588c887cb-wfdvt   1/1     Running   0          62s   172.18.41.46   spider-control-plane \nshared-static-ippool-deploy-2-797c8df6cf-6vllv   1/1     Running   0          62s   172.18.41.44   spider-worker \nshared-static-ippool-deploy-2-797c8df6cf-ftk2d   1/1     Running   0          62s   172.18.41.47   spider-worker\n</code></pre> </li> </ol>"},{"location":"usage/ippool-affinity-pod/#occupied-ippool","title":"Occupied IPPool","text":"<ol> <li> <p>Create an IPPool configured with <code>podAffinity</code>. The <code>spec.podAffinity</code> means only application labeled with <code>app: static</code> can get IP from this IPPool.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: occupied-static-ipv4-ippool\nspec:\nipVersion: 4\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.43\npodAffinity:\nmatchLabels:\napp: static\n</code></pre> </li> <li> <p>Create a Deployment whose Pods are labeled with <code>app: static</code>, and set the Pod annotation <code>ipam.spidernet.io/ippool</code> to explicitly specify the pool selection rule. It will succeed to get IP address.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: occupied-static-ippool-deploy\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: static\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"static-ipv4-ippool\"]\n}\nlabels:\napp: static\nspec:\ncontainers:\n- name: occupied-static-ippool-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>The Pods are running.</p> <pre><code>kubectl get po -l app=static -o wide\nNAME                                             READY   STATUS    RESTARTS   AGE   IP             NODE\noccupied-static-ippool-deploy-7f478cc7d7-l7wm5   1/1     Running   0          20s   172.18.41.42   spider-control-plane\noccupied-static-ippool-deploy-7f478cc7d7-vphw9   1/1     Running   0          20s   172.18.41.40   spider-worker\n</code></pre> </li> <li> <p>Create a Deployment using the same IPPool <code>occupied-static-ipv4-ippool</code> to allocate IP addresses, but its Pods do not have the label <code>app: static</code>, it will fail to get IP address.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml\n</code></pre> <p>As a result, these Pods cannot run successfully because they do not have permission to use the IPPool.</p> <pre><code>kubectl describe po wrong-static-ippool-deploy-6c496cfb7d-wptq5\n...\nEvents:\nType     Reason                  Age   From               Message\n----     ------                  ----  ----               -------\nNormal   Scheduled               35s   default-scheduler  Successfully assigned default/wrong-static-ippool-deploy-6c496cfb7d-wptq5 to spider-worker\nWarning  FailedCreatePodSandBox  34s   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"a6f717aede91a356b552ad38c66112a26e5f7a4f7d23b7067870f33f05d350bc\": [default/wrong-static-ippool-deploy-6c496cfb7d-wptq5:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure  failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [static-ipv4-ippool] of eth0 filtered out: unmatched Pod affinity of IPPool static-ipv4-ippool\n</code></pre> </li> </ol>"},{"location":"usage/ippool-affinity-pod/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-subnet.yaml   \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml  \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml  \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/ippool-multi/","title":"Backup IPPool","text":"<p>Multiple IP pools can be set for a pod for the usage of backup IP resources.</p>"},{"location":"usage/ippool-multi/#get-started","title":"Get Started","text":""},{"location":"usage/ippool-multi/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>Follow the guide installation to install Spiderpool.</p>"},{"location":"usage/ippool-multi/#backup-ippool-effect","title":"Backup IPPool effect","text":"<p>Create two IPPools each containing 2 IP addresses.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml\n</code></pre> <p>Create a Pod and allocate an IP address to it from these IPPools.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml\n</code></pre> <p>You will find that you still have 3 available IP addresses, one in IPPool <code>default-ipv4-ippool</code> and two in IPPool <code>backup-ipv4-ippool</code>.</p> <pre><code>kubectl get sp -l case=backup\nNAME                  VERSION   SUBNET           ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nbackup-ipv4-ippool    4         172.18.42.0/24   0                    2                false\ndefault-ipv4-ippool   4         172.18.41.0/24   1                    2                false\n</code></pre> <p>Then, create a Deployment with 2 replicas and allocate IP addresses to its Pods from the two IPPools above.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: multi-ippool-deploy\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: multi-ippool-deploy\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"default-ipv4-ippool\", \"backup-ipv4-ippool\"]\n}\nlabels:\napp: multi-ippool-deploy\nspec:\ncontainers:\n- name: multi-ippool-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>Spiderpool will successively try to allocate IP addresses in the order of the elements in the \"IP pool array\" until the first allocation succeeds or all fail. Of course, you can specify the pool selection rules (that defines alternative IP pools) in many ways, the Pod annotation <code>ipam.spidernet.io/ippool</code> is used here to select IP pools.</p> <p>Finally, when addresses in IPPool <code>default-ipv4-ippool</code> are used up, the IPPool <code>backup-ipv4-ippool</code> takes over.</p> <pre><code>kubectl get se\nNAME                                   INTERFACE   IPV4POOL              IPV4              IPV6POOL   IPV6   NODE            CREATETION TIME\ndummy                                  eth0        default-ipv4-ippool   172.18.41.41/24                     spider-worker   1m20s\nmulti-ippool-deploy-669bf7cf79-4x88m   eth0        default-ipv4-ippool   172.18.41.40/24                     spider-worker   2m31s\nmulti-ippool-deploy-669bf7cf79-k7zkk   eth0        backup-ipv4-ippool    172.18.42.41/24                     spider-worker   2m31s\n</code></pre>"},{"location":"usage/ippool-multi/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/ippool-namespace/","title":"Namespace default IPPool","text":"<p>Spiderpool provides default IP pools at Namespace level. A Pod not configured with a pool selection rule of higher priority will be assigned with IP addresses from the default IP pools of its Namespace.</p>"},{"location":"usage/ippool-namespace/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.</p>"},{"location":"usage/ippool-namespace/#get-started","title":"Get started","text":"<ol> <li> <p>Create a Namespace named as <code>test-ns1</code>.</p> <pre><code>kubectl create ns test-ns1\n</code></pre> </li> <li> <p>Create an IPPool to be bound with Namespace <code>test-ns1</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml\n</code></pre> </li> <li> <p>Check the status of this IPPool with the following command.</p> <pre><code>kubectl get sp -l case=ns\nNAME                      VERSION   SUBNET           ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DISABLE\nns1-default-ipv4-ippool   4         172.18.41.0/24   0                    4                false\n</code></pre> </li> <li> <p>Specify pool selection rules for Namespace <code>test-ns1</code> with the following command and annotation.</p> <pre><code>kubectl patch ns test-ns1 --patch-file https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-ippool-selection-patch.yaml\n</code></pre> <pre><code>metadata:\nannotations:\nipam.spidernet.io/default-ipv4-ippool: '[\"ns1-default-ipv4-ippool\"]'\n</code></pre> </li> <li> <p>Create a Deployment with 3 replicas in the Namespace <code>test-ns1</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ippool-deploy.yaml\n</code></pre> </li> </ol> <p>Now, all Pods in the Namespace should have been assigned with an IP address from the specified IPPool. Verify it with the following command:</p> <pre><code>kubectl get se -n test-ns1\nNAME                                         INTERFACE   IPV4POOL                  IPV4              IPV6POOL   IPV6   NODE            CREATETION TIME\nns1-default-ippool-deploy-7cd5449c88-9xncm   eth0        ns1-default-ipv4-ippool   172.18.41.41/24                     spider-worker   57s\nns1-default-ippool-deploy-7cd5449c88-dpfjs   eth0        ns1-default-ipv4-ippool   172.18.41.43/24                     spider-worker   57s\nns1-default-ippool-deploy-7cd5449c88-vjtdd   eth0        ns1-default-ipv4-ippool   172.18.41.42/24                     spider-worker   58s\n</code></pre> <p>The Namespace annotation <code>ipam.spidernet.io/defaultv4ippool</code> also supports the syntax of alternative IP pools, which means you can specify multiple default IP pools for a Namespace. In addition, one IPPool can be specified as the default IP pool for different Namespaces.</p> <p>If you want to bind an IPPool to a specific Namespace in an exclusive way, it means that no Namespace other than this (or a group of Namespaces) has permission to use this IPPool, please refer to SpiderIPPool namespace affinity.</p>"},{"location":"usage/ippool-namespace/#clean-up","title":"Clean up","text":"<p>Clean relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete ns test-ns1\nkubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml --ignore-not-found=true\n</code></pre>"},{"location":"usage/ipv6/","title":"IPv6 support","text":""},{"location":"usage/ipv6/#description","title":"Description","text":"<p>Spiderpool supports:</p> <ul> <li> <p>Dual stack</p> <p>Each workload can get IPv4 and IPv6 addresses, and can communicate over IPv4 or IPv6.</p> </li> <li> <p>IPv4 only</p> <p>Each workload can acquire IPv4 addresses, and can communicate over IPv4.</p> </li> <li> <p>IPv6 only</p> <p>Each workload can acquire IPv6 addresses, and can communicate over IPv6.</p> </li> </ul>"},{"location":"usage/ipv6/#get-started","title":"Get Started","text":""},{"location":"usage/ipv6/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>follow the guide installation to install Spiderpool.</p>"},{"location":"usage/ipv6/#create-spidersubnet","title":"Create SpiderSubnet","text":"<p>Create a SpiderSubnet and allocate IP addresses from the IPPool.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\nname: custom-ipv4-subnet\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.50\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\nname: custom-ipv6-subnet\nspec:\nsubnet: fd00:172:18::/64\nips:\n- fd00:172:18::40-fd00:172:18::50\n</code></pre>"},{"location":"usage/ipv6/#create-deployment-by-subnet","title":"Create Deployment By Subnet","text":"<p>create a Deployment whose Pods are setting the Pod annotation <code>ipam.spidernet.io/subnet</code> to  explicitly specify the subnet.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-subnet-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: custom-dual-subnet-deploy\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: custom-dual-subnet-deploy\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/subnet: |-\n{\n\"ipv4\": [\"custom-ipv4-subnet\"],\"ipv6\": [\"custom-ipv6-subnet\"]\n}\nlabels:\napp: custom-dual-subnet-deploy\nspec:\ncontainers:\n- name: custom-dual-subnet-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>The Pods are running.</p> <pre><code>kubectl get pod -l app=custom-dual-subnet-deploy -owide\nNAME                                         READY   STATUS    RESTARTS   AGE   IP             NODE                NOMINATED NODE   READINESS GATES\ncustom-dual-subnet-deploy-7fdbccfbb8-h5l4d   1/1     Running   0          33s   172.18.41.41   controller-node-1   &lt;none&gt;           &lt;none&gt;\ncustom-dual-subnet-deploy-7fdbccfbb8-rhdbd   1/1     Running   0          33s   172.18.41.42   controller-node-1   &lt;none&gt;           &lt;none&gt;\ncustom-dual-subnet-deploy-7fdbccfbb8-t6m5c   1/1     Running   0          33s   172.18.41.40   controller-node-1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>View all IPs of Pods</p> <pre><code>kubectl get pod -l app=custom-dual-subnet-deploy  -o go-template='{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}'\ncustom-dual-subnet-deploy-7fdbccfbb8-h5l4d: map[ip:172.18.41.41] map[ip:fd00:172:18::42]\ncustom-dual-subnet-deploy-7fdbccfbb8-rhdbd: map[ip:172.18.41.42] map[ip:fd00:172:18::41]\ncustom-dual-subnet-deploy-7fdbccfbb8-t6m5c: map[ip:172.18.41.40] map[ip:fd00:172:18::40]\n</code></pre>"},{"location":"usage/ipv6/#create-deployment-by-ippool","title":"Create Deployment By IPPool","text":"<ol> <li> <p>Create IPPool</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: custom-ipv4-ippool\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.50\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: custom-ipv6-ippool\nspec:\nsubnet: fd00:172:18::/64\nips:\n- fd00:172:18::40-fd00:172:18::50\n</code></pre> </li> <li> <p>Create Deployment</p> <p>create a Deployment whose Pods are setting the Pod annotation <code>ipam.spidernet.io/ippool</code> to  explicitly specify the pool.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: custom-dual-ippool-deploy\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: custom-dual-ippool-deploy\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"]\n}\nlabels:\napp: custom-dual-ippool-deploy\nspec:\ncontainers:\n- name: custom-dual-ippool-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>The Pods are running.</p> <pre><code>kubectl get pod -owide -l app=custom-dual-ippool-deploy\nNAME                                        READY   STATUS    RESTARTS   AGE   IP             NODE                NOMINATED NODE   READINESS GATES\ncustom-dual-ippool-deploy-9bb6696c4-6wjnl   1/1     Running   0          76s   172.18.41.42   controller-node-1   &lt;none&gt;           &lt;none&gt;\ncustom-dual-ippool-deploy-9bb6696c4-8vtpf   1/1     Running   0          76s   172.18.41.45   controller-node-1   &lt;none&gt;           &lt;none&gt;\ncustom-dual-ippool-deploy-9bb6696c4-zbknv   1/1     Running   0          76s   172.18.41.43   controller-node-1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>View all IPs of Pods</p> <pre><code>kubectl get pod -l app=custom-dual-ippool-deploy  -o go-template='{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}'\ncustom-dual-ippool-deploy-9bb6696c4-6wjnl: map[ip:172.18.41.42] map[ip:fd00:172:18::4d]\ncustom-dual-ippool-deploy-9bb6696c4-8vtpf: map[ip:172.18.41.45] map[ip:fd00:172:18::4e]\ncustom-dual-ippool-deploy-9bb6696c4-zbknv: map[ip:172.18.41.43] map[ip:fd00:172:18::46]\n</code></pre> </li> </ol>"},{"location":"usage/ipv6/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/multi-interfaces-annotation/","title":"Pod annotation of multi-NIC","text":"<p>When assigning multiple NICs to a pod with Multus CNI, Spiderpool supports to specify the IP pools for each interface.</p> <p>This feature supports to implement by annotation <code>ipam.spidernet.io/subnets</code> and <code>ipam.spidernet.io/ippools</code> </p>"},{"location":"usage/multi-interfaces-annotation/#get-started","title":"Get Started","text":"<p>The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a pod with two NICs with IP in different subnets.</p>"},{"location":"usage/multi-interfaces-annotation/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>Follow the guide installation to install Spiderpool. </p>"},{"location":"usage/multi-interfaces-annotation/#set-up-multus-configuration","title":"Set up Multus Configuration","text":"<p>In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed:</p> <ul> <li> <p>Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use.</p> </li> <li> <p>In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command <code>kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service</code>.</p> </li> </ul> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml\n\n~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system\nNAME                  AGE\nmacvlan-conf-ens192   20s\nmacvlan-conf-ens224   22s\n</code></pre>"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-subnet","title":"multiple NICs by subnet","text":"<p>Create two Subnets to provide IP addresses for different interfaces.</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml\n\n~# kubectl get spidersubnet\nNAME                 VERSION   SUBNET        ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nsubnet-test-ens192   4         10.6.0.1/16   0                    10\nsubnet-test-ens224   4         10.7.0.1/16   0                    10\n</code></pre> <p>In the following example Yaml, 2 copies of the Deployment are created\uff1a</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml\n</code></pre> <p>Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs.</p> <pre><code>~# kubectl get spiderippool\nNAME                                 VERSION   SUBNET        ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-test-app-v4-eth0-b1a361c7e9df   4         10.6.0.1/16   2                    3                false     false\nauto-test-app-v4-net1-b1a361c7e9df   4         10.7.0.1/16   2                    3                false     false\n\n~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath='{.spec.ips}'\n[\"10.6.168.171-10.6.168.173\"]\n\n~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath='{.spec.ips}'\n[\"10.7.168.171-10.7.168.173\"]\n\n~# kubectl get po -l app=test-app -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES\ntest-app-6f4594ff67-fkqbw   1/1     Running   0          40s   10.6.168.172   node2   &lt;none&gt;           &lt;none&gt;\ntest-app-6f4594ff67-gwlx8   1/1     Running   0          40s   10.6.168.173   node1   &lt;none&gt;           &lt;none&gt;\n\n~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a\n3: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0\ninet 10.6.168.172/16 brd 10.6.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n4: veth0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether 26:6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n5: net1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0\ninet 10.7.168.173/16 brd 10.7.255.255 scope global net1\n       valid_lft forever preferred_lft forever\n</code></pre> <p>The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs.</p> <pre><code>~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show\n0:  from all lookup local\n32764:  from 10.7.168.173 lookup 100\n32765:  from all to 10.7.168.173/16 lookup 100\n32766:  from all lookup main\n32767:  from all lookup default\n\n~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main\ndefault via 10.6.0.1 dev eth0\n\n~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100\ndefault via 10.7.0.1 dev net1\n10.6.168.123 dev veth0 scope link\n10.7.0.0/16 dev net1 proto kernel scope link src 10.7.168.173\n10.96.0.0/12 via 10.6.168.123 dev veth0\n</code></pre>"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-ippool","title":"multiple NICs by IPPool","text":"<p>Create two IPPools to provide IP addresses for different interfaces.</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml\n\n~# kubectl get spiderippool\nNAME                   VERSION   SUBNET        ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nippool-test-ens192     4         10.6.0.1/16   0                    5                false     false\nippool-test-ens224     4         10.7.0.1/16   0                    5                false     false\n</code></pre> <p>In the following example Yaml, 1 copies of the Deployment are created\uff1a</p> <pre><code>~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml\n</code></pre> <p>Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs.</p> <pre><code>~# kubectl get spiderippool\nNAME                   VERSION   SUBNET        ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nippool-test-ens192     4         10.6.0.1/16   1                    5                false     false\nippool-test-ens224     4         10.7.0.1/16   1                    5                false     false\n\n~# kubectl get po -l app=ippool-test-app -o wide\nNAME                               READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES\nippool-test-app-65f646574c-mpr47   1/1     Running   0          6m18s   10.6.168.175   node2   &lt;none&gt;           &lt;none&gt;\n\n~#  kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a\n...\n3: eth0@tunl0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue\n    link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff\n    inet 10.6.168.175/16 brd 10.6.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n4: veth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue\n    link/ether 86:ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff\n5: net1@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue\n    link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff\n    inet 10.7.168.177/16 brd 10.7.255.255 scope global net1\n       valid_lft forever preferred_lft forever\n</code></pre> <p>The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs.</p> <pre><code>~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show\n0: from all lookup local\n32764: from 10.7.168.177 lookup 100\n32765: from all to 10.7.168.177/16 lookup 100\n32766: from all lookup main\n32767: from all lookup default\n\n~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main\ndefault via 10.6.0.1 dev eth0\n\n~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100\ndefault via 10.7.0.1 dev net1\n10.6.168.123 dev veth0 scope link\n10.7.0.0/16 dev net1 scope link  src 10.7.168.177\n10.96.0.0/12 via 10.6.168.123 dev veth0\n</code></pre>"},{"location":"usage/multi-interfaces-annotation/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/performance-zh_CH/","title":"Spiderpool \u6027\u80fd\u6d4b\u8bd5","text":"<p>Spiderpool \u662f\u4e00\u4e2a\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684\u9ad8\u6027\u80fd IPAM CNI \u63d2\u4ef6\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684 underlay IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts\uff0cKube-OVN\uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u5728 \u201d1000 Pod\u201c \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002</p>"},{"location":"usage/performance-zh_CH/#_1","title":"\u80cc\u666f","text":"<p>\u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f</p> <ol> <li>IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7684\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002</li> <li>\u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002</li> <li>underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u7684\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u7684\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002</li> </ol>"},{"location":"usage/performance-zh_CH/#_2","title":"\u73af\u5883","text":"<ul> <li>Kubernetes: <code>v1.25.4</code></li> <li>container runtime: <code>containerd 1.6.12</code></li> <li>OS: <code>CentOS Linux 8</code></li> <li>kernel: <code>4.18.0-348.7.1.el8_5.x86_64</code></li> </ul> Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi"},{"location":"usage/performance-zh_CH/#_3","title":"\u6d4b\u8bd5\u5bf9\u8c61","text":"<p>\u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e <code>0.3.1</code> \u7248\u672c\u7684 CNI Specification\uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5bf9\u63a5 underlay \u7f51\u7edc\u7684\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a</p> Main CNI Main CNI \u7248\u672c IPAM CNI IPAM CNI \u7248\u672c \u7279\u70b9 macvlan <code>v1.1.1</code> Spiderpool <code>v0.4.1</code> \u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u4e2a IP \u6c60\uff0c\u6bcf\u4e2a\u6c60\u4e2d\u7684 IP \u5730\u5740\u90fd\u53ef\u4ee5\u88ab\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u7684 Pod \u6240\u4f7f\u7528\uff0c\u5f53\u96c6\u7fa4\u4e2d\u7684\u591a\u4e2a Pod \u5e76\u53d1\u7684\u4ece\u540c\u4e00\u4e2a\u6c60\u4e2d\u5206\u914d IP \u5730\u5740\u65f6\uff0c\u5b58\u5728\u7ade\u4e89\u3002\u652f\u6301\u6258\u7ba1 IP \u6c60\u7684\u5168\u751f\u547d\u6d41\u7a0b\uff0c\u4f7f\u5176\u540c\u6b65\u7684\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u521b\u5efa\u3001\u6269\u7f29\u5bb9\u3001\u5220\u9664\uff0c\u5f31\u5316\u4e86\u8fc7\u5927\u7684\u5171\u4eab\u6c60\u6240\u5e26\u6765\u7684\u5e76\u53d1\u6216\u5b58\u50a8\u95ee\u9898\u3002 macvlan <code>v1.1.1</code> Whereabouts (CRD backend) <code>v0.6.1</code> \u5404\u8282\u70b9\u53ef\u4ee5\u5b9a\u4e49\u5404\u81ea\u53ef\u7528\u7684 IP \u6c60\u8303\u56f4\uff0c\u82e5\u8282\u70b9\u95f4\u5b58\u5728\u91cd\u590d\u5b9a\u4e49\u7684 IP \u5730\u5740\uff0c\u90a3\u4e48\u8fd9\u4e9b IP \u5730\u5740\u4e0a\u5347\u4e3a\u4e00\u79cd\u5171\u4eab\u8d44\u6e90\u3002 Kube-OVN (underlay) <code>v1.11.3</code> Kube-OVN <code>v1.11.3</code> \u4ee5\u5b50\u7f51\u6765\u7ec4\u7ec7 IP \u5730\u5740\uff0c\u6bcf\u4e2a Namespace \u53ef\u4ee5\u5f52\u5c5e\u4e8e\u7279\u5b9a\u7684\u5b50\u7f51\uff0c Namespace \u4e0b\u7684 Pod \u4f1a\u81ea\u52a8\u4ece\u6240\u5c5e\u7684\u5b50\u7f51\u4e2d\u83b7\u53d6 IP \u5730\u5740\u3002\u5b50\u7f51\u4e5f\u662f\u4e00\u79cd\u96c6\u7fa4\u8d44\u6e90\uff0c\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u5730\u5740\u53ef\u4ee5\u5206\u5e03\u5728\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u3002 Calico <code>v3.23.3</code> calico-ipam (CRD backend) <code>v3.23.3</code> \u6bcf\u4e2a\u8282\u70b9\u72ec\u4eab\u4e00\u4e2a\u6216\u591a\u4e2a IP block\uff0c\u5404\u8282\u70b9\u4e0a\u7684 Pod \u4ec5\u4f7f\u7528\u672c\u5730 IP block \u4e2d\u7684 IP \u5730\u5740\uff0c\u8282\u70b9\u95f4\u65e0\u4efb\u4f55\u7ade\u4e89\u4e0e\u51b2\u7a81\uff0c\u5206\u914d\u7684\u6548\u7387\u975e\u5e38\u9ad8\u3002"},{"location":"usage/performance-zh_CH/#_4","title":"\u65b9\u6848","text":"<p>\u6d4b\u8bd5\u671f\u95f4\uff0c\u6211\u4eec\u4f1a\u9075\u5faa\u5982\u4e0b\u7ea6\u5b9a\uff1a</p> <ul> <li>\u6d4b\u8bd5 IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002</li> <li>\u6d4b\u8bd5 underlay IPAM CNI \u63d2\u4ef6\u65f6\uff0c\u5c3d\u6700\u5927\u53ef\u80fd\u7684\u786e\u4fdd\u53ef\u7528\u7684 IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u91cf\u4e3a 1:1\u3002\u4f8b\u5982\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u5212\u521b\u5efa 1000 \u4e2a Pod\uff0c\u90a3\u4e48\u5e94\u5f53\u9650\u5236\u53ef\u7528\u7684 IPv4/IPv6 \u5730\u5740\u6570\u91cf\u5747\u4e3a 1000 \u4e2a\u3002</li> </ul> <p>\u5177\u4f53\u7684\uff0c\u6211\u4eec\u4f1a\u5c1d\u8bd5\u4ee5\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5728\u4e0a\u8ff0 Kubenetes \u96c6\u7fa4\u4e0a\u6765\u542f\u52a8\u603b\u8ba1 1000 \u4e2a Pod\uff0c\u5e76\u8bb0\u5f55\u6240\u6709 Pod \u5747\u8fbe\u5230 <code>Running</code> \u7684\u8017\u65f6\uff1a</p> <ul> <li>\u4ec5\u521b\u5efa\u4e00\u4e2a Deployment\uff0c\u5176\u526f\u672c\u6570\u4e3a 1000\u3002</li> <li>\u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002</li> </ul> <p>\u7136\u540e\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e00\u6b21\u6027\u7684\u5220\u9664\u8fd9 1000 \u4e2a Pod\uff0c\u8bb0\u5f55\u88ab\u91cd\u5efa\u7684 1000 \u4e2a Pod \u5168\u90e8 <code>Running</code> \u7684\u8017\u65f6\uff1a</p> <pre><code>kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u5c06\u6240\u6709\u8282\u70b9\u4e0b\u7535\u540e\u518d\u4e0a\u7535\uff0c\u6a21\u62df\u6545\u969c\u6062\u590d\uff0c\u8bb0\u5f55 1000 \u4e2a Pod \u518d\u6b21\u8fbe\u5230 <code>Running</code> \u7684\u8017\u65f6\u3002</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u5220\u9664\u6240\u6709\u7684 Deployment\uff0c\u8bb0\u5f55\u6240\u6709 Pod \u5b8c\u5168\u6d88\u5931\u7684\u8017\u65f6\u3002</p>"},{"location":"usage/performance-zh_CH/#_5","title":"\u7ed3\u679c","text":""},{"location":"usage/performance-zh_CH/#ipv4ipv6","title":"IPv4/IPv6 \u53cc\u6808","text":"<ul> <li>\u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a</li> </ul> CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s \u5931\u8d25 \u5931\u8d25 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s <p>\u5728\u6d4b\u8bd5 macvlan + Whereabouts \u8fd9\u4e2a\u7ec4\u5408\u671f\u95f4\uff0c\u521b\u5efa\u7684\u573a\u666f\u4e2d 922 \u4e2a Pod \u5728 14m25s \u5185\u4ee5\u8f83\u4e3a\u5747\u5300\u7684\u901f\u7387\u8fbe\u5230 <code>Running</code> \u72b6\u6001\uff0c\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u6700\u7ec8 1000 \u4e2a Pod \u82b1\u4e86 25m18s \u8fbe\u5230 <code>Running</code> \u72b6\u6001\u3002\u81f3\u4e8e\u91cd\u5efa\u7684\u573a\u666f\uff0c\u5728 55 \u4e2a Pod \u8fbe\u5230 <code>Running</code> \u72b6\u6001\u540e\uff0cWhereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002</p> <ul> <li>100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a</li> </ul> CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s \u5931\u8d25 \u5931\u8d25 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s"},{"location":"usage/performance-zh_CH/#ipv4","title":"IPv4 \u5355\u6808","text":"<ul> <li>\u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a</li> </ul> CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s <ul> <li>100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a</li> </ul> CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s"},{"location":"usage/performance-zh_CH/#_6","title":"\u5c0f\u7ed3","text":"<p>\u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684 IPAM CNI \u63d2\u4ef6\uff0c\u5176\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7684 overlay IPAM CNI \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u4ea6\u4e0d\u900a\u8272\u4e8e\u540e\u8005\u3002</p>"},{"location":"usage/performance/","title":"Spiderpool Performance Testing","text":"<p>Spiderpool is a high-performance IPAM CNI plugin for underlay networks. This report will compare its performance with the mainstream underlay IPAM CNI plugins (such as  Whereabouts, Kube-OVN) and the widely used overlay IPAM CNI plugin calico-ipam under the \"1000 Pod\" scenario.</p>"},{"location":"usage/performance/#background","title":"Background","text":"<p>Why do we need to do performance testing on the underlay IPAM CNI plugin?</p> <ol> <li>The speed at which IPAM allocates IP addresses largely determines the speed of application publishing.</li> <li>Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures.</li> <li>Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources.</li> </ol>"},{"location":"usage/performance/#env","title":"ENV","text":"<ul> <li>Kubernetes: <code>v1.25.4</code></li> <li>container runtime: <code>containerd 1.6.12</code></li> <li>OS: <code>CentOS Linux 8</code></li> <li>kernel: <code>4.18.0-348.7.1.el8_5.x86_64</code></li> </ul> Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi"},{"location":"usage/performance/#objects","title":"Objects","text":"<p>This test is based on the CNI Specification <code>0.3.1</code>, using macvlanwith Spiderpool, and selecting several other underlay network solutions in the open source community for comparison:</p> Main CNI Main CNI Version IPAM CNI IPAM CNI Version Features macvlan <code>v1.1.1</code> Spiderpool <code>v0.4.1</code> There are multiple IP pools in a cluster, and the IP addresses in each pool can be used by Pods on any Node in the cluster. Competition occurs when multiple Pods in a cluster allocate IP addresses from the same pool. Support hosting the entire lifecycle of an IP pool, synchronizing it with workload creation, scaling, deletion, and reducing concurrency or storage issues caused by overly large shared pools. macvlan <code>v1.1.1</code> Whereabouts (CRD backend) <code>v0.6.1</code> Each Node can define its own available IP pool ranges. If there are duplicate IP addresses defined between Nodes, these IP addresses are promoted as a shared resource. Kube-OVN (underlay) <code>v1.11.3</code> Kube-OVN <code>v1.11.3</code> IP addresses are organized by Subnet. Each Namespace can belong to a specific Subnet. Pods under the Namespace will automatically obtain IP addresses from the Subnet they belong to. Subnets are also a cluster resource, and the IP addresses of the same Subnet can be distributed on any Node. Calico <code>v3.23.3</code> calico-ipam (CRD backend) <code>v3.23.3</code> Each Node has one or more IP blocks exclusively, and the Pods on each Node only use the IP addresses in the local IP block. There is no competition or conflict between Nodes, and the allocation efficiency is very high."},{"location":"usage/performance/#plan","title":"Plan","text":"<p>During the testing, we will follow the following agreement:</p> <ul> <li>Testing under IPv4 stack and IPv4/IPv6 dual-stack.</li> <li>When testing the underlay IPAM CNI plugins, ensure that the number of available IP addresses is 1:1 to the number of Pods as much as possible. For example, if we plan to create 1000 Pods in the next step, we should limit the number of available IPv4/IPv6 addresses to 1000.</li> </ul> <p>Specifically, we will attempt to create a total of 1000 Pods on the Kubernetes cluster in the following two ways, and record the time taken for all Pods to become <code>Running</code>:</p> <ul> <li>Create only one Deployment with 1000 replicas.</li> <li>Create 100 Deployments with 10 replicas per Deployment.</li> </ul> <p>Then, we will use the following command to delete these 1000 Pods at once, and record the time it took for all Pods to become <code>Running</code>:</p> <pre><code>kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod\n</code></pre> <p>Next, power down all nodes and then power up, simulate fault recovery, and record the time it took for all Pods to become <code>Running</code> again.</p> <p>Finally, we delete all Deployments and record the time taken for all Pods to completely disappear.</p>"},{"location":"usage/performance/#result","title":"Result","text":""},{"location":"usage/performance/#ipv4ipv6-dual-stack","title":"IPv4/IPv6 dual-stack","text":"<ul> <li>1 Deployment with 1000 replicas: </li> </ul> CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s failure failure 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s <p>During the testing of macvlan + Whereabouts, in the creation scenario, 922 Pods became <code>Running</code> at a relatively uniform rate within 14m25s. After that, the growth rate of Pods significantly decreased, and ultimately it took 25m18s for 1000 Pods to become <code>Running</code>. As for the re-creation scenario, after 55 Pods became <code>Running</code>, Whereabouts basically stopped working, and the time consumption was close to infinity.</p> <ul> <li>100 Deployments with 10 replicas: </li> </ul> CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s failure failure 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s"},{"location":"usage/performance/#ipv4-stack","title":"IPv4 stack","text":"<ul> <li>1 Deployment with 1000 replicas: </li> </ul> CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s failure failure 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s <ul> <li>100 Deployments with 10 replicas per Deployment: </li> </ul> CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s failure failure 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s"},{"location":"usage/performance/#summary","title":"Summary","text":"<p>Although Spiderpool is an IPAM CNI plugin suitable for underlay networks, it faces more complex IP address preemption and conflict issues than mainstream overlay IPAM CNI plugins, but its performance in most scenarios is not inferior to the latter.</p>"},{"location":"usage/reserved-ip/","title":"Reserved IP","text":"<p>Spiderpool reserve some IP addresses for the whole Kubernetes cluster, which will not be used by any IPAM allocation results. Typically, these IP addresses are external IP addresses or cannot be used for network communication (e.g. broadcast address).</p>"},{"location":"usage/reserved-ip/#ippool-excludeips","title":"IPPool excludeIPs","text":"<p>You may have observed that there is a field <code>excludeIPs</code> in SpiderIPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main function is not like this. Field <code>excludeIPs</code> is more of a syntax sugar, so that users can more flexibly define the IP address ranges of the IPPool.</p> <p>For example, create an IPPool without using <code>excludeIPs</code>, which contains two IP ranges: <code>172.18.41.40-172.18.41.44</code> and <code>172.18.41.46-172.18.41.50</code>, you should define the <code>ips</code> as follows:</p> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: not-use-excludeips\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.44\n- 172.18.41.46-172.18.41.50\n</code></pre> <p>But in fact, this semantics can be more succinctly described through <code>excludeIPs</code>:</p> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: use-excludeips\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.50\nexcludeIPs:\n- 172.18.41.45\n</code></pre> <p>Field <code>excludeIPs</code> will make sure that any Pod that allocates IP addresses from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with <code>excludeIPs</code> defined.</p>"},{"location":"usage/reserved-ip/#use-spiderreservedip","title":"Use SpiderReservedIP","text":"<p>Unlike configuring field <code>excluedIPs</code> in SpiderIPPool CR, creating a SpiderReservedIP CR is really a way to define the global reserved IP address rules of a Kubernetes cluster. The IP addresses defined in ReservedIP cannot be used by any Pod in the cluster, regardless of whether some IPPools have inadvertently defined them. More details refer to definition of SpiderReservedIP.</p>"},{"location":"usage/reserved-ip/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.</p>"},{"location":"usage/reserved-ip/#get-started","title":"Get Started","text":"<p>To understand how it works, let's do such a test. First, create an ReservedIP which reserves 9 IP addresses from <code>172.18.42.41</code> to <code>172.18.42.49</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderReservedIP\nmetadata:\nname: test-ipv4-reservedip\nspec:\nips:\n- 172.18.42.41-172.18.42.49\n</code></pre> <p>At the same time, create an IPPool with 10 IP addresses from <code>172.18.42.41</code> to <code>172.18.42.50</code>. Yes, we deliberately make it hold one more IP address than the ReservedIP above.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: test-ipv4-ippool\nspec:\nsubnet: 172.18.42.0/24\nips:\n- 172.18.42.41-172.18.42.50\n</code></pre> <p>Then, create a Deployment with 3 replicas and allocate IP addresses to its Pods from the IPPool above.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: reservedip-deploy\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: reservedip-deploy\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"test-ipv4-ippool\"]\n}\nlabels:\napp: reservedip-deploy\nspec:\ncontainers:\n- name: reservedip-deploy\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>After a while, only one of these Pods using IP <code>172.18.42.50</code> can run successfully because \"all IP used out\".</p> <pre><code>kubectl get po -l app=reservedip-deploy -o wide\nNAME                                 READY   STATUS              RESTARTS   AGE   IP             NODE            \nreservedip-deploy-6cf9858886-cm7bp   0/1     ContainerCreating   0          35s   &lt;none&gt;         spider-worker\nreservedip-deploy-6cf9858886-lb7cr   0/1     ContainerCreating   0          35s   &lt;none&gt;         spider-worker\nreservedip-deploy-6cf9858886-pkcfl   1/1     Running             0          35s   172.18.42.50   spider-worker\n</code></pre> <p>But when you delete this ReservedIP, everything will return to normal.</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml\n</code></pre> <p>Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle its IP addresses, but ReservedIP will still ensure that when the Pod is terminated, no other Pods can continue to use the reserved IP address.</p> <p>Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work.</p>"},{"location":"usage/reserved-ip/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again.</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/reserved-ip/#a-trap","title":"A Trap","text":"<p>So, can you use IPPool's field <code>excludeIPs</code> to achieve the same effect as ReservedIP? The answer is NO! Look at such a case, now you want to reserve an IP <code>172.18.43.31</code> for an external application of the Kubernetes cluster, which may be a Redis node. To achieve this, you created such an IPPool:</p> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: already-in-use\nspec:\nsubnet: 172.18.43.0/24\nips:\n- 172.18.43.1-172.18.43.31\nexcludeIPs:\n- 172.18.43.31\n</code></pre> <p>I believe that if there is only one IPPool under the subnet <code>172.18.43.0/24</code> network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool:</p> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: created-by-someone-else\nspec:\nsubnet: 172.18.43.0/24\nips:\n- 172.18.43.31-172.18.43.50\n</code></pre> <p>Different IPPools allow to define the same field <code>subnet</code>, more details refer to validation of IPPool.</p> <p>After a period of time, a Pod may be allocated with IP <code>172.18.43.31</code> from the IPPool <code>created-by-someone-else</code>, and then it holds the same IP address as your Redis node. After that, the Redis may not work as well.</p> <p>So, if you really want to reserve an IP address instead of excluding an IP address, SpiderReservedIP makes life better.</p>"},{"location":"usage/route/","title":"Route support","text":""},{"location":"usage/route/#description","title":"Description","text":"<p>Spiderpool supports the configuration of routing information.</p>"},{"location":"usage/route/#get-started","title":"Get Started","text":""},{"location":"usage/route/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>follow the guide installation to install Spiderpool.</p>"},{"location":"usage/route/#create-subnet","title":"Create Subnet","text":"<p>Create a SpiderSubnet and set up a subnet routes, a Pod and get the IP address from the AutoIPPool of the subnet, then see the routes configured in the subnet that exist in the AutoIPPool and Pod.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderSubnet\nmetadata:\nname: ipv4-subnet-route\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.41-172.18.41.60\nroutes:\n- dst: 172.18.42.0/24\ngw: 172.18.41.1\n</code></pre>"},{"location":"usage/route/#create-deployment-by-spidersubnet","title":"Create Deployment By SpiderSubnet","text":"<p>Create a Deployment whose Pods sets the Pod annotation <code>ipam.spidernet.io/subnet</code> to explicitly specify the subnet.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: subnet-test-app\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: subnet-test-app\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/subnet: |-\n{\n\"ipv4\": [\"ipv4-subnet-route\"]\n}\nlabels:\napp: subnet-test-app\nspec:\ncontainers:\n- name: route-test\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges.</p> <pre><code>~# kubectl get spiderpool\nNAME                                        VERSION   SUBNET           ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE\nauto-subnet-test-app-v4-eth0-d69f2fb7bccf   4         172.18.41.0/24   1                    1                false     false\n\n~# kubectl get spiderpool auto-subnet-test-app-v4-eth0-d69f2fb7bccf -oyaml\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\n...\n  ips:\n  - 172.18.41.41\n  podAffinity:\n    matchLabels:\n      app: subnet-test-app\n  routes:\n  - dst: 172.18.42.0/24\n    gw: 172.18.41.1\n  subnet: 172.18.41.0/24\n...\n</code></pre> <p>The Pods are running.</p> <pre><code>~# kubectl get pod -l app=subnet-test-app -owide\nNAME                               READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES\nsubnet-test-app-59df44fc57-clp8t   1/1     Running   0          3m48s   172.18.41.41   spider-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>After the created Pod has obtained an IP from the automatic IPPool, the route set in the subnet, which is inherited by the automatic pool and takes effect in the Pod, you can view it via IP r as follows:</p> <pre><code>~# kubectl exec -it route-test-app-bdc84f8f5-2bxbr  -- ip r\n172.18.41.0/24 dev eth0 scope link  src 172.18.41.41 172.18.42.0/24 via 172.18.41.1 dev eth0 </code></pre>"},{"location":"usage/route/#create-ippool","title":"Create IPPool","text":"<p>Create a SpiderIPPool and set up the routes for the IPPool, create the Pod and assign IP addresses from the IPPool, you can see the routing information in the ippool pool taking effect within the Pod.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: ipv4-ippool-route\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.51-172.18.41.60\nroutes:\n- dst: 172.18.42.0/24\ngw: 172.18.41.1\n</code></pre>"},{"location":"usage/route/#create-deployment-by-ippool","title":"Create Deployment By IPPool","text":"<p>Create a Deployment whose Pods sets the Pod annotation <code>ipam.spidernet.io/ippool</code> to  explicitly specify the IPPool.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: ippool-test-app\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: ippool-test-app\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"ipv4-ippool-route\"]\n}\nlabels:\napp: ippool-test-app\nspec:\ncontainers:\n- name: route-test\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>The Pods are running.</p> <pre><code>~# kubectl get pod -l app=ippool-test-app -owide\nNAME                               READY   STATUS    RESTARTS   AGE   IP             NODE            NOMINATED NODE   READINESS GATES\nippool-test-app-66fd47d895-pthx5   1/1     Running   0          45s   172.18.41.53   spider-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>After the created Pod has obtained an IP from IPPool, the route set in IPPool is already in effect in the Pod and you can view it via IP r as follows:</p> <pre><code>~# kubectl exec -it ippool-test-app-66fd47d895-pthx5  -- ip r\n172.18.41.0/24 dev eth0 scope link  src 172.18.41.53 172.18.42.0/24 via 172.18.41.1 dev eth0 </code></pre>"},{"location":"usage/route/#clean-up","title":"Clean up","text":"<p>Clean the relevant resources so that you can run this tutorial again</p> <pre><code>kubectl delete \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml \\\n-f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml \\\n--ignore-not-found=true\n</code></pre>"},{"location":"usage/spider-subnet/","title":"SpiderSubnet","text":"<p>The Spiderpool owns a CRD SpiderSubnet, which can help applications (such as Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet) to create a corresponding SpiderIPPool.</p> <p>Here are some annotations that you should write down on the application template pod annotation:</p> Annotation Description Example ipam.spidernet.io/subnet Choose one SpiderSubnet V4 and V6 CR to use {{\"interface\":\"eth0\", \"ipv4\":[\"subnet-demo-v4\"], \"ipv6\":[\"subnet-demo-v6\"]} ipam.spidernet.io/subnets Choose multiple SpiderSubnet V4 and V6 CR to use (the current version only supports to use the first one) [{\"interface\":\"eth0\", \"ipv4\":[\"v4-subnet1\"], \"ipv6\":[\"v6-subnet1\"]}] ipam.spidernet.io/ippool-ip-number The IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1') +2 ipam.spidernet.io/ippool-reclaim Specify the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true') true"},{"location":"usage/spider-subnet/#notice","title":"Notice","text":"<ol> <li> <p>The annotation <code>ipam.spidernet.io/subnets</code> has higher priority over <code>ipam.spidernet.io/subnet</code>.    If you specify both of them two, it only uses <code>ipam.spidernet.io/subnets</code> mode.</p> </li> <li> <p>In annotation <code>ipam.spidernet.io/subnet</code> mode, it will use default interface name <code>eth0</code> if you do not set <code>interface</code> property.</p> </li> <li> <p>For annotation <code>ipam.spidernet.io/ippool-ip-number</code>, you can use '2' for fixed IP number or '+2' for flexible mode.    The value '+2' means the SpiderSubnet auto-created IPPool will add 2 more IPs based on your application replicas.    If you choose to use flexible mode, the auto-created IPPool IPs will expand or shrink dynamically by your application replicas.    This is an optional annotation. If left unset, it will use the <code>clusterSubnetDefaultFlexibleIPNumber</code> property from the <code>spiderpool-conf</code> ConfigMap as the flexible IP number in flexible mode. Refer to config for details.</p> </li> <li> <p>The current version only supports to use one SpiderSubnet V4/V6 CR, you shouldn't specify 2 or more SpiderSubnet V4 CRs and the spiderpool-controller will choose the first one to use.</p> </li> </ol>"},{"location":"usage/spider-subnet/#get-started","title":"Get Started","text":""},{"location":"usage/spider-subnet/#enable-spidersubnet-feature","title":"Enable SpiderSubnet feature","text":"<p>Firstly, please ensure you have installed the Spiderpool and configure the CNI file, refer to install for details.</p> <p>Check configmap <code>spiderpool-conf</code> property <code>enableSpiderSubnet</code> whether is already set to <code>true</code> or not.</p> <pre><code>kubectl -n kube-system get configmap spiderpool-conf -o yaml\n</code></pre> <p>If you want to set it <code>true</code>, just execute <code>helm upgrade spiderpool spiderpool/spiderpool --set feature.enableSpiderSubnet=true -n kube-system</code>.</p>"},{"location":"usage/spider-subnet/#create-a-spidersubnet","title":"Create a SpiderSubnet","text":"<p>Install a SpiderSubnet example:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/subnet-demo.yaml\nkubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/deploy-use-subnet.yaml\n</code></pre>"},{"location":"usage/spider-subnet/#check-the-related-cr-data","title":"Check the related CR data","text":"<p>Here's the SpiderSubnet, SpiderIPPool, and Pod information.</p> <pre><code>$ kubectl get ss\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nsubnet-demo-v4      4         172.16.0.0/16             3                    200\nsubnet-demo-v6      6         fc00:f853:ccd:e790::/64   3                    200\n\n$ kubectl get sp -o wide\nNAME                                  VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE   APP-NAMESPACE\nauto4-demo-deploy-subnet-eth0-337bc   4         172.16.0.0/16             1                    3                false     false     default\nauto6-demo-deploy-subnet-eth0-337bc   6         fc00:f853:ccd:e790::/64   1                    3                false     false     default\n------------------------------------------------------------------------------------------\n$ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\n  creationTimestamp: \"2023-05-09T09:25:24Z\"\n  finalizers:\n  - spiderpool.spidernet.io\n  generation: 1\n  labels:\n    ipam.spidernet.io/interface: eth0\n    ipam.spidernet.io/ip-version: IPv4\n    ipam.spidernet.io/ippool-cidr: 172-16-0-0-16\n    ipam.spidernet.io/ippool-reclaim: \"true\"\n    ipam.spidernet.io/owner-application-gv: apps_v1\n    ipam.spidernet.io/owner-application-kind: Deployment\n    ipam.spidernet.io/owner-application-name: demo-deploy-subnet\n    ipam.spidernet.io/owner-application-namespace: default\n    ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41\n    ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4\n  name: auto4-demo-deploy-subnet-eth0-337bc\n  ownerReferences:\n  - apiVersion: spiderpool.spidernet.io/v2beta1\n    blockOwnerDeletion: true\n    controller: true\n    kind: SpiderSubnet\n    name: subnet-demo-v4\n    uid: 3f5882a0-12e6-40f5-a103-431de700195f\n  resourceVersion: \"1381\"\n  uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa\nspec:\n  default: false\n  disable: false\n  ipVersion: 4\n  ips:\n  - 172.16.41.1-172.16.41.3\n  podAffinity:\n    matchLabels:\n      ipam.spidernet.io/app-api-group: apps\n      ipam.spidernet.io/app-api-version: v1\n      ipam.spidernet.io/app-kind: Deployment\n      ipam.spidernet.io/app-name: demo-deploy-subnet\n      ipam.spidernet.io/app-namespace: default\n  subnet: 172.16.0.0/16\n  vlan: 0\nstatus:\n  allocatedIPCount: 1\n  allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"}}'\n  totalIPCount: 3\n\n$ kubectl get po -o wide \nNAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES\ndemo-deploy-subnet-778786474b-kls7s   1/1     Running   0          3m10s   172.16.41.1    spider-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Try to scale the deployment replicas and check the SpiderIPPool.</p> <pre><code>$ kubectl patch deploy demo-deploy-subnet --patch '{\"spec\": {\"replicas\": 2}}'\ndeployment.apps/demo-deploy-subnet patched\n------------------------------------------------------------------------------------------\n$ kubectl get ss\nNAME                VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT\nsubnet-demo-v4      4         172.16.0.0/16             4                    200\nsubnet-demo-v6      6         fc00:f853:ccd:e790::/64   4                    200\n------------------------------------------------------------------------------------------\n$ kubectl get sp -o wide\nNAME                                  VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE   APP-NAMESPACE\nauto4-demo-deploy-subnet-eth0-337bc   4         172.16.0.0/16             2                    4                false     false     default\nauto6-demo-deploy-subnet-eth0-337bc   6         fc00:f853:ccd:e790::/64   2                    4                false     false     default\n------------------------------------------------------------------------------------------\n$ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml\napiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\n  creationTimestamp: \"2023-05-09T09:25:24Z\"\n  finalizers:\n  - spiderpool.spidernet.io\n  generation: 2\n  labels:\n    ipam.spidernet.io/interface: eth0\n    ipam.spidernet.io/ip-version: IPv4\n    ipam.spidernet.io/ippool-cidr: 172-16-0-0-16\n    ipam.spidernet.io/ippool-reclaim: \"true\"\n    ipam.spidernet.io/owner-application-gv: apps_v1\n    ipam.spidernet.io/owner-application-kind: Deployment\n    ipam.spidernet.io/owner-application-name: demo-deploy-subnet\n    ipam.spidernet.io/owner-application-namespace: default\n    ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41\n    ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4\n  name: auto4-demo-deploy-subnet-eth0-337bc\n  ownerReferences:\n  - apiVersion: spiderpool.spidernet.io/v2beta1\n    blockOwnerDeletion: true\n    controller: true\n    kind: SpiderSubnet\n    name: subnet-demo-v4\n    uid: 3f5882a0-12e6-40f5-a103-431de700195f\n  resourceVersion: \"2004\"\n  uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa\nspec:\n  default: false\n  disable: false\n  ipVersion: 4\n  ips:\n  - 172.16.41.1-172.16.41.4\n  podAffinity:\n    matchLabels:\n      ipam.spidernet.io/app-api-group: apps\n      ipam.spidernet.io/app-api-version: v1\n      ipam.spidernet.io/app-kind: Deployment\n      ipam.spidernet.io/app-name: demo-deploy-subnet\n      ipam.spidernet.io/app-namespace: default\n  subnet: 172.16.0.0/16\n  vlan: 0\nstatus:\n  allocatedIPCount: 2\n  allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"},\"172.16.41.2\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-2kf24\",\"podUid\":\"0a239f85-be47-4b98-bf29-c8bf02b6b59e\"}}'\n  totalIPCount: 4\n------------------------------------------------------------------------------------------\n$ kubectl get po -o wide\nNAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE                   NOMINATED NODE   READINESS GATES\ndemo-deploy-subnet-778786474b-2kf24   1/1     Running   0          63s     172.16.41.2    spider-control-plane   &lt;none&gt;           &lt;none&gt;\ndemo-deploy-subnet-778786474b-kls7s   1/1     Running   0          4m39s   172.16.41.1    spider-worker          &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>As you can see, the SpiderSubnet object <code>subnet-demo-v4</code> allocates another IP to SpiderIPPool <code>auto4-demo-deploy-subnet-eth0-337bc</code> and SpiderSubnet object <code>subnet-demo-v6</code> allocates another IP to SpiderIPPool <code>auto6-demo-deploy-subnet-eth0-337bc</code>.</p>"},{"location":"usage/spider-subnet/#spidersubnet-with-multiple-interfaces","title":"SpiderSubnet with multiple interfaces","text":"<p>Make sure the interface name and use <code>ipam.spidernet.io/subnets</code> annotation just like this:</p> <pre><code>      annotations:\n        k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni2\n        ipam.spidernet.io/subnets: |-\n          [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, \n           {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}]\n</code></pre> <p>Install the example:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/multiple-interfaces.yaml\n</code></pre>"},{"location":"usage/statefulset/","title":"StatefulSet","text":""},{"location":"usage/statefulset/#description","title":"Description","text":"<p>The spiderpool supports IP assignment for StatefulSet.</p> <p>When the number of statefulset replicas is not scaled up or down, all pods could hold same IP address even when the pod is restarting or rebuilding.</p> <ul> <li> <p>Pod restarts</p> <p>Once a pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID.</p> <p>In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property.</p> </li> <li> <p>Pod is deleted and re-created</p> <p>After deleting a StatefulSet pod, kubernetes will re-create a pod with the same name.</p> <p>In this case, Spiderpool will also keep the previous IP and update the ContainerID.</p> </li> </ul>"},{"location":"usage/statefulset/#notice","title":"Notice","text":"<ul> <li> <p>Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its pods are running.</p> </li> <li> <p>When the statefulset is scaled down and then scaled up, the scaled-up pod is not guaranteed to get the previous IP.</p> </li> <li> <p>The IP-GC feature (reclaim IP for the pod of graceful-period timeout) does work for StatefulSet pod.</p> </li> </ul>"},{"location":"usage/statefulset/#get-started","title":"Get Started","text":""},{"location":"usage/statefulset/#enable-statefulset-support","title":"Enable StatefulSet support","text":"<p>Firstly, please ensure you have installed the spiderpool and configure the CNI file. Refer to install for details.</p> <p>Check whether the property <code>enableStatefulSet</code> of the configmap <code>spiderpool-conf</code> is already set to <code>true</code> or not.</p> <pre><code>kubectl -n kube-system get configmap spiderpool-conf -o yaml\n</code></pre> <p>If you want to set it <code>true</code>, run <code>helm upgrade spiderpool spiderpool/spiderpool --set feature.enableStatefulSet=true -n kube-system</code>.</p>"},{"location":"usage/statefulset/#create-a-statefulset","title":"Create a StatefulSet","text":"<p>This is an example to install a StatefulSet.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset/statefulset-demo.yaml\n</code></pre>"},{"location":"usage/statefulset/#validate-the-spiderpool-related-cr-data","title":"Validate the Spiderpool related CR data","text":"<p>Here's the created Pod, spiderippool, and spiderendpoint CR information:</p> <pre><code>$ kubectl get po -o wide\nNAME                       READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES\ndemo-sts-0                 1/1     Running   0          8s    172.22.40.181   spider-worker   &lt;none&gt;           &lt;none&gt;\n---------------------------------------------------------------------------------------------------------------------\n$ kubectl get sp default-v4-ippool -o yaml\n\n...\n\"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"fa50a7c2-99e7-4e97-a3f1-8d6503067b54\"}\n...\n\n---------------------------------------------------------------------------------------------------------------------\n$ kubectl get se demo-sts-0 -o yaml\n\n...\nstatus:\n  current:\n    ips:\n    - interface: eth0\n      ipv4: 172.22.40.181/16\n      ipv4Pool: default-v4-ippool\n      ipv6: fc00:f853:ccd:e793:f::d/64\n      ipv6Pool: default-v6-ippool\n      vlan: 0\n    node: spider-worker\n    uid: fa50a7c2-99e7-4e97-a3f1-8d6503067b54\n  ownerControllerName: demo-sts\n  ownerControllerType: StatefulSet\n...\n</code></pre> <p>Try to delete pod <code>demo-sts-0</code> and check whether the rebuilding pod keeps the previous IP or not.</p> <pre><code>$ kubectl delete po demo-sts-0\npod \"demo-sts-0\" deleted\n---------------------------------------------------------------------------------------------------------------------\n$ kubectl get po -o wide\nNAME                       READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES\ndemo-sts-0                 1/1     Running   0          12s   172.22.40.181   spider-worker   &lt;none&gt;           &lt;none&gt;\n---------------------------------------------------------------------------------------------------------------------\n$ kubectl get sp default-v4-ippool -o yaml\n\n...\n\"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"425d6552-63bb-4b4c-aab2-b2db95de0ab1\"}\n...\n\n---------------------------------------------------------------------------------------------------------------------\n$ kubectl get se demo-sts-0 -o yaml\n\n...\nstatus:\n  current:\n    ips:\n    - interface: eth0\n      ipv4: 172.22.40.181/16\n      ipv4Pool: default-v4-ippool\n      ipv6: fc00:f853:ccd:e793:f::d/64\n      ipv6Pool: default-v6-ippool\n      vlan: 0\n    node: spider-worker\n    uid: 425d6552-63bb-4b4c-aab2-b2db95de0ab1\n  ownerControllerName: demo-sts\n  ownerControllerType: StatefulSet\n...\n</code></pre> <p>And you can see, the re-created Pod still holds the previous IP, spiderippool, and spiderendpoint updated containerID property.</p>"},{"location":"usage/statefulset/#clean-up","title":"clean up","text":"<p>Delete the StatefulSet object: <code>demo-sts</code>.</p> <pre><code>$ kubectl delete sts demo-sts\nstatefulset.apps \"demo-sts\" deleted\n\n---------------------------------------------------------------------------------------------------------------------\n\n$ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0\n\n---------------------------------------------------------------------------------------------------------------------\n\n$ kubectl get se demo-sts-0 -o yaml\nError from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found\n</code></pre> <p>The related data is cleaned up now.</p>"},{"location":"usage/third-party-controller/","title":"Spiderpool supports third-party controllers","text":""},{"location":"usage/third-party-controller/#description","title":"Description","text":"<p>Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to pods created not by kubernetes-native controller. There are two ways to do this:</p> <ol> <li> <p>Manual ippool</p> <p>The administrator could create ippool object and assign IP to pods.</p> </li> <li> <p>Automatical ippool</p> <p>Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application.</p> <p>This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet.</p> <p>This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations:</p> <ul> <li> <p>does not support automatically scale up and down the IP</p> </li> <li> <p>does not support automatically delete the ippool</p> </li> </ul> <p>In the future, spiderpool may support all operation of automatical ippool.</p> </li> </ol> <p>Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as <code>stateless</code> pod like <code>Deployment</code>, this means pods created by none kubernetes-native controller is able to fix the IP range like <code>Deployment</code>, but not able to bind each pod to a specific IP address like <code>Statefulset</code>.</p>"},{"location":"usage/third-party-controller/#get-started","title":"Get Started","text":"<p>It will use OpenKruise to demonstrate how Spiderpool supports third-party controllers. </p>"},{"location":"usage/third-party-controller/#set-up-spiderpool","title":"Set up Spiderpool","text":"<p>See installation for more details.</p>"},{"location":"usage/third-party-controller/#set-up-openkruise","title":"Set up OpenKruise","text":"<p>Please refer to OpenKruise</p>"},{"location":"usage/third-party-controller/#create-pod-by-manual-ippool-way","title":"Create pod by <code>Manual ippool</code> way","text":"<ol> <li> <p>Create a custom IPPool.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml\n</code></pre> <pre><code>apiVersion: spiderpool.spidernet.io/v2beta1\nkind: SpiderIPPool\nmetadata:\nname: custom-ipv4-ippool\nspec:\nsubnet: 172.18.41.0/24\nips:\n- 172.18.41.40-172.18.41.50\n</code></pre> </li> <li> <p>Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations <code>ipam.spidernet.io/ippool</code></p> <pre><code>apiVersion: apps.kruise.io/v1alpha1\nkind: CloneSet\nmetadata:\nname: custom-kruise-cloneset\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: custom-kruise-cloneset\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/ippool: |-\n{\n\"ipv4\": [\"custom-ipv4-ippool\"]\n}\nlabels:\napp: custom-kruise-cloneset\nspec:\ncontainers:\n- name: custom-kruise-cloneset\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>As expected, Pods of OpenKruise CloneSet <code>custom-kruise-cloneset</code> will be assigned with IP addresses from IPPool <code>custom-ipv4-ippool</code>.</p> <pre><code>kubectl get po -l app=custom-kruise-cloneset -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP             NODE            NOMINATED NODE   READINESS GATES\ncustom-kruise-cloneset-8m9ls   1/1     Running   0          96s   172.18.41.44   spider-worker   &lt;none&gt;           2/2\ncustom-kruise-cloneset-c4z9f   1/1     Running   0          96s   172.18.41.50   spider-worker   &lt;none&gt;           2/2\ncustom-kruise-cloneset-w9kfm   1/1     Running   0          96s   172.18.41.46   spider-worker   &lt;none&gt;           2/2\n</code></pre> </li> </ol>"},{"location":"usage/third-party-controller/#create-pod-by-automatical-ippool-way","title":"Create pod by <code>Automatical ippool</code> way","text":"<ol> <li> <p>Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations <code>ipam.spidernet.io/subnet</code></p> <pre><code>apiVersion: apps.kruise.io/v1alpha1\nkind: CloneSet\nmetadata:\nname: custom-kruise-cloneset\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: custom-kruise-cloneset\ntemplate:\nmetadata:\nannotations:\nipam.spidernet.io/subnet: |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]}\nipam.spidernet.io/ippool-ip-number: \"5\"\nlabels:\napp: custom-kruise-cloneset\nspec:\ncontainers:\n- name: custom-kruise-cloneset\nimage: busybox\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\n</code></pre> <p>NOTICE:</p> <ol> <li>You must specify a fixed IP number for auto-created IPPool like <code>ipam.spidernet.io/ippool-ip-number: \"5\"</code>.    Because Spiderpool has no idea about the replica number, so it does not support annotation like <code>ipam.spidernet.io/ippool-ip-number: \"+5\"</code>.</li> </ol> </li> <li> <p>Check status</p> <p>As expected, Spiderpool will create auto-created IPPool from <code>subnet-demo-v4</code> and <code>subnet-demo-v6</code> objects. And Pods of OpenKruise CloneSet <code>custom-kruise-cloneset</code> will be assigned with IP addresses from the created IPPools.</p> <pre><code>$ kubectl get sp | grep kruise\nNAME                                      VERSION   SUBNET                    ALLOCATED-IP-COUNT   TOTAL-IP-COUNT   DEFAULT   DISABLE   APP-NAMESPACE\nauto4-custom-kruise-cloneset-eth0-028d6   4         172.16.0.0/16             3                    5                false     false     default\nauto6-custom-kruise-cloneset-eth0-028d6   6         fc00:f853:ccd:e790::/64   3                    5                false     false     default\n------------------------------------------------------------------------------------------\n$ kubectl get po -l app=custom-kruise-cloneset -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES\ncustom-kruise-cloneset-f52dn   1/1     Running   0          61s   172.16.41.4   spider-worker   &lt;none&gt;           2/2\ncustom-kruise-cloneset-mq67v   1/1     Running   0          61s   172.16.41.5   spider-worker   &lt;none&gt;           2/2\ncustom-kruise-cloneset-nprpf   1/1     Running   0          61s   172.16.41.1   spider-worker   &lt;none&gt;           2/2\n</code></pre> </li> </ol>"},{"location":"usage/upgrade/","title":"Upgrading Spiderpool Versions","text":"<p>This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool.</p>"},{"location":"usage/upgrade/#upgrade-to-036-from-035","title":"Upgrade to 0.3.6 from (&lt;=0.3.5)","text":""},{"location":"usage/upgrade/#description","title":"Description","text":"<ol> <li>There's a design flaw for SpiderSubnet feature in auto-created IPPool label.    The previous label <code>ipam.spidernet.io/owner-application</code> corresponding value uses '-' as separative sign.    For example, we have deployment <code>ns398-174835790/deploy398-82311862</code> and the corresponding label value is <code>Deployment-ns398-174835790-deploy398-82311862</code>.    It's very hard to unpack it to trace back what the application namespace and name is.    Now, we use '_' rather than '-' as slash for SpiderSubnet feature label <code>ipam.spidernet.io/owner-application</code>, and the upper case    will be like <code>Deployment_ns398-174835790_deploy398-82311862</code>.    Reference PR: #1162</li> <li>In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool.    The key is <code>ipam.spidernet.io/interface</code>, and the value is the corresponding interface name.</li> </ol>"},{"location":"usage/upgrade/#operation-steps","title":"Operation steps","text":"<ol> <li> <p>Find all auto-created IPPools, their name format is <code>auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid}</code> such as <code>auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41</code>.</p> </li> <li> <p>Replace their label, just like this:</p> <pre><code>kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}'\n</code></pre> </li> <li> <p>Add one more label</p> <pre><code>kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}'\n</code></pre> </li> <li> <p>Update your Spiderpool components version and restart them all.</p> </li> </ol>"},{"location":"usage/upgrade/#upgrade-to-040-from-040","title":"Upgrade to 0.4.0 from (&lt;0.4.0)","text":""},{"location":"usage/upgrade/#description_1","title":"Description","text":"<p>Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from <code>None</code> to <code>Pod</code>.</p>"},{"location":"usage/upgrade/#operation-steps_1","title":"Operation steps","text":"<ol> <li> <p>Find all SpiderEndpoint objects that their Status OwnerControllerType is <code>None</code></p> </li> <li> <p>Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from <code>None</code> to <code>Pod</code></p> </li> </ol>"}]}